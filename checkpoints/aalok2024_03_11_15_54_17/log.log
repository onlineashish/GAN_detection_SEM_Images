train.py  --dataroot  ./dataset/  --classes  eval_100000  --batch_size  64  --delr_freq  10  --lr  0.0002  --niter  100  --loss_freq  10  --name  aalok
cwd: /media/aalok/Zone_C/SahAshish/Detection/NPR-DeepfakeDetection(Resnet9)
2024_03_11_15_54_23 Train loss: 0.6436446309089661 at step: 10 lr 0.0002
(Val @ epoch 0) acc: 0.5; ap: 0.6635026141644025
2024_03_11_15_54_28 Train loss: 0.2677052319049835 at step: 20 lr 0.0002
2024_03_11_15_54_33 Train loss: 0.19925540685653687 at step: 30 lr 0.0002
(Val @ epoch 1) acc: 0.5066666666666667; ap: 0.5735499368279517
2024_03_11_15_54_39 Train loss: 0.07941737771034241 at step: 40 lr 0.0002
2024_03_11_15_54_44 Train loss: 0.09572690725326538 at step: 50 lr 0.0002
(Val @ epoch 2) acc: 0.5; ap: 0.739874746426064
2024_03_11_15_54_50 Train loss: 0.038797710090875626 at step: 60 lr 0.0002
(Val @ epoch 3) acc: 0.5; ap: 0.9476331996591938
2024_03_11_15_54_55 Train loss: 0.015173867344856262 at step: 70 lr 0.0002
2024_03_11_15_55_00 Train loss: 0.02066289633512497 at step: 80 lr 0.0002
(Val @ epoch 4) acc: 0.94; ap: 0.9999999999999998
2024_03_11_15_55_06 Train loss: 0.008106031455099583 at step: 90 lr 0.0002
2024_03_11_15_55_11 Train loss: 0.012883144430816174 at step: 100 lr 0.0002
(Val @ epoch 5) acc: 0.9133333333333333; ap: 0.9999999999999998
2024_03_11_15_55_17 Train loss: 0.07440875470638275 at step: 110 lr 0.0002
(Val @ epoch 6) acc: 0.7666666666666667; ap: 0.9998245614035086
2024_03_11_15_55_23 Train loss: 0.0540330670773983 at step: 120 lr 0.0002
2024_03_11_15_55_28 Train loss: 0.01808088831603527 at step: 130 lr 0.0002
(Val @ epoch 7) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_55_34 Train loss: 0.06521763652563095 at step: 140 lr 0.0002
2024_03_11_15_55_39 Train loss: 0.00798251386731863 at step: 150 lr 0.0002
(Val @ epoch 8) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_55_44 Train loss: 0.009638535790145397 at step: 160 lr 0.0002
2024_03_11_15_55_49 Train loss: 0.004161293618381023 at step: 170 lr 0.0002
(Val @ epoch 9) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_55_55 Train loss: 0.009685218334197998 at step: 180 lr 0.0002
2024_03_11_15_55_59 changing lr at the end of epoch 10, iters 187
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 10) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_56_01 Train loss: 0.013114639557898045 at step: 190 lr 0.00018
2024_03_11_15_56_06 Train loss: 0.021026700735092163 at step: 200 lr 0.00018
(Val @ epoch 11) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_56_12 Train loss: 0.00818224623799324 at step: 210 lr 0.00018
2024_03_11_15_56_17 Train loss: 0.002333158627152443 at step: 220 lr 0.00018
(Val @ epoch 12) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_56_23 Train loss: 0.026693381369113922 at step: 230 lr 0.00018
(Val @ epoch 13) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_56_29 Train loss: 0.002159442752599716 at step: 240 lr 0.00018
2024_03_11_15_56_34 Train loss: 0.004942084662616253 at step: 250 lr 0.00018
(Val @ epoch 14) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_56_40 Train loss: 0.017117323353886604 at step: 260 lr 0.00018
2024_03_11_15_56_45 Train loss: 0.0019130134023725986 at step: 270 lr 0.00018
(Val @ epoch 15) acc: 0.86; ap: 0.9999999999999998
2024_03_11_15_56_51 Train loss: 0.042041145265102386 at step: 280 lr 0.00018
(Val @ epoch 16) acc: 0.7666666666666667; ap: 0.9998245614035086
2024_03_11_15_56_56 Train loss: 0.03118065558373928 at step: 290 lr 0.00018
2024_03_11_15_57_01 Train loss: 0.01458197645843029 at step: 300 lr 0.00018
(Val @ epoch 17) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_57_07 Train loss: 0.03234422206878662 at step: 310 lr 0.00018
2024_03_11_15_57_12 Train loss: 0.04467255622148514 at step: 320 lr 0.00018
(Val @ epoch 18) acc: 0.98; ap: 0.9985669952662751
2024_03_11_15_57_18 Train loss: 0.0027231425046920776 at step: 330 lr 0.00018
2024_03_11_15_57_23 Train loss: 0.011731434613466263 at step: 340 lr 0.00018
(Val @ epoch 19) acc: 1.0; ap: 0.9999999999999999
2024_03_11_15_57_29 Train loss: 0.0016155753983184695 at step: 350 lr 0.00018
2024_03_11_15_57_33 changing lr at the end of epoch 20, iters 357
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 20) acc: 0.9733333333333334; ap: 0.9999999999999998
2024_03_11_15_57_35 Train loss: 0.007581803016364574 at step: 360 lr 0.000162
2024_03_11_15_57_40 Train loss: 0.005404925439506769 at step: 370 lr 0.000162
(Val @ epoch 21) acc: 0.8133333333333334; ap: 0.9999999999999998
2024_03_11_15_57_46 Train loss: 0.0023462381213903427 at step: 380 lr 0.000162
2024_03_11_15_57_51 Train loss: 0.007817245088517666 at step: 390 lr 0.000162
(Val @ epoch 22) acc: 1.0; ap: 0.9999999999999999
2024_03_11_15_57_57 Train loss: 0.0024301994126290083 at step: 400 lr 0.000162
(Val @ epoch 23) acc: 1.0; ap: 0.9999999999999999
2024_03_11_15_58_03 Train loss: 0.002348155714571476 at step: 410 lr 0.000162
2024_03_11_15_58_08 Train loss: 0.005112874321639538 at step: 420 lr 0.000162
(Val @ epoch 24) acc: 0.9933333333333333; ap: 0.9999999999999998
2024_03_11_15_58_14 Train loss: 0.003405217081308365 at step: 430 lr 0.000162
2024_03_11_15_58_19 Train loss: 0.010657843202352524 at step: 440 lr 0.000162
(Val @ epoch 25) acc: 0.92; ap: 0.9999999999999998
2024_03_11_15_58_24 Train loss: 0.001979248132556677 at step: 450 lr 0.000162
(Val @ epoch 26) acc: 0.9333333333333333; ap: 0.9969566165116638
2024_03_11_15_58_30 Train loss: 0.048688314855098724 at step: 460 lr 0.000162
2024_03_11_15_58_35 Train loss: 0.006407830864191055 at step: 470 lr 0.000162
(Val @ epoch 27) acc: 0.9933333333333333; ap: 0.9999999999999999
2024_03_11_15_58_41 Train loss: 0.005002777092158794 at step: 480 lr 0.000162
2024_03_11_15_58_46 Train loss: 0.018533233553171158 at step: 490 lr 0.000162
(Val @ epoch 28) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_58_52 Train loss: 0.0009423335432074964 at step: 500 lr 0.000162
2024_03_11_15_58_57 Train loss: 0.004522955976426601 at step: 510 lr 0.000162
(Val @ epoch 29) acc: 1.0; ap: 1.0
2024_03_11_15_59_03 Train loss: 0.0021137376315891743 at step: 520 lr 0.000162
2024_03_11_15_59_06 changing lr at the end of epoch 30, iters 527
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 30) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_59_09 Train loss: 0.002034964505583048 at step: 530 lr 0.00014580000000000002
2024_03_11_15_59_14 Train loss: 0.0031177918426692486 at step: 540 lr 0.00014580000000000002
(Val @ epoch 31) acc: 1.0; ap: 0.9999999999999999
2024_03_11_15_59_20 Train loss: 0.0009764303686097264 at step: 550 lr 0.00014580000000000002
2024_03_11_15_59_25 Train loss: 0.0021490082144737244 at step: 560 lr 0.00014580000000000002
(Val @ epoch 32) acc: 1.0; ap: 0.9999999999999999
2024_03_11_15_59_31 Train loss: 0.002236072439700365 at step: 570 lr 0.00014580000000000002
(Val @ epoch 33) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_59_37 Train loss: 0.003859073854982853 at step: 580 lr 0.00014580000000000002
2024_03_11_15_59_42 Train loss: 0.01394573599100113 at step: 590 lr 0.00014580000000000002
(Val @ epoch 34) acc: 1.0; ap: 0.9999999999999998
2024_03_11_15_59_47 Train loss: 0.0016231357585638762 at step: 600 lr 0.00014580000000000002
2024_03_11_15_59_53 Train loss: 0.001090062316507101 at step: 610 lr 0.00014580000000000002
(Val @ epoch 35) acc: 1.0; ap: 1.0
2024_03_11_15_59_58 Train loss: 0.0031032096594572067 at step: 620 lr 0.00014580000000000002
(Val @ epoch 36) acc: 1.0; ap: 1.0
2024_03_11_16_00_04 Train loss: 0.0006989133544266224 at step: 630 lr 0.00014580000000000002
2024_03_11_16_00_09 Train loss: 0.0042787762358784676 at step: 640 lr 0.00014580000000000002
(Val @ epoch 37) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_00_15 Train loss: 0.0010652488563209772 at step: 650 lr 0.00014580000000000002
2024_03_11_16_00_20 Train loss: 0.01152743212878704 at step: 660 lr 0.00014580000000000002
(Val @ epoch 38) acc: 1.0; ap: 1.0
2024_03_11_16_00_26 Train loss: 0.0015462373849004507 at step: 670 lr 0.00014580000000000002
2024_03_11_16_00_31 Train loss: 0.0015352582558989525 at step: 680 lr 0.00014580000000000002
(Val @ epoch 39) acc: 1.0; ap: 1.0
2024_03_11_16_00_37 Train loss: 0.002164422068744898 at step: 690 lr 0.00014580000000000002
2024_03_11_16_00_40 changing lr at the end of epoch 40, iters 697
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 40) acc: 0.9933333333333333; ap: 0.9999999999999998
2024_03_11_16_00_43 Train loss: 0.002633039839565754 at step: 700 lr 0.00013122000000000003
2024_03_11_16_00_48 Train loss: 0.006577081046998501 at step: 710 lr 0.00013122000000000003
(Val @ epoch 41) acc: 0.9533333333333334; ap: 1.0
2024_03_11_16_00_54 Train loss: 0.003991047851741314 at step: 720 lr 0.00013122000000000003
2024_03_11_16_00_59 Train loss: 0.007376234047114849 at step: 730 lr 0.00013122000000000003
(Val @ epoch 42) acc: 0.7933333333333333; ap: 0.9956513629886041
2024_03_11_16_01_05 Train loss: 0.0012890722136944532 at step: 740 lr 0.00013122000000000003
(Val @ epoch 43) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_01_11 Train loss: 0.012597287073731422 at step: 750 lr 0.00013122000000000003
2024_03_11_16_01_16 Train loss: 0.0033622789196670055 at step: 760 lr 0.00013122000000000003
(Val @ epoch 44) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_01_22 Train loss: 0.0022027790546417236 at step: 770 lr 0.00013122000000000003
2024_03_11_16_01_27 Train loss: 0.0022488385438919067 at step: 780 lr 0.00013122000000000003
(Val @ epoch 45) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_01_32 Train loss: 0.0013305079191923141 at step: 790 lr 0.00013122000000000003
(Val @ epoch 46) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_01_38 Train loss: 0.0015740327071398497 at step: 800 lr 0.00013122000000000003
2024_03_11_16_01_43 Train loss: 0.003179403953254223 at step: 810 lr 0.00013122000000000003
(Val @ epoch 47) acc: 1.0; ap: 1.0
2024_03_11_16_01_49 Train loss: 0.0012095373822376132 at step: 820 lr 0.00013122000000000003
2024_03_11_16_01_54 Train loss: 0.0010719404090195894 at step: 830 lr 0.00013122000000000003
(Val @ epoch 48) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_02_00 Train loss: 0.0017344767693430185 at step: 840 lr 0.00013122000000000003
2024_03_11_16_02_05 Train loss: 0.00927051343023777 at step: 850 lr 0.00013122000000000003
(Val @ epoch 49) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_02_11 Train loss: 0.001896940404549241 at step: 860 lr 0.00013122000000000003
2024_03_11_16_02_14 changing lr at the end of epoch 50, iters 867
*************************
Changing lr from 0.00013122000000000003 to 0.00011809800000000003
*************************
(Val @ epoch 50) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_02_17 Train loss: 0.0009659430361352861 at step: 870 lr 0.00011809800000000003
2024_03_11_16_02_22 Train loss: 0.0015530020464211702 at step: 880 lr 0.00011809800000000003
(Val @ epoch 51) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_02_28 Train loss: 0.0008490007603541017 at step: 890 lr 0.00011809800000000003
2024_03_11_16_02_33 Train loss: 0.010303264483809471 at step: 900 lr 0.00011809800000000003
(Val @ epoch 52) acc: 1.0; ap: 1.0
2024_03_11_16_02_39 Train loss: 0.0025050726253539324 at step: 910 lr 0.00011809800000000003
(Val @ epoch 53) acc: 1.0; ap: 1.0
2024_03_11_16_02_45 Train loss: 0.0017724968492984772 at step: 920 lr 0.00011809800000000003
2024_03_11_16_02_50 Train loss: 0.0022184173576533794 at step: 930 lr 0.00011809800000000003
(Val @ epoch 54) acc: 1.0; ap: 1.0
2024_03_11_16_02_55 Train loss: 0.0010861695045605302 at step: 940 lr 0.00011809800000000003
2024_03_11_16_03_01 Train loss: 0.002280214335769415 at step: 950 lr 0.00011809800000000003
(Val @ epoch 55) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_03_06 Train loss: 0.001913030631840229 at step: 960 lr 0.00011809800000000003
(Val @ epoch 56) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_03_12 Train loss: 0.0007090821745805442 at step: 970 lr 0.00011809800000000003
2024_03_11_16_03_17 Train loss: 0.005426246672868729 at step: 980 lr 0.00011809800000000003
(Val @ epoch 57) acc: 1.0; ap: 1.0
2024_03_11_16_03_23 Train loss: 0.0010516747133806348 at step: 990 lr 0.00011809800000000003
2024_03_11_16_03_28 Train loss: 0.0011552725918591022 at step: 1000 lr 0.00011809800000000003
(Val @ epoch 58) acc: 1.0; ap: 1.0
2024_03_11_16_03_34 Train loss: 0.0004371336835902184 at step: 1010 lr 0.00011809800000000003
2024_03_11_16_03_39 Train loss: 0.0022016633301973343 at step: 1020 lr 0.00011809800000000003
(Val @ epoch 59) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_03_45 Train loss: 0.0028779797721654177 at step: 1030 lr 0.00011809800000000003
2024_03_11_16_03_48 changing lr at the end of epoch 60, iters 1037
*************************
Changing lr from 0.00011809800000000003 to 0.00010628820000000004
*************************
(Val @ epoch 60) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_03_51 Train loss: 0.00035147435846738517 at step: 1040 lr 0.00010628820000000004
2024_03_11_16_03_56 Train loss: 0.0003736933576874435 at step: 1050 lr 0.00010628820000000004
(Val @ epoch 61) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_04_02 Train loss: 0.0024626797530800104 at step: 1060 lr 0.00010628820000000004
2024_03_11_16_04_07 Train loss: 0.0043349796906113625 at step: 1070 lr 0.00010628820000000004
(Val @ epoch 62) acc: 1.0; ap: 1.0
2024_03_11_16_04_13 Train loss: 0.0006832725484855473 at step: 1080 lr 0.00010628820000000004
(Val @ epoch 63) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_04_19 Train loss: 0.0005932345520704985 at step: 1090 lr 0.00010628820000000004
2024_03_11_16_04_24 Train loss: 0.00034765011514537036 at step: 1100 lr 0.00010628820000000004
(Val @ epoch 64) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_04_29 Train loss: 0.0005824735853821039 at step: 1110 lr 0.00010628820000000004
2024_03_11_16_04_35 Train loss: 0.0004028490511700511 at step: 1120 lr 0.00010628820000000004
(Val @ epoch 65) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_04_40 Train loss: 0.0007455686572939157 at step: 1130 lr 0.00010628820000000004
(Val @ epoch 66) acc: 1.0; ap: 1.0
2024_03_11_16_04_46 Train loss: 0.00023058432270772755 at step: 1140 lr 0.00010628820000000004
2024_03_11_16_04_51 Train loss: 0.01116110011935234 at step: 1150 lr 0.00010628820000000004
(Val @ epoch 67) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_04_57 Train loss: 0.0026505552232265472 at step: 1160 lr 0.00010628820000000004
2024_03_11_16_05_02 Train loss: 0.00035524257691577077 at step: 1170 lr 0.00010628820000000004
(Val @ epoch 68) acc: 0.9933333333333333; ap: 1.0
2024_03_11_16_05_08 Train loss: 0.002333285752683878 at step: 1180 lr 0.00010628820000000004
2024_03_11_16_05_13 Train loss: 0.0027225681114941835 at step: 1190 lr 0.00010628820000000004
(Val @ epoch 69) acc: 0.9466666666666667; ap: 0.9999999999999998
2024_03_11_16_05_19 Train loss: 0.0005712558049708605 at step: 1200 lr 0.00010628820000000004
2024_03_11_16_05_22 changing lr at the end of epoch 70, iters 1207
*************************
Changing lr from 0.00010628820000000004 to 9.565938000000004e-05
*************************
(Val @ epoch 70) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_05_25 Train loss: 0.0007788001094013453 at step: 1210 lr 9.565938000000004e-05
2024_03_11_16_05_30 Train loss: 0.0015155896544456482 at step: 1220 lr 9.565938000000004e-05
(Val @ epoch 71) acc: 1.0; ap: 1.0
2024_03_11_16_05_36 Train loss: 0.0006722622783854604 at step: 1230 lr 9.565938000000004e-05
2024_03_11_16_05_41 Train loss: 0.000869470473844558 at step: 1240 lr 9.565938000000004e-05
(Val @ epoch 72) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_05_47 Train loss: 0.005343077704310417 at step: 1250 lr 9.565938000000004e-05
(Val @ epoch 73) acc: 0.9933333333333333; ap: 0.9999999999999998
2024_03_11_16_05_52 Train loss: 0.0032228839118033648 at step: 1260 lr 9.565938000000004e-05
2024_03_11_16_05_58 Train loss: 0.0007118411595001817 at step: 1270 lr 9.565938000000004e-05
(Val @ epoch 74) acc: 0.98; ap: 0.9999999999999999
2024_03_11_16_06_03 Train loss: 0.002857124898582697 at step: 1280 lr 9.565938000000004e-05
2024_03_11_16_06_09 Train loss: 0.0021445078309625387 at step: 1290 lr 9.565938000000004e-05
(Val @ epoch 75) acc: 1.0; ap: 1.0
2024_03_11_16_06_14 Train loss: 0.0007602866971865296 at step: 1300 lr 9.565938000000004e-05
(Val @ epoch 76) acc: 1.0; ap: 1.0
2024_03_11_16_06_20 Train loss: 0.00016700170817784965 at step: 1310 lr 9.565938000000004e-05
2024_03_11_16_06_25 Train loss: 0.0014176266267895699 at step: 1320 lr 9.565938000000004e-05
(Val @ epoch 77) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_06_31 Train loss: 0.00029384822119027376 at step: 1330 lr 9.565938000000004e-05
2024_03_11_16_06_36 Train loss: 0.00037199570215307176 at step: 1340 lr 9.565938000000004e-05
(Val @ epoch 78) acc: 1.0; ap: 1.0
2024_03_11_16_06_42 Train loss: 0.0018033076776191592 at step: 1350 lr 9.565938000000004e-05
2024_03_11_16_06_47 Train loss: 0.00031176168704405427 at step: 1360 lr 9.565938000000004e-05
(Val @ epoch 79) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_06_53 Train loss: 0.00038445531390607357 at step: 1370 lr 9.565938000000004e-05
2024_03_11_16_06_56 changing lr at the end of epoch 80, iters 1377
*************************
Changing lr from 9.565938000000004e-05 to 8.609344200000004e-05
*************************
(Val @ epoch 80) acc: 1.0; ap: 1.0
2024_03_11_16_06_59 Train loss: 0.0012684091925621033 at step: 1380 lr 8.609344200000004e-05
2024_03_11_16_07_04 Train loss: 0.0005534623633138835 at step: 1390 lr 8.609344200000004e-05
(Val @ epoch 81) acc: 1.0; ap: 1.0
2024_03_11_16_07_10 Train loss: 0.0005465825088322163 at step: 1400 lr 8.609344200000004e-05
2024_03_11_16_07_15 Train loss: 0.0007486343965865672 at step: 1410 lr 8.609344200000004e-05
(Val @ epoch 82) acc: 1.0; ap: 0.9999999999999998
2024_03_11_16_07_21 Train loss: 0.002578308340162039 at step: 1420 lr 8.609344200000004e-05
(Val @ epoch 83) acc: 1.0; ap: 1.0
2024_03_11_16_07_27 Train loss: 0.0004844306968152523 at step: 1430 lr 8.609344200000004e-05
2024_03_11_16_07_32 Train loss: 0.008313121274113655 at step: 1440 lr 8.609344200000004e-05
(Val @ epoch 84) acc: 1.0; ap: 1.0
2024_03_11_16_07_37 Train loss: 0.00043874105904251337 at step: 1450 lr 8.609344200000004e-05
2024_03_11_16_07_43 Train loss: 0.0008260710164904594 at step: 1460 lr 8.609344200000004e-05
(Val @ epoch 85) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_07_48 Train loss: 0.0010985927656292915 at step: 1470 lr 8.609344200000004e-05
(Val @ epoch 86) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_07_54 Train loss: 0.0005018082447350025 at step: 1480 lr 8.609344200000004e-05
2024_03_11_16_07_59 Train loss: 0.0030039974953979254 at step: 1490 lr 8.609344200000004e-05
(Val @ epoch 87) acc: 1.0; ap: 1.0
2024_03_11_16_08_05 Train loss: 0.00017692474648356438 at step: 1500 lr 8.609344200000004e-05
2024_03_11_16_08_10 Train loss: 0.00021118442236911505 at step: 1510 lr 8.609344200000004e-05
(Val @ epoch 88) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_08_16 Train loss: 0.000103291793493554 at step: 1520 lr 8.609344200000004e-05
2024_03_11_16_08_21 Train loss: 0.003060758812353015 at step: 1530 lr 8.609344200000004e-05
(Val @ epoch 89) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_08_27 Train loss: 0.0020908331498503685 at step: 1540 lr 8.609344200000004e-05
2024_03_11_16_08_30 changing lr at the end of epoch 90, iters 1547
*************************
Changing lr from 8.609344200000004e-05 to 7.748409780000004e-05
*************************
(Val @ epoch 90) acc: 1.0; ap: 1.0
2024_03_11_16_08_33 Train loss: 0.0005355991888791323 at step: 1550 lr 7.748409780000004e-05
2024_03_11_16_08_38 Train loss: 0.0003757579543162137 at step: 1560 lr 7.748409780000004e-05
(Val @ epoch 91) acc: 1.0; ap: 1.0
2024_03_11_16_08_44 Train loss: 0.0005600313888862729 at step: 1570 lr 7.748409780000004e-05
2024_03_11_16_08_49 Train loss: 0.00012440397404134274 at step: 1580 lr 7.748409780000004e-05
(Val @ epoch 92) acc: 1.0; ap: 1.0
2024_03_11_16_08_55 Train loss: 0.0008437493816018105 at step: 1590 lr 7.748409780000004e-05
(Val @ epoch 93) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_09_00 Train loss: 0.00544380210340023 at step: 1600 lr 7.748409780000004e-05
2024_03_11_16_09_06 Train loss: 0.00023096923541743308 at step: 1610 lr 7.748409780000004e-05
(Val @ epoch 94) acc: 1.0; ap: 1.0
2024_03_11_16_09_11 Train loss: 0.0011007990688085556 at step: 1620 lr 7.748409780000004e-05
2024_03_11_16_09_16 Train loss: 0.00020674099505413324 at step: 1630 lr 7.748409780000004e-05
(Val @ epoch 95) acc: 1.0; ap: 1.0
2024_03_11_16_09_22 Train loss: 0.00040241482201963663 at step: 1640 lr 7.748409780000004e-05
(Val @ epoch 96) acc: 1.0; ap: 1.0
2024_03_11_16_09_28 Train loss: 0.001197660225443542 at step: 1650 lr 7.748409780000004e-05
2024_03_11_16_09_33 Train loss: 0.0020283469930291176 at step: 1660 lr 7.748409780000004e-05
(Val @ epoch 97) acc: 1.0; ap: 0.9999999999999999
2024_03_11_16_09_39 Train loss: 0.0015997117152437568 at step: 1670 lr 7.748409780000004e-05
2024_03_11_16_09_44 Train loss: 0.0007889014668762684 at step: 1680 lr 7.748409780000004e-05
(Val @ epoch 98) acc: 1.0; ap: 1.0
2024_03_11_16_09_50 Train loss: 0.0001620883704163134 at step: 1690 lr 7.748409780000004e-05
2024_03_11_16_09_55 Train loss: 0.00031554195447824895 at step: 1700 lr 7.748409780000004e-05
(Val @ epoch 99) acc: 1.0; ap: 1.0
*************************
2024_03_11_16_09_56
(0 eval_100000) acc: 100.0; ap: 100.0
(1 Mean      ) acc: 100.0; ap: 100.0
*************************
2024_03_11_16_09_57
Saving model ./checkpoints/aalok2024_03_11_15_54_17/model_epoch_last.pth
