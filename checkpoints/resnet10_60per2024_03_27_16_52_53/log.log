train.py  --name  resnet10_60per  --dataroot  /home/ashish/detect_dataset/splitted_data_60/  --classes  FastGan,StyleGAN_ADA  --batch_size  128  --delr_freq  30  --lr  0.0002  --niter  500
Model size: 19697.00000
cwd: /home/ashish/NPR-DeepfakeDetectionResnet18
(Val @ epoch 0) acc: 0.5; ap: 0.4173635920954471
(Val @ epoch 1) acc: 0.5; ap: 0.43207210920852757
(Val @ epoch 2) acc: 0.5; ap: 0.5124319715183092
(Val @ epoch 3) acc: 0.5; ap: 0.5211338927306219
2024_03_27_16_53_02 Train loss: 0.6521825194358826 at step: 40 lr 0.0002
(Val @ epoch 4) acc: 0.5; ap: 0.5546943604457826
(Val @ epoch 5) acc: 0.5394736842105263; ap: 0.600776155948014
(Val @ epoch 6) acc: 0.5921052631578947; ap: 0.5782931221010607
(Val @ epoch 7) acc: 0.5789473684210527; ap: 0.5980981892570031
(Val @ epoch 8) acc: 0.5592105263157895; ap: 0.5830759196527937
2024_03_27_16_53_11 Train loss: 0.7492424249649048 at step: 80 lr 0.0002
(Val @ epoch 9) acc: 0.5921052631578947; ap: 0.661120145933048
(Val @ epoch 10) acc: 0.625; ap: 0.6761866787461468
(Val @ epoch 11) acc: 0.6052631578947368; ap: 0.6539113574652007
(Val @ epoch 12) acc: 0.6644736842105263; ap: 0.682020755117119
(Val @ epoch 13) acc: 0.6052631578947368; ap: 0.6962805887232166
2024_03_27_16_53_21 Train loss: 0.624190092086792 at step: 120 lr 0.0002
(Val @ epoch 14) acc: 0.625; ap: 0.7015778914277228
(Val @ epoch 15) acc: 0.6842105263157895; ap: 0.7092191025596964
(Val @ epoch 16) acc: 0.6381578947368421; ap: 0.694415447308697
(Val @ epoch 17) acc: 0.618421052631579; ap: 0.6432842926001014
(Val @ epoch 18) acc: 0.6710526315789473; ap: 0.7393873261140979
2024_03_27_16_53_30 Train loss: 0.8323684930801392 at step: 160 lr 0.0002
(Val @ epoch 19) acc: 0.6973684210526315; ap: 0.7669728975792625
(Val @ epoch 20) acc: 0.6381578947368421; ap: 0.6962318760359493
(Val @ epoch 21) acc: 0.6644736842105263; ap: 0.7814552332636887
(Val @ epoch 22) acc: 0.6578947368421053; ap: 0.7908359457420834
(Val @ epoch 23) acc: 0.6842105263157895; ap: 0.8080593934907032
2024_03_27_16_53_39 Train loss: 0.6491603851318359 at step: 200 lr 0.0002
(Val @ epoch 24) acc: 0.75; ap: 0.8176492804821337
(Val @ epoch 25) acc: 0.7302631578947368; ap: 0.822226618887632
(Val @ epoch 26) acc: 0.6578947368421053; ap: 0.7903300867364471
(Val @ epoch 27) acc: 0.75; ap: 0.816444408014942
(Val @ epoch 28) acc: 0.6776315789473685; ap: 0.8575061308441012
2024_03_27_16_53_48 Train loss: 0.6962910890579224 at step: 240 lr 0.0002
(Val @ epoch 29) acc: 0.7039473684210527; ap: 0.8455856943837778
2024_03_27_16_53_50 changing lr at the end of epoch 30, iters 248
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 30) acc: 0.5263157894736842; ap: 0.6965579718381468
(Val @ epoch 31) acc: 0.7894736842105263; ap: 0.8577700960443462
(Val @ epoch 32) acc: 0.7894736842105263; ap: 0.8721117442058749
(Val @ epoch 33) acc: 0.7302631578947368; ap: 0.8823240459057851
2024_03_27_16_53_57 Train loss: 0.6018418669700623 at step: 280 lr 0.00018
(Val @ epoch 34) acc: 0.75; ap: 0.8875503684919186
(Val @ epoch 35) acc: 0.7960526315789473; ap: 0.8822942704793106
(Val @ epoch 36) acc: 0.7697368421052632; ap: 0.8734657667698834
(Val @ epoch 37) acc: 0.7368421052631579; ap: 0.9027632212047716
(Val @ epoch 38) acc: 0.7763157894736842; ap: 0.9034768086692675
2024_03_27_16_54_07 Train loss: 0.48068177700042725 at step: 320 lr 0.00018
(Val @ epoch 39) acc: 0.5526315789473685; ap: 0.8245718334194923
(Val @ epoch 40) acc: 0.7763157894736842; ap: 0.8876308055708731
(Val @ epoch 41) acc: 0.8289473684210527; ap: 0.9263941525702702
(Val @ epoch 42) acc: 0.8223684210526315; ap: 0.9308126728190799
(Val @ epoch 43) acc: 0.7894736842105263; ap: 0.8931434530613767
2024_03_27_16_54_16 Train loss: 0.48871350288391113 at step: 360 lr 0.00018
(Val @ epoch 44) acc: 0.8157894736842105; ap: 0.9310763370934384
(Val @ epoch 45) acc: 0.8157894736842105; ap: 0.9050077414763377
(Val @ epoch 46) acc: 0.8026315789473685; ap: 0.9359291628721068
(Val @ epoch 47) acc: 0.8289473684210527; ap: 0.9373143373156791
(Val @ epoch 48) acc: 0.8026315789473685; ap: 0.9241336372149698
2024_03_27_16_54_25 Train loss: 0.7107572555541992 at step: 400 lr 0.00018
(Val @ epoch 49) acc: 0.8157894736842105; ap: 0.9426707477654314
(Val @ epoch 50) acc: 0.8355263157894737; ap: 0.9404411016466605
(Val @ epoch 51) acc: 0.8355263157894737; ap: 0.9401074878494555
(Val @ epoch 52) acc: 0.8355263157894737; ap: 0.9398244552020636
(Val @ epoch 53) acc: 0.8289473684210527; ap: 0.9400797255371267
2024_03_27_16_54_34 Train loss: 0.43281352519989014 at step: 440 lr 0.00018
(Val @ epoch 54) acc: 0.8092105263157895; ap: 0.9396684894637061
(Val @ epoch 55) acc: 0.7171052631578947; ap: 0.9275484017072797
(Val @ epoch 56) acc: 0.7894736842105263; ap: 0.9470821675350646
(Val @ epoch 57) acc: 0.8026315789473685; ap: 0.9372485753875226
(Val @ epoch 58) acc: 0.7894736842105263; ap: 0.937556170327435
2024_03_27_16_54_43 Train loss: 0.5140624046325684 at step: 480 lr 0.00018
(Val @ epoch 59) acc: 0.8552631578947368; ap: 0.9421893415807158
2024_03_27_16_54_45 changing lr at the end of epoch 60, iters 488
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 60) acc: 0.7960526315789473; ap: 0.9159019133519746
(Val @ epoch 61) acc: 0.8289473684210527; ap: 0.937778768354483
(Val @ epoch 62) acc: 0.8355263157894737; ap: 0.9481735169072703
(Val @ epoch 63) acc: 0.8157894736842105; ap: 0.9523536029357205
2024_03_27_16_54_53 Train loss: 0.37426185607910156 at step: 520 lr 0.000162
(Val @ epoch 64) acc: 0.8289473684210527; ap: 0.9483881086277697
(Val @ epoch 65) acc: 0.8092105263157895; ap: 0.9528872359409244
(Val @ epoch 66) acc: 0.7894736842105263; ap: 0.9452040698462547
(Val @ epoch 67) acc: 0.7828947368421053; ap: 0.9464061208842092
(Val @ epoch 68) acc: 0.8552631578947368; ap: 0.9575749561004342
2024_03_27_16_55_02 Train loss: 0.7824496626853943 at step: 560 lr 0.000162
(Val @ epoch 69) acc: 0.868421052631579; ap: 0.9568425757689539
(Val @ epoch 70) acc: 0.8552631578947368; ap: 0.9550240999430246
(Val @ epoch 71) acc: 0.8486842105263158; ap: 0.9595812316943697
(Val @ epoch 72) acc: 0.8552631578947368; ap: 0.9538367025082276
(Val @ epoch 73) acc: 0.8618421052631579; ap: 0.9613654378458396
2024_03_27_16_55_11 Train loss: 0.49777165055274963 at step: 600 lr 0.000162
(Val @ epoch 74) acc: 0.868421052631579; ap: 0.9592556341359819
(Val @ epoch 75) acc: 0.8552631578947368; ap: 0.9570729958647403
(Val @ epoch 76) acc: 0.8223684210526315; ap: 0.9606841439705986
(Val @ epoch 77) acc: 0.8618421052631579; ap: 0.9531529852156991
(Val @ epoch 78) acc: 0.8618421052631579; ap: 0.964187786494216
2024_03_27_16_55_20 Train loss: 0.33490678668022156 at step: 640 lr 0.000162
(Val @ epoch 79) acc: 0.881578947368421; ap: 0.96155998436086
(Val @ epoch 80) acc: 0.8618421052631579; ap: 0.9502557662924906
(Val @ epoch 81) acc: 0.868421052631579; ap: 0.9635470206008818
(Val @ epoch 82) acc: 0.8486842105263158; ap: 0.9607664080470878
(Val @ epoch 83) acc: 0.8486842105263158; ap: 0.9611907309538329
2024_03_27_16_55_29 Train loss: 0.33074140548706055 at step: 680 lr 0.000162
(Val @ epoch 84) acc: 0.8486842105263158; ap: 0.9525655809973421
(Val @ epoch 85) acc: 0.8026315789473685; ap: 0.9629519027652587
(Val @ epoch 86) acc: 0.8552631578947368; ap: 0.9429121828959056
(Val @ epoch 87) acc: 0.8223684210526315; ap: 0.9677519195871447
(Val @ epoch 88) acc: 0.8223684210526315; ap: 0.9620668357681829
2024_03_27_16_55_39 Train loss: 0.3850754499435425 at step: 720 lr 0.000162
(Val @ epoch 89) acc: 0.7828947368421053; ap: 0.9404920606227745
2024_03_27_16_55_41 changing lr at the end of epoch 90, iters 728
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 90) acc: 0.7960526315789473; ap: 0.966690252149803
(Val @ epoch 91) acc: 0.9013157894736842; ap: 0.9697122290957921
(Val @ epoch 92) acc: 0.875; ap: 0.9655328385894686
(Val @ epoch 93) acc: 0.8881578947368421; ap: 0.972120727775003
2024_03_27_16_55_48 Train loss: 0.28147199749946594 at step: 760 lr 0.00014580000000000002
(Val @ epoch 94) acc: 0.868421052631579; ap: 0.9681616044057022
(Val @ epoch 95) acc: 0.8618421052631579; ap: 0.9673655863233991
(Val @ epoch 96) acc: 0.868421052631579; ap: 0.9704345576233623
(Val @ epoch 97) acc: 0.875; ap: 0.9708050209032912
(Val @ epoch 98) acc: 0.881578947368421; ap: 0.9664397083022923
2024_03_27_16_55_57 Train loss: 0.450814813375473 at step: 800 lr 0.00014580000000000002
(Val @ epoch 99) acc: 0.868421052631579; ap: 0.964920434289346
(Val @ epoch 100) acc: 0.8552631578947368; ap: 0.9725942314445403
(Val @ epoch 101) acc: 0.8881578947368421; ap: 0.9720456164030693
(Val @ epoch 102) acc: 0.8947368421052632; ap: 0.9722856139519376
(Val @ epoch 103) acc: 0.8947368421052632; ap: 0.9726609949338489
2024_03_27_16_56_06 Train loss: 0.29723116755485535 at step: 840 lr 0.00014580000000000002
(Val @ epoch 104) acc: 0.9078947368421053; ap: 0.9730601728342544
(Val @ epoch 105) acc: 0.875; ap: 0.9733272435607478
(Val @ epoch 106) acc: 0.8947368421052632; ap: 0.973098306034028
(Val @ epoch 107) acc: 0.8947368421052632; ap: 0.9740811429035591
(Val @ epoch 108) acc: 0.9078947368421053; ap: 0.9730138362943611
2024_03_27_16_56_16 Train loss: 0.9498088359832764 at step: 880 lr 0.00014580000000000002
(Val @ epoch 109) acc: 0.8947368421052632; ap: 0.9720795132932162
(Val @ epoch 110) acc: 0.9078947368421053; ap: 0.9757006545020288
(Val @ epoch 111) acc: 0.9144736842105263; ap: 0.9742857213703677
(Val @ epoch 112) acc: 0.9144736842105263; ap: 0.97335818536704
(Val @ epoch 113) acc: 0.8881578947368421; ap: 0.9717677154892852
2024_03_27_16_56_25 Train loss: 0.3038756847381592 at step: 920 lr 0.00014580000000000002
(Val @ epoch 114) acc: 0.9013157894736842; ap: 0.9743371348434031
(Val @ epoch 115) acc: 0.9210526315789473; ap: 0.9753697869894874
(Val @ epoch 116) acc: 0.9078947368421053; ap: 0.9749310734664847
(Val @ epoch 117) acc: 0.9013157894736842; ap: 0.9790170756216171
(Val @ epoch 118) acc: 0.875; ap: 0.9785641377013844
2024_03_27_16_56_34 Train loss: 0.5741685032844543 at step: 960 lr 0.00014580000000000002
(Val @ epoch 119) acc: 0.9342105263157895; ap: 0.9759696008413514
2024_03_27_16_56_36 changing lr at the end of epoch 120, iters 968
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 120) acc: 0.881578947368421; ap: 0.9738385590573756
(Val @ epoch 121) acc: 0.8947368421052632; ap: 0.9773387117336637
(Val @ epoch 122) acc: 0.8947368421052632; ap: 0.9779052883002788
(Val @ epoch 123) acc: 0.9210526315789473; ap: 0.9801805131121208
2024_03_27_16_56_43 Train loss: 0.43272191286087036 at step: 1000 lr 0.00013122000000000003
(Val @ epoch 124) acc: 0.8947368421052632; ap: 0.9794959393158226
(Val @ epoch 125) acc: 0.9276315789473685; ap: 0.9807800910274721
(Val @ epoch 126) acc: 0.9013157894736842; ap: 0.981981219189953
(Val @ epoch 127) acc: 0.9276315789473685; ap: 0.980158125680421
(Val @ epoch 128) acc: 0.9144736842105263; ap: 0.9801307977564826
2024_03_27_16_56_52 Train loss: 0.8686726689338684 at step: 1040 lr 0.00013122000000000003
(Val @ epoch 129) acc: 0.9342105263157895; ap: 0.979688155162607
(Val @ epoch 130) acc: 0.9276315789473685; ap: 0.9822883124509396
(Val @ epoch 131) acc: 0.9078947368421053; ap: 0.9840115818718255
(Val @ epoch 132) acc: 0.9276315789473685; ap: 0.983084093326251
(Val @ epoch 133) acc: 0.9078947368421053; ap: 0.9834643720077948
2024_03_27_16_57_02 Train loss: 0.5794245004653931 at step: 1080 lr 0.00013122000000000003
(Val @ epoch 134) acc: 0.9342105263157895; ap: 0.9823739567352098
(Val @ epoch 135) acc: 0.875; ap: 0.9768091019094067
(Val @ epoch 136) acc: 0.9013157894736842; ap: 0.9811928567238374
(Val @ epoch 137) acc: 0.9407894736842105; ap: 0.9836387191379797
(Val @ epoch 138) acc: 0.9013157894736842; ap: 0.9813004734957728
2024_03_27_16_57_11 Train loss: 0.2397804856300354 at step: 1120 lr 0.00013122000000000003
(Val @ epoch 139) acc: 0.8552631578947368; ap: 0.9861862138724274
(Val @ epoch 140) acc: 0.9342105263157895; ap: 0.9840049824692579
(Val @ epoch 141) acc: 0.9342105263157895; ap: 0.9841694285417496
(Val @ epoch 142) acc: 0.9342105263157895; ap: 0.9842351387013155
(Val @ epoch 143) acc: 0.9342105263157895; ap: 0.9848072234730679
2024_03_27_16_57_20 Train loss: 0.11429902911186218 at step: 1160 lr 0.00013122000000000003
(Val @ epoch 144) acc: 0.9078947368421053; ap: 0.9832490706545984
(Val @ epoch 145) acc: 0.9407894736842105; ap: 0.9851350286440649
(Val @ epoch 146) acc: 0.9473684210526315; ap: 0.9850524446996184
(Val @ epoch 147) acc: 0.9342105263157895; ap: 0.9859603177508558
(Val @ epoch 148) acc: 0.9013157894736842; ap: 0.9866615235390612
2024_03_27_16_57_29 Train loss: 0.27325835824012756 at step: 1200 lr 0.00013122000000000003
(Val @ epoch 149) acc: 0.8881578947368421; ap: 0.9856105957147019
2024_03_27_16_57_31 changing lr at the end of epoch 150, iters 1208
*************************
Changing lr from 0.00013122000000000003 to 0.00011809800000000003
*************************
(Val @ epoch 150) acc: 0.8486842105263158; ap: 0.9892261294295541
(Val @ epoch 151) acc: 0.9407894736842105; ap: 0.980060607069651
(Val @ epoch 152) acc: 0.9078947368421053; ap: 0.9818009165547029
(Val @ epoch 153) acc: 0.8355263157894737; ap: 0.9800966128747312
2024_03_27_16_57_39 Train loss: 0.33127087354660034 at step: 1240 lr 0.00011809800000000003
(Val @ epoch 154) acc: 0.881578947368421; ap: 0.9817755331058078
(Val @ epoch 155) acc: 0.9276315789473685; ap: 0.9838009620142107
(Val @ epoch 156) acc: 0.9407894736842105; ap: 0.9872890870098445
(Val @ epoch 157) acc: 0.9276315789473685; ap: 0.9865888056442611
(Val @ epoch 158) acc: 0.9276315789473685; ap: 0.9869231802009854
2024_03_27_16_57_48 Train loss: 0.3221202790737152 at step: 1280 lr 0.00011809800000000003
(Val @ epoch 159) acc: 0.9144736842105263; ap: 0.9882973449064568
(Val @ epoch 160) acc: 0.9144736842105263; ap: 0.9885326604558261
(Val @ epoch 161) acc: 0.9473684210526315; ap: 0.9893166244753152
(Val @ epoch 162) acc: 0.9342105263157895; ap: 0.9886765423438266
(Val @ epoch 163) acc: 0.9407894736842105; ap: 0.9901424914518515
2024_03_27_16_57_57 Train loss: 0.05873178318142891 at step: 1320 lr 0.00011809800000000003
(Val @ epoch 164) acc: 0.9605263157894737; ap: 0.9870163966153327
(Val @ epoch 165) acc: 0.9013157894736842; ap: 0.9871102592366074
(Val @ epoch 166) acc: 0.9407894736842105; ap: 0.9894186330453055
(Val @ epoch 167) acc: 0.9605263157894737; ap: 0.9871715802989848
(Val @ epoch 168) acc: 0.9210526315789473; ap: 0.9872210938045065
2024_03_27_16_58_06 Train loss: 0.38315367698669434 at step: 1360 lr 0.00011809800000000003
(Val @ epoch 169) acc: 0.9407894736842105; ap: 0.9884318649888979
(Val @ epoch 170) acc: 0.9539473684210527; ap: 0.9870692870638874
(Val @ epoch 171) acc: 0.9342105263157895; ap: 0.9878126751811078
(Val @ epoch 172) acc: 0.9342105263157895; ap: 0.9878775079990239
(Val @ epoch 173) acc: 0.9473684210526315; ap: 0.9878502895031862
2024_03_27_16_58_15 Train loss: 0.22090622782707214 at step: 1400 lr 0.00011809800000000003
(Val @ epoch 174) acc: 0.9605263157894737; ap: 0.987351658801274
(Val @ epoch 175) acc: 0.9605263157894737; ap: 0.9878002447977896
(Val @ epoch 176) acc: 0.9342105263157895; ap: 0.9893095343602158
(Val @ epoch 177) acc: 0.9342105263157895; ap: 0.9882489936551503
(Val @ epoch 178) acc: 0.9342105263157895; ap: 0.98822052917516
2024_03_27_16_58_24 Train loss: 0.7626241445541382 at step: 1440 lr 0.00011809800000000003
(Val @ epoch 179) acc: 0.9473684210526315; ap: 0.9839214251880817
2024_03_27_16_58_26 changing lr at the end of epoch 180, iters 1448
*************************
Changing lr from 0.00011809800000000003 to 0.00010628820000000004
*************************
(Val @ epoch 180) acc: 0.9605263157894737; ap: 0.987838200134639
(Val @ epoch 181) acc: 0.8947368421052632; ap: 0.9900810814375628
(Val @ epoch 182) acc: 0.9605263157894737; ap: 0.9876243059989784
(Val @ epoch 183) acc: 0.9276315789473685; ap: 0.9873640829778212
2024_03_27_16_58_34 Train loss: 0.19204403460025787 at step: 1480 lr 0.00010628820000000004
(Val @ epoch 184) acc: 0.9539473684210527; ap: 0.9898377326810078
(Val @ epoch 185) acc: 0.9407894736842105; ap: 0.9882473510298249
(Val @ epoch 186) acc: 0.9407894736842105; ap: 0.9907105796891674
(Val @ epoch 187) acc: 0.9407894736842105; ap: 0.9872930991081006
(Val @ epoch 188) acc: 0.9407894736842105; ap: 0.9881479535678516
2024_03_27_16_58_43 Train loss: 0.22325682640075684 at step: 1520 lr 0.00010628820000000004
(Val @ epoch 189) acc: 0.9407894736842105; ap: 0.9893312148061529
(Val @ epoch 190) acc: 0.9605263157894737; ap: 0.9880735082507385
(Val @ epoch 191) acc: 0.9473684210526315; ap: 0.9890472143829502
(Val @ epoch 192) acc: 0.9539473684210527; ap: 0.9864747140890703
(Val @ epoch 193) acc: 0.9407894736842105; ap: 0.988592034268378
2024_03_27_16_58_52 Train loss: 0.33133143186569214 at step: 1560 lr 0.00010628820000000004
(Val @ epoch 194) acc: 0.9013157894736842; ap: 0.9882785267890506
(Val @ epoch 195) acc: 0.9539473684210527; ap: 0.9900702948296577
(Val @ epoch 196) acc: 0.9736842105263158; ap: 0.9889677279488409
(Val @ epoch 197) acc: 0.9605263157894737; ap: 0.9892773338597234
(Val @ epoch 198) acc: 0.9605263157894737; ap: 0.9882926883976683
2024_03_27_16_59_01 Train loss: 0.28588154911994934 at step: 1600 lr 0.00010628820000000004
(Val @ epoch 199) acc: 0.9605263157894737; ap: 0.9880302407503877
(Val @ epoch 200) acc: 0.9605263157894737; ap: 0.9889268958675624
(Val @ epoch 201) acc: 0.9539473684210527; ap: 0.9886136126595425
(Val @ epoch 202) acc: 0.9671052631578947; ap: 0.9893189180806633
(Val @ epoch 203) acc: 0.9539473684210527; ap: 0.99024099184246
2024_03_27_16_59_10 Train loss: 0.14568603038787842 at step: 1640 lr 0.00010628820000000004
(Val @ epoch 204) acc: 0.9671052631578947; ap: 0.9896536371297995
(Val @ epoch 205) acc: 0.9539473684210527; ap: 0.9909287239598253
(Val @ epoch 206) acc: 0.9539473684210527; ap: 0.9906515008736587
(Val @ epoch 207) acc: 0.9736842105263158; ap: 0.9906243728438421
(Val @ epoch 208) acc: 0.9736842105263158; ap: 0.9912056173076104
2024_03_27_16_59_19 Train loss: 0.11218971014022827 at step: 1680 lr 0.00010628820000000004
(Val @ epoch 209) acc: 0.9671052631578947; ap: 0.9906739309488564
2024_03_27_16_59_21 changing lr at the end of epoch 210, iters 1688
*************************
Changing lr from 0.00010628820000000004 to 9.565938000000004e-05
*************************
(Val @ epoch 210) acc: 0.9605263157894737; ap: 0.9896163041862881
(Val @ epoch 211) acc: 0.9407894736842105; ap: 0.9867502720782305
(Val @ epoch 212) acc: 0.9671052631578947; ap: 0.9883577603583354
(Val @ epoch 213) acc: 0.9473684210526315; ap: 0.9866237205471876
2024_03_27_16_59_29 Train loss: 0.2110506296157837 at step: 1720 lr 9.565938000000004e-05
(Val @ epoch 214) acc: 0.9407894736842105; ap: 0.9893929253167281
(Val @ epoch 215) acc: 0.9802631578947368; ap: 0.9917588945070503
(Val @ epoch 216) acc: 0.9473684210526315; ap: 0.9920134335696388
(Val @ epoch 217) acc: 0.9736842105263158; ap: 0.9911298299903918
(Val @ epoch 218) acc: 0.9671052631578947; ap: 0.9920721634970078
2024_03_27_16_59_38 Train loss: 0.22936415672302246 at step: 1760 lr 9.565938000000004e-05
(Val @ epoch 219) acc: 0.9407894736842105; ap: 0.9915321506742532
(Val @ epoch 220) acc: 0.9539473684210527; ap: 0.99256538104953
(Val @ epoch 221) acc: 0.9605263157894737; ap: 0.9914033566484256
(Val @ epoch 222) acc: 0.9342105263157895; ap: 0.991981078558637
(Val @ epoch 223) acc: 0.9013157894736842; ap: 0.9935821534467499
2024_03_27_16_59_47 Train loss: 0.7239122986793518 at step: 1800 lr 9.565938000000004e-05
(Val @ epoch 224) acc: 0.8552631578947368; ap: 0.9930751247524664
(Val @ epoch 225) acc: 0.9736842105263158; ap: 0.9926914040127085
(Val @ epoch 226) acc: 0.9671052631578947; ap: 0.9931334604666289
(Val @ epoch 227) acc: 0.9276315789473685; ap: 0.9946211920949235
(Val @ epoch 228) acc: 0.9473684210526315; ap: 0.9906546011528078
2024_03_27_16_59_56 Train loss: 0.15439032018184662 at step: 1840 lr 9.565938000000004e-05
(Val @ epoch 229) acc: 0.9736842105263158; ap: 0.9927869218311642
(Val @ epoch 230) acc: 0.9671052631578947; ap: 0.9930400641506519
(Val @ epoch 231) acc: 0.9078947368421053; ap: 0.9929692418811851
(Val @ epoch 232) acc: 0.9276315789473685; ap: 0.9922462869063771
(Val @ epoch 233) acc: 0.9736842105263158; ap: 0.9934305678076741
2024_03_27_17_00_05 Train loss: 0.246824711561203 at step: 1880 lr 9.565938000000004e-05
(Val @ epoch 234) acc: 0.9539473684210527; ap: 0.9932954847735058
(Val @ epoch 235) acc: 0.9473684210526315; ap: 0.9939968807839594
(Val @ epoch 236) acc: 0.9605263157894737; ap: 0.9928085060397641
(Val @ epoch 237) acc: 0.9736842105263158; ap: 0.9926505909049278
(Val @ epoch 238) acc: 0.9736842105263158; ap: 0.9927347797220408
2024_03_27_17_00_15 Train loss: 0.19224752485752106 at step: 1920 lr 9.565938000000004e-05
(Val @ epoch 239) acc: 0.9144736842105263; ap: 0.9928605171475957
2024_03_27_17_00_17 changing lr at the end of epoch 240, iters 1928
*************************
Changing lr from 9.565938000000004e-05 to 8.609344200000004e-05
*************************
(Val @ epoch 240) acc: 0.9473684210526315; ap: 0.9950886443756672
(Val @ epoch 241) acc: 0.9539473684210527; ap: 0.9932795527198343
(Val @ epoch 242) acc: 0.9210526315789473; ap: 0.9929716253976729
(Val @ epoch 243) acc: 0.9605263157894737; ap: 0.99518448801979
2024_03_27_17_00_24 Train loss: 0.28689277172088623 at step: 1960 lr 8.609344200000004e-05
(Val @ epoch 244) acc: 0.9276315789473685; ap: 0.9964908573977385
(Val @ epoch 245) acc: 0.9736842105263158; ap: 0.9952225820633281
(Val @ epoch 246) acc: 0.9802631578947368; ap: 0.991875521945659
(Val @ epoch 247) acc: 0.9539473684210527; ap: 0.9929653675663902
(Val @ epoch 248) acc: 0.9144736842105263; ap: 0.9912914323545077
2024_03_27_17_00_33 Train loss: 0.35871151089668274 at step: 2000 lr 8.609344200000004e-05
(Val @ epoch 249) acc: 0.9144736842105263; ap: 0.9953713965155481
(Val @ epoch 250) acc: 0.9407894736842105; ap: 0.9962247507037078
(Val @ epoch 251) acc: 0.9736842105263158; ap: 0.995131470221274
(Val @ epoch 252) acc: 0.9671052631578947; ap: 0.9952425921252835
(Val @ epoch 253) acc: 0.9802631578947368; ap: 0.9951654021405786
2024_03_27_17_00_43 Train loss: 0.2839395999908447 at step: 2040 lr 8.609344200000004e-05
(Val @ epoch 254) acc: 0.9802631578947368; ap: 0.9941068649476511
(Val @ epoch 255) acc: 0.9671052631578947; ap: 0.9943903518363825
(Val @ epoch 256) acc: 0.9736842105263158; ap: 0.9935290593969354
(Val @ epoch 257) acc: 0.9605263157894737; ap: 0.9949949815291421
(Val @ epoch 258) acc: 0.8881578947368421; ap: 0.9954580874260239
2024_03_27_17_00_52 Train loss: 0.15063689649105072 at step: 2080 lr 8.609344200000004e-05
(Val @ epoch 259) acc: 0.9539473684210527; ap: 0.9922636750235642
(Val @ epoch 260) acc: 0.9736842105263158; ap: 0.9936079440717381
(Val @ epoch 261) acc: 0.9671052631578947; ap: 0.9937408530902586
(Val @ epoch 262) acc: 0.9605263157894737; ap: 0.9938384975809457
(Val @ epoch 263) acc: 0.9671052631578947; ap: 0.9937949048262394
2024_03_27_17_01_01 Train loss: 0.16423925757408142 at step: 2120 lr 8.609344200000004e-05
(Val @ epoch 264) acc: 0.9671052631578947; ap: 0.9939285589294204
(Val @ epoch 265) acc: 0.9671052631578947; ap: 0.9946642872926308
(Val @ epoch 266) acc: 0.9539473684210527; ap: 0.9912941393652275
(Val @ epoch 267) acc: 0.9144736842105263; ap: 0.9936429613414288
(Val @ epoch 268) acc: 0.9473684210526315; ap: 0.9951455385902683
2024_03_27_17_01_10 Train loss: 0.8767480850219727 at step: 2160 lr 8.609344200000004e-05
(Val @ epoch 269) acc: 0.9342105263157895; ap: 0.9940672521974023
2024_03_27_17_01_12 changing lr at the end of epoch 270, iters 2168
*************************
Changing lr from 8.609344200000004e-05 to 7.748409780000004e-05
*************************
(Val @ epoch 270) acc: 0.9473684210526315; ap: 0.9953693590904003
(Val @ epoch 271) acc: 0.9539473684210527; ap: 0.9961623775763192
(Val @ epoch 272) acc: 0.9671052631578947; ap: 0.995350768466888
(Val @ epoch 273) acc: 0.9539473684210527; ap: 0.9942799327522978
2024_03_27_17_01_19 Train loss: 0.3893030881881714 at step: 2200 lr 7.748409780000004e-05
(Val @ epoch 274) acc: 0.9736842105263158; ap: 0.9943377052061921
(Val @ epoch 275) acc: 0.9736842105263158; ap: 0.9946508290332479
(Val @ epoch 276) acc: 0.9736842105263158; ap: 0.9947681212750586
(Val @ epoch 277) acc: 0.9802631578947368; ap: 0.9942469209700601
(Val @ epoch 278) acc: 0.9802631578947368; ap: 0.994848052349007
2024_03_27_17_01_29 Train loss: 0.06479382514953613 at step: 2240 lr 7.748409780000004e-05
(Val @ epoch 279) acc: 0.9605263157894737; ap: 0.9947046214826846
(Val @ epoch 280) acc: 0.9802631578947368; ap: 0.9946508290332479
(Val @ epoch 281) acc: 0.9671052631578947; ap: 0.9943134518537654
(Val @ epoch 282) acc: 0.9671052631578947; ap: 0.9956323264523931
(Val @ epoch 283) acc: 0.9605263157894737; ap: 0.9945365581938214
2024_03_27_17_01_38 Train loss: 0.2143748700618744 at step: 2280 lr 7.748409780000004e-05
(Val @ epoch 284) acc: 0.9605263157894737; ap: 0.9947517482449449
(Val @ epoch 285) acc: 0.9605263157894737; ap: 0.9949180344979173
(Val @ epoch 286) acc: 0.9473684210526315; ap: 0.9952928737860706
(Val @ epoch 287) acc: 0.9671052631578947; ap: 0.9945403121078489
(Val @ epoch 288) acc: 0.9605263157894737; ap: 0.9960828361426778
2024_03_27_17_01_47 Train loss: 0.5796799659729004 at step: 2320 lr 7.748409780000004e-05
(Val @ epoch 289) acc: 0.9605263157894737; ap: 0.994011958564045
(Val @ epoch 290) acc: 0.9539473684210527; ap: 0.9921397925587668
(Val @ epoch 291) acc: 0.9736842105263158; ap: 0.9937075931668004
(Val @ epoch 292) acc: 0.9539473684210527; ap: 0.992685017783412
(Val @ epoch 293) acc: 0.9342105263157895; ap: 0.9925071371763121
2024_03_27_17_01_56 Train loss: 0.3777284324169159 at step: 2360 lr 7.748409780000004e-05
(Val @ epoch 294) acc: 0.9210526315789473; ap: 0.993078281459525
(Val @ epoch 295) acc: 0.9605263157894737; ap: 0.9955316905026987
(Val @ epoch 296) acc: 0.9736842105263158; ap: 0.9965208680163152
(Val @ epoch 297) acc: 0.9605263157894737; ap: 0.9969452048648093
(Val @ epoch 298) acc: 0.9736842105263158; ap: 0.9952042443804638
2024_03_27_17_02_05 Train loss: 0.04224645718932152 at step: 2400 lr 7.748409780000004e-05
(Val @ epoch 299) acc: 0.9671052631578947; ap: 0.996060441328872
2024_03_27_17_02_07 changing lr at the end of epoch 300, iters 2408
*************************
Changing lr from 7.748409780000004e-05 to 6.973568802000003e-05
*************************
(Val @ epoch 300) acc: 0.9736842105263158; ap: 0.9956524200738113
(Val @ epoch 301) acc: 0.9736842105263158; ap: 0.996603294683903
(Val @ epoch 302) acc: 0.9605263157894737; ap: 0.9969600448213547
(Val @ epoch 303) acc: 0.9736842105263158; ap: 0.9945733238900867
2024_03_27_17_02_15 Train loss: 0.1686544418334961 at step: 2440 lr 6.973568802000003e-05
(Val @ epoch 304) acc: 0.9736842105263158; ap: 0.9949606484299536
(Val @ epoch 305) acc: 0.9671052631578947; ap: 0.9950710675140382
(Val @ epoch 306) acc: 0.9736842105263158; ap: 0.9947376332649224
(Val @ epoch 307) acc: 0.9802631578947368; ap: 0.9942799327522978
(Val @ epoch 308) acc: 0.9407894736842105; ap: 0.9927123311291434
2024_03_27_17_02_24 Train loss: 0.13723339140415192 at step: 2480 lr 6.973568802000003e-05
(Val @ epoch 309) acc: 0.9671052631578947; ap: 0.9949464510594488
(Val @ epoch 310) acc: 0.9802631578947368; ap: 0.9936440458001029
(Val @ epoch 311) acc: 0.9802631578947368; ap: 0.9929293766709167
(Val @ epoch 312) acc: 0.9671052631578947; ap: 0.9941071516969846
(Val @ epoch 313) acc: 0.9671052631578947; ap: 0.9943508164143335
2024_03_27_17_02_33 Train loss: 0.2935163378715515 at step: 2520 lr 6.973568802000003e-05
(Val @ epoch 314) acc: 0.9736842105263158; ap: 0.9943618806827879
(Val @ epoch 315) acc: 0.9736842105263158; ap: 0.9946011151325487
(Val @ epoch 316) acc: 0.9736842105263158; ap: 0.9946778496904625
(Val @ epoch 317) acc: 0.9605263157894737; ap: 0.9953504083545438
(Val @ epoch 318) acc: 0.9671052631578947; ap: 0.9953652483110893
2024_03_27_17_02_42 Train loss: 0.5288828015327454 at step: 2560 lr 6.973568802000003e-05
(Val @ epoch 319) acc: 0.9736842105263158; ap: 0.9940312131367162
(Val @ epoch 320) acc: 0.9276315789473685; ap: 0.9952245296188817
(Val @ epoch 321) acc: 0.9144736842105263; ap: 0.9964472939985823
(Val @ epoch 322) acc: 0.9605263157894737; ap: 0.994684744146747
(Val @ epoch 323) acc: 0.9276315789473685; ap: 0.9953019189020362
2024_03_27_17_02_51 Train loss: 0.09417755156755447 at step: 2600 lr 6.973568802000003e-05
(Val @ epoch 324) acc: 0.9605263157894737; ap: 0.996156176858375
(Val @ epoch 325) acc: 0.9802631578947368; ap: 0.9954710750710248
(Val @ epoch 326) acc: 0.9407894736842105; ap: 0.9956630922213044
(Val @ epoch 327) acc: 0.9671052631578947; ap: 0.996603294683903
(Val @ epoch 328) acc: 0.9802631578947368; ap: 0.9953956498679749
2024_03_27_17_03_00 Train loss: 0.26422637701034546 at step: 2640 lr 6.973568802000003e-05
(Val @ epoch 329) acc: 0.9802631578947368; ap: 0.993871902541636
2024_03_27_17_03_02 changing lr at the end of epoch 330, iters 2648
*************************
Changing lr from 6.973568802000003e-05 to 6.276211921800003e-05
*************************
(Val @ epoch 330) acc: 0.9802631578947368; ap: 0.9935973504709943
(Val @ epoch 331) acc: 0.9736842105263158; ap: 0.9931759440248943
(Val @ epoch 332) acc: 0.9671052631578947; ap: 0.9951983287109013
(Val @ epoch 333) acc: 0.9736842105263158; ap: 0.9952960186505054
2024_03_27_17_03_10 Train loss: 0.17562705278396606 at step: 2680 lr 6.276211921800003e-05
(Val @ epoch 334) acc: 0.9736842105263158; ap: 0.9957170982311775
(Val @ epoch 335) acc: 0.9802631578947368; ap: 0.9949606484299536
(Val @ epoch 336) acc: 0.9736842105263158; ap: 0.9962536579339981
(Val @ epoch 337) acc: 0.9736842105263158; ap: 0.996552023566294
(Val @ epoch 338) acc: 0.9671052631578947; ap: 0.9959578761809752
2024_03_27_17_03_19 Train loss: 0.07114221155643463 at step: 2720 lr 6.276211921800003e-05
(Val @ epoch 339) acc: 0.9802631578947368; ap: 0.9959152407932147
(Val @ epoch 340) acc: 0.9802631578947368; ap: 0.996552023566294
(Val @ epoch 341) acc: 0.9605263157894737; ap: 0.9959251905978368
(Val @ epoch 342) acc: 0.9605263157894737; ap: 0.9963431680942807
(Val @ epoch 343) acc: 0.9802631578947368; ap: 0.9945645654602756
2024_03_27_17_03_28 Train loss: 0.21244488656520844 at step: 2760 lr 6.276211921800003e-05
(Val @ epoch 344) acc: 0.9605263157894737; ap: 0.9943377052061921
(Val @ epoch 345) acc: 0.9671052631578947; ap: 0.9955878520009136
(Val @ epoch 346) acc: 0.9473684210526315; ap: 0.9954729273825694
(Val @ epoch 347) acc: 0.9605263157894737; ap: 0.9959152407932147
(Val @ epoch 348) acc: 0.9276315789473685; ap: 0.9962357536137274
2024_03_27_17_03_37 Train loss: 0.23596152663230896 at step: 2800 lr 6.276211921800003e-05
(Val @ epoch 349) acc: 0.9671052631578947; ap: 0.9944448103126862
(Val @ epoch 350) acc: 0.9671052631578947; ap: 0.9956959425476006
(Val @ epoch 351) acc: 0.9671052631578947; ap: 0.9941850263686917
(Val @ epoch 352) acc: 0.9407894736842105; ap: 0.9949348895374037
(Val @ epoch 353) acc: 0.9605263157894737; ap: 0.9960105422182761
2024_03_27_17_03_47 Train loss: 0.10812053829431534 at step: 2840 lr 6.276211921800003e-05
(Val @ epoch 354) acc: 0.9605263157894737; ap: 0.9955194017726404
(Val @ epoch 355) acc: 0.9407894736842105; ap: 0.9934750416600107
(Val @ epoch 356) acc: 0.9605263157894737; ap: 0.9949735482477151
(Val @ epoch 357) acc: 0.9539473684210527; ap: 0.9961601721528699
(Val @ epoch 358) acc: 0.9736842105263158; ap: 0.995131470221274
2024_03_27_17_03_56 Train loss: 0.4652128219604492 at step: 2880 lr 6.276211921800003e-05
(Val @ epoch 359) acc: 0.9605263157894737; ap: 0.9951343413951312
2024_03_27_17_03_58 changing lr at the end of epoch 360, iters 2888
*************************
Changing lr from 6.276211921800003e-05 to 5.6485907296200035e-05
*************************
(Val @ epoch 360) acc: 0.9605263157894737; ap: 0.9942799327522978
(Val @ epoch 361) acc: 0.9342105263157895; ap: 0.9935620711791731
(Val @ epoch 362) acc: 0.9736842105263158; ap: 0.9955966104307247
(Val @ epoch 363) acc: 0.9539473684210527; ap: 0.9941068649476511
2024_03_27_17_04_05 Train loss: 0.23310837149620056 at step: 2920 lr 5.6485907296200035e-05
(Val @ epoch 364) acc: 0.9802631578947368; ap: 0.99407154732677
(Val @ epoch 365) acc: 0.9736842105263158; ap: 0.996130943985622
(Val @ epoch 366) acc: 0.9671052631578947; ap: 0.99707812849207
(Val @ epoch 367) acc: 0.9736842105263158; ap: 0.9945645654602756
(Val @ epoch 368) acc: 0.9671052631578947; ap: 0.9962471455175135
2024_03_27_17_04_14 Train loss: 0.2751768231391907 at step: 2960 lr 5.6485907296200035e-05
(Val @ epoch 369) acc: 0.9736842105263158; ap: 0.9967576156715572
(Val @ epoch 370) acc: 0.9736842105263158; ap: 0.9967349905589044
(Val @ epoch 371) acc: 0.9802631578947368; ap: 0.9964179673088338
(Val @ epoch 372) acc: 0.9473684210526315; ap: 0.9964302268792562
(Val @ epoch 373) acc: 0.9342105263157895; ap: 0.9931617970290147
2024_03_27_17_04_23 Train loss: 0.22663281857967377 at step: 3000 lr 5.6485907296200035e-05
(Val @ epoch 374) acc: 0.9407894736842105; ap: 0.9939314913043611
(Val @ epoch 375) acc: 0.9276315789473685; ap: 0.9953186063949151
(Val @ epoch 376) acc: 0.9671052631578947; ap: 0.9961124508035641
(Val @ epoch 377) acc: 0.9605263157894737; ap: 0.9962726004473914
(Val @ epoch 378) acc: 0.9671052631578947; ap: 0.9952903657596522
2024_03_27_17_04_32 Train loss: 0.18432460725307465 at step: 3040 lr 5.6485907296200035e-05
(Val @ epoch 379) acc: 0.9671052631578947; ap: 0.994848052349007
(Val @ epoch 380) acc: 0.9539473684210527; ap: 0.99407154732677
(Val @ epoch 381) acc: 0.9736842105263158; ap: 0.9939161549806067
(Val @ epoch 382) acc: 0.9671052631578947; ap: 0.9957609198055604
(Val @ epoch 383) acc: 0.9671052631578947; ap: 0.9956811025910551
2024_03_27_17_04_42 Train loss: 0.16667261719703674 at step: 3080 lr 5.6485907296200035e-05
(Val @ epoch 384) acc: 0.9802631578947368; ap: 0.9950729993238392
(Val @ epoch 385) acc: 0.9802631578947368; ap: 0.9948776892873313
(Val @ epoch 386) acc: 0.9671052631578947; ap: 0.99656852436996
(Val @ epoch 387) acc: 0.9605263157894737; ap: 0.9967741164752233
(Val @ epoch 388) acc: 0.9539473684210527; ap: 0.9949084550562428
2024_03_27_17_04_51 Train loss: 0.6803584098815918 at step: 3120 lr 5.6485907296200035e-05
(Val @ epoch 389) acc: 0.9802631578947368; ap: 0.9950710675140382
2024_03_27_17_04_53 changing lr at the end of epoch 390, iters 3128
*************************
Changing lr from 5.6485907296200035e-05 to 5.083731656658003e-05
*************************
(Val @ epoch 390) acc: 0.9802631578947368; ap: 0.9943377052061921
(Val @ epoch 391) acc: 0.9736842105263158; ap: 0.9947376332649224
(Val @ epoch 392) acc: 0.9671052631578947; ap: 0.9954908244892969
(Val @ epoch 393) acc: 0.9736842105263158; ap: 0.996535628517437
2024_03_27_17_05_00 Train loss: 0.2071729153394699 at step: 3160 lr 5.083731656658003e-05
(Val @ epoch 394) acc: 0.9671052631578947; ap: 0.9959968877281618
(Val @ epoch 395) acc: 0.9802631578947368; ap: 0.9954908244892969
(Val @ epoch 396) acc: 0.9802631578947368; ap: 0.99656852436996
(Val @ epoch 397) acc: 0.9671052631578947; ap: 0.9963977025786397
(Val @ epoch 398) acc: 0.9539473684210527; ap: 0.9969158435196585
2024_03_27_17_05_09 Train loss: 0.8491373062133789 at step: 3200 lr 5.083731656658003e-05
(Val @ epoch 399) acc: 0.9473684210526315; ap: 0.9946966611349909
(Val @ epoch 400) acc: 0.9473684210526315; ap: 0.9955416215599463
(Val @ epoch 401) acc: 0.9736842105263158; ap: 0.9970478351364348
(Val @ epoch 402) acc: 0.9342105263157895; ap: 0.996652086606358
(Val @ epoch 403) acc: 0.9736842105263158; ap: 0.994186313992176
2024_03_27_17_05_18 Train loss: 0.04928641393780708 at step: 3240 lr 5.083731656658003e-05
(Val @ epoch 404) acc: 0.9671052631578947; ap: 0.9945645654602756
(Val @ epoch 405) acc: 0.9736842105263158; ap: 0.9951799466755676
(Val @ epoch 406) acc: 0.9671052631578947; ap: 0.9949606484299536
(Val @ epoch 407) acc: 0.9736842105263158; ap: 0.9945733238900867
(Val @ epoch 408) acc: 0.9605263157894737; ap: 0.9958507523343585
2024_03_27_17_05_28 Train loss: 0.13494807481765747 at step: 3280 lr 5.083731656658003e-05
(Val @ epoch 409) acc: 0.9671052631578947; ap: 0.9964179673088338
(Val @ epoch 410) acc: 0.9539473684210527; ap: 0.9963977025786397
(Val @ epoch 411) acc: 0.9736842105263158; ap: 0.9942149781930925
(Val @ epoch 412) acc: 0.9407894736842105; ap: 0.9946508290332479
(Val @ epoch 413) acc: 0.9539473684210527; ap: 0.9956856491731908
2024_03_27_17_05_37 Train loss: 0.03523401916027069 at step: 3320 lr 5.083731656658003e-05
(Val @ epoch 414) acc: 0.9736842105263158; ap: 0.996513989885601
(Val @ epoch 415) acc: 0.9671052631578947; ap: 0.9955618862901405
(Val @ epoch 416) acc: 0.9539473684210527; ap: 0.9954472538738685
(Val @ epoch 417) acc: 0.9736842105263158; ap: 0.9966098071003875
(Val @ epoch 418) acc: 0.9473684210526315; ap: 0.9970866653109788
2024_03_27_17_05_46 Train loss: 0.20249390602111816 at step: 3360 lr 5.083731656658003e-05
(Val @ epoch 419) acc: 0.9539473684210527; ap: 0.9948609058822905
2024_03_27_17_05_48 changing lr at the end of epoch 420, iters 3368
*************************
Changing lr from 5.083731656658003e-05 to 4.575358490992203e-05
*************************
(Val @ epoch 420) acc: 0.9802631578947368; ap: 0.9951455385902687
(Val @ epoch 421) acc: 0.9671052631578947; ap: 0.994638002047453
(Val @ epoch 422) acc: 0.9342105263157895; ap: 0.9942805252834427
(Val @ epoch 423) acc: 0.9407894736842105; ap: 0.9950531672305744
2024_03_27_17_05_55 Train loss: 0.03114216774702072 at step: 3400 lr 4.575358490992203e-05
(Val @ epoch 424) acc: 0.9671052631578947; ap: 0.9956978625352472
(Val @ epoch 425) acc: 0.9342105263157895; ap: 0.996361776650983
(Val @ epoch 426) acc: 0.9802631578947368; ap: 0.9945447958965505
(Val @ epoch 427) acc: 0.9802631578947368; ap: 0.994143133846689
(Val @ epoch 428) acc: 0.9671052631578947; ap: 0.9956864429595228
2024_03_27_17_06_04 Train loss: 1.1495802402496338 at step: 3440 lr 4.575358490992203e-05
(Val @ epoch 429) acc: 0.9802631578947368; ap: 0.995778695767954
(Val @ epoch 430) acc: 0.9671052631578947; ap: 0.995778695767954
(Val @ epoch 431) acc: 0.9736842105263158; ap: 0.9949276366477158
(Val @ epoch 432) acc: 0.9802631578947368; ap: 0.9944199887747067
(Val @ epoch 433) acc: 0.9802631578947368; ap: 0.9958260659368414
2024_03_27_17_06_13 Train loss: 0.1092904582619667 at step: 3480 lr 4.575358490992203e-05
(Val @ epoch 434) acc: 0.9802631578947368; ap: 0.9945107730108389
(Val @ epoch 435) acc: 0.9736842105263158; ap: 0.9959875512399673
(Val @ epoch 436) acc: 0.9736842105263158; ap: 0.9961276072623763
(Val @ epoch 437) acc: 0.9736842105263158; ap: 0.9949946713156652
(Val @ epoch 438) acc: 0.9736842105263158; ap: 0.9944507545436183
2024_03_27_17_06_22 Train loss: 0.10286852717399597 at step: 3520 lr 4.575358490992203e-05
(Val @ epoch 439) acc: 0.9736842105263158; ap: 0.9945107730108389
(Val @ epoch 440) acc: 0.9671052631578947; ap: 0.9955357058903839
(Val @ epoch 441) acc: 0.9736842105263158; ap: 0.9943377052061921
(Val @ epoch 442) acc: 0.9802631578947368; ap: 0.9942799327522978
(Val @ epoch 443) acc: 0.9802631578947368; ap: 0.996130943985622
2024_03_27_17_06_32 Train loss: 0.029943842440843582 at step: 3560 lr 4.575358490992203e-05
(Val @ epoch 444) acc: 0.9736842105263158; ap: 0.9967228453576144
(Val @ epoch 445) acc: 0.9802631578947368; ap: 0.9956959425476006
(Val @ epoch 446) acc: 0.9605263157894737; ap: 0.9964496574565602
(Val @ epoch 447) acc: 0.9671052631578947; ap: 0.9966098071003875
(Val @ epoch 448) acc: 0.9671052631578947; ap: 0.995832578353326
2024_03_27_17_06_41 Train loss: 0.11278001964092255 at step: 3600 lr 4.575358490992203e-05
(Val @ epoch 449) acc: 0.9736842105263158; ap: 0.9946508290332479
2024_03_27_17_06_43 changing lr at the end of epoch 450, iters 3608
*************************
Changing lr from 4.575358490992203e-05 to 4.117822641892983e-05
*************************
(Val @ epoch 450) acc: 0.9736842105263158; ap: 0.9949186783361849
(Val @ epoch 451) acc: 0.9802631578947368; ap: 0.9959152407932147
(Val @ epoch 452) acc: 0.9671052631578947; ap: 0.9959766229979676
(Val @ epoch 453) acc: 0.9539473684210527; ap: 0.9958433875634667
2024_03_27_17_06_50 Train loss: 0.3320348262786865 at step: 3640 lr 4.117822641892983e-05
(Val @ epoch 454) acc: 0.9736842105263158; ap: 0.9948776892873313
(Val @ epoch 455) acc: 0.9802631578947368; ap: 0.9958035551933209
(Val @ epoch 456) acc: 0.9736842105263158; ap: 0.9953685537552995
(Val @ epoch 457) acc: 0.9671052631578947; ap: 0.9954886887573589
(Val @ epoch 458) acc: 0.9736842105263158; ap: 0.9943903518363825
2024_03_27_17_06_59 Train loss: 0.01802731864154339 at step: 3680 lr 4.117822641892983e-05
(Val @ epoch 459) acc: 0.9671052631578947; ap: 0.994848052349007
(Val @ epoch 460) acc: 0.9736842105263158; ap: 0.9954460533695985
(Val @ epoch 461) acc: 0.9671052631578947; ap: 0.9956103627444342
(Val @ epoch 462) acc: 0.9671052631578947; ap: 0.9947942598995703
(Val @ epoch 463) acc: 0.9736842105263158; ap: 0.995463433564299
2024_03_27_17_07_09 Train loss: 0.3171495795249939 at step: 3720 lr 4.117822641892983e-05
(Val @ epoch 464) acc: 0.9802631578947368; ap: 0.9951007044523625
(Val @ epoch 465) acc: 0.9802631578947368; ap: 0.9956103627444342
(Val @ epoch 466) acc: 0.9671052631578947; ap: 0.9950729993238392
(Val @ epoch 467) acc: 0.9473684210526315; ap: 0.9889230233773598
(Val @ epoch 468) acc: 0.9210526315789473; ap: 0.9905704877872421
2024_03_27_17_07_18 Train loss: 0.2805573046207428 at step: 3760 lr 4.117822641892983e-05
(Val @ epoch 469) acc: 0.9144736842105263; ap: 0.9953879843326039
(Val @ epoch 470) acc: 0.9671052631578947; ap: 0.9959766229979676
(Val @ epoch 471) acc: 0.9671052631578947; ap: 0.9952441353186849
(Val @ epoch 472) acc: 0.9802631578947368; ap: 0.9951680493337889
(Val @ epoch 473) acc: 0.9671052631578947; ap: 0.9959317415968807
2024_03_27_17_07_27 Train loss: 0.11542615294456482 at step: 3800 lr 4.117822641892983e-05
(Val @ epoch 474) acc: 0.9736842105263158; ap: 0.9964042149951243
(Val @ epoch 475) acc: 0.9802631578947368; ap: 0.9963596688979467
(Val @ epoch 476) acc: 0.9671052631578947; ap: 0.9957609198055604
(Val @ epoch 477) acc: 0.9736842105263158; ap: 0.996130943985622
(Val @ epoch 478) acc: 0.9802631578947368; ap: 0.996130943985622
2024_03_27_17_07_36 Train loss: 0.32229626178741455 at step: 3840 lr 4.117822641892983e-05
(Val @ epoch 479) acc: 0.9802631578947368; ap: 0.9967641280880417
2024_03_27_17_07_38 changing lr at the end of epoch 480, iters 3848
*************************
Changing lr from 4.117822641892983e-05 to 3.7060403777036845e-05
*************************
(Val @ epoch 480) acc: 0.9736842105263158; ap: 0.99656852436996
(Val @ epoch 481) acc: 0.9671052631578947; ap: 0.9959968877281618
(Val @ epoch 482) acc: 0.9605263157894737; ap: 0.996147444789288
(Val @ epoch 483) acc: 0.9605263157894737; ap: 0.9955436550141645
2024_03_27_17_07_45 Train loss: 1.08349609375 at step: 3880 lr 3.7060403777036845e-05
(Val @ epoch 484) acc: 0.9407894736842105; ap: 0.9968220296833397
(Val @ epoch 485) acc: 0.9736842105263158; ap: 0.9955357058903839
(Val @ epoch 486) acc: 0.9671052631578947; ap: 0.9945107730108389
(Val @ epoch 487) acc: 0.9736842105263158; ap: 0.9958978732818496
(Val @ epoch 488) acc: 0.9736842105263158; ap: 0.9950211201536537
2024_03_27_17_07_54 Train loss: 0.6722552180290222 at step: 3920 lr 3.7060403777036845e-05
(Val @ epoch 489) acc: 0.9736842105263158; ap: 0.9942469209700601
(Val @ epoch 490) acc: 0.9671052631578947; ap: 0.9959317415968807
(Val @ epoch 491) acc: 0.9802631578947368; ap: 0.9959766229979676
(Val @ epoch 492) acc: 0.9736842105263158; ap: 0.9957609198055604
(Val @ epoch 493) acc: 0.9671052631578947; ap: 0.9963017657769423
2024_03_27_17_08_04 Train loss: 0.09305984526872635 at step: 3960 lr 3.7060403777036845e-05
(Val @ epoch 494) acc: 0.9671052631578947; ap: 0.996513989885601
(Val @ epoch 495) acc: 0.9736842105263158; ap: 0.9965585359827787
(Val @ epoch 496) acc: 0.9671052631578947; ap: 0.9967641280880417
(Val @ epoch 497) acc: 0.9605263157894737; ap: 0.9967741164752233
(Val @ epoch 498) acc: 0.9671052631578947; ap: 0.9962091118368205
2024_03_27_17_08_13 Train loss: 0.13575991988182068 at step: 4000 lr 3.7060403777036845e-05
(Val @ epoch 499) acc: 0.9736842105263158; ap: 0.9959766229979676
*************************
2024_03_27_17_08_14
(0 FastGan   ) acc: 96.4; ap: 99.8
(1 StyleGAN_ADA) acc: 95.1; ap: 99.1
(2 Mean      ) acc: 95.8; ap: 99.5
*************************
2024_03_27_17_08_15
Saving model ./checkpoints/resnet10_60per2024_03_27_16_52_53/model_epoch_last.pth
