train.py  --name  resnet10_60per  --dataroot  /home/ashish/detect_dataset/splitted_data_60/  --classes  FastGan,StyleGAN_ADA  --batch_size  128  --delr_freq  30  --lr  0.0002  --niter  500
Model size: 5113.00000
cwd: /home/ashish/NPR-DeepfakeDetectionResnet18
(Val @ epoch 0) acc: 0.5; ap: 0.4159395385199444
(Val @ epoch 1) acc: 0.5; ap: 0.41310241311044826
(Val @ epoch 2) acc: 0.5; ap: 0.4152834966146779
(Val @ epoch 3) acc: 0.5; ap: 0.42284661722298267
2024_03_27_23_23_36 Train loss: 0.7440948486328125 at step: 40 lr 0.0002
(Val @ epoch 4) acc: 0.5; ap: 0.5110487667474022
(Val @ epoch 5) acc: 0.506578947368421; ap: 0.5767724196022963
(Val @ epoch 6) acc: 0.5131578947368421; ap: 0.5944827887292446
(Val @ epoch 7) acc: 0.5197368421052632; ap: 0.5978592798290945
(Val @ epoch 8) acc: 0.5657894736842105; ap: 0.5808614173946239
2024_03_27_23_23_45 Train loss: 0.7481922507286072 at step: 80 lr 0.0002
(Val @ epoch 9) acc: 0.5855263157894737; ap: 0.5842221790162111
(Val @ epoch 10) acc: 0.5723684210526315; ap: 0.5892135052440408
(Val @ epoch 11) acc: 0.5723684210526315; ap: 0.5949271052043784
(Val @ epoch 12) acc: 0.5855263157894737; ap: 0.5988492625490689
(Val @ epoch 13) acc: 0.5921052631578947; ap: 0.6078506726236431
2024_03_27_23_23_54 Train loss: 0.6646275520324707 at step: 120 lr 0.0002
(Val @ epoch 14) acc: 0.5855263157894737; ap: 0.6107788683236823
(Val @ epoch 15) acc: 0.6118421052631579; ap: 0.6175975080875016
(Val @ epoch 16) acc: 0.5986842105263158; ap: 0.613400238287035
(Val @ epoch 17) acc: 0.5855263157894737; ap: 0.6218217186835345
(Val @ epoch 18) acc: 0.5789473684210527; ap: 0.6345244191242447
2024_03_27_23_24_03 Train loss: 0.5741356611251831 at step: 160 lr 0.0002
(Val @ epoch 19) acc: 0.5723684210526315; ap: 0.6348421573732571
(Val @ epoch 20) acc: 0.6052631578947368; ap: 0.6335420076125509
(Val @ epoch 21) acc: 0.6118421052631579; ap: 0.6362486920536722
(Val @ epoch 22) acc: 0.6052631578947368; ap: 0.6425901994202068
(Val @ epoch 23) acc: 0.5789473684210527; ap: 0.6498293987538418
2024_03_27_23_24_12 Train loss: 0.7576649188995361 at step: 200 lr 0.0002
(Val @ epoch 24) acc: 0.5789473684210527; ap: 0.6630570097072703
(Val @ epoch 25) acc: 0.5657894736842105; ap: 0.661415719724711
(Val @ epoch 26) acc: 0.5657894736842105; ap: 0.669229526330695
(Val @ epoch 27) acc: 0.5723684210526315; ap: 0.6606681313559867
(Val @ epoch 28) acc: 0.5723684210526315; ap: 0.6557180134756782
2024_03_27_23_24_21 Train loss: 0.6544457674026489 at step: 240 lr 0.0002
(Val @ epoch 29) acc: 0.5986842105263158; ap: 0.6700987068140076
2024_03_27_23_24_23 changing lr at the end of epoch 30, iters 248
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 30) acc: 0.6381578947368421; ap: 0.6848552169902935
(Val @ epoch 31) acc: 0.5986842105263158; ap: 0.6734008571858356
(Val @ epoch 32) acc: 0.5986842105263158; ap: 0.6723398219565178
(Val @ epoch 33) acc: 0.618421052631579; ap: 0.680914390835895
2024_03_27_23_24_30 Train loss: 0.6110996603965759 at step: 280 lr 0.00018
(Val @ epoch 34) acc: 0.6447368421052632; ap: 0.6908260578557265
(Val @ epoch 35) acc: 0.625; ap: 0.6891763818560778
(Val @ epoch 36) acc: 0.6381578947368421; ap: 0.6990601459888728
(Val @ epoch 37) acc: 0.625; ap: 0.7130638646526017
(Val @ epoch 38) acc: 0.6447368421052632; ap: 0.7162538147251152
2024_03_27_23_24_39 Train loss: 0.7341920137405396 at step: 320 lr 0.00018
(Val @ epoch 39) acc: 0.6381578947368421; ap: 0.7117897407247278
(Val @ epoch 40) acc: 0.631578947368421; ap: 0.7083354558927311
(Val @ epoch 41) acc: 0.625; ap: 0.6956265276891187
(Val @ epoch 42) acc: 0.631578947368421; ap: 0.6966930256608175
(Val @ epoch 43) acc: 0.6447368421052632; ap: 0.7482796995905674
2024_03_27_23_24_48 Train loss: 0.6587537527084351 at step: 360 lr 0.00018
(Val @ epoch 44) acc: 0.6644736842105263; ap: 0.7613431039478807
(Val @ epoch 45) acc: 0.6710526315789473; ap: 0.7362995089110005
(Val @ epoch 46) acc: 0.6710526315789473; ap: 0.7519768646356358
(Val @ epoch 47) acc: 0.6907894736842105; ap: 0.7421307991410755
(Val @ epoch 48) acc: 0.6842105263157895; ap: 0.7647199138469567
2024_03_27_23_24_57 Train loss: 0.652810275554657 at step: 400 lr 0.00018
(Val @ epoch 49) acc: 0.6710526315789473; ap: 0.7662000710779581
(Val @ epoch 50) acc: 0.6842105263157895; ap: 0.7690406619160506
(Val @ epoch 51) acc: 0.6578947368421053; ap: 0.7487201268094925
(Val @ epoch 52) acc: 0.6907894736842105; ap: 0.797940778926125
(Val @ epoch 53) acc: 0.6973684210526315; ap: 0.8083857139222119
2024_03_27_23_25_06 Train loss: 0.5834065675735474 at step: 440 lr 0.00018
(Val @ epoch 54) acc: 0.6907894736842105; ap: 0.786395381116477
(Val @ epoch 55) acc: 0.6842105263157895; ap: 0.7811843234740202
(Val @ epoch 56) acc: 0.6973684210526315; ap: 0.7994346645949426
(Val @ epoch 57) acc: 0.6907894736842105; ap: 0.7812849165053699
(Val @ epoch 58) acc: 0.7302631578947368; ap: 0.8081660032009859
2024_03_27_23_25_15 Train loss: 0.5225425362586975 at step: 480 lr 0.00018
(Val @ epoch 59) acc: 0.7171052631578947; ap: 0.8046124290205089
2024_03_27_23_25_16 changing lr at the end of epoch 60, iters 488
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 60) acc: 0.7105263157894737; ap: 0.815822353157236
(Val @ epoch 61) acc: 0.6973684210526315; ap: 0.7975226435968641
(Val @ epoch 62) acc: 0.7171052631578947; ap: 0.8167033744666365
(Val @ epoch 63) acc: 0.75; ap: 0.839638138904718
2024_03_27_23_25_24 Train loss: 0.5058783292770386 at step: 520 lr 0.000162
(Val @ epoch 64) acc: 0.7039473684210527; ap: 0.8416455414382293
(Val @ epoch 65) acc: 0.6842105263157895; ap: 0.8226858523987058
(Val @ epoch 66) acc: 0.6973684210526315; ap: 0.8101093516596848
(Val @ epoch 67) acc: 0.743421052631579; ap: 0.829867885588239
(Val @ epoch 68) acc: 0.7105263157894737; ap: 0.8383703726327008
2024_03_27_23_25_33 Train loss: 0.6839661598205566 at step: 560 lr 0.000162
(Val @ epoch 69) acc: 0.6973684210526315; ap: 0.8275227097746154
(Val @ epoch 70) acc: 0.6776315789473685; ap: 0.806101454110747
(Val @ epoch 71) acc: 0.7368421052631579; ap: 0.8472808831734405
(Val @ epoch 72) acc: 0.75; ap: 0.862956540315978
(Val @ epoch 73) acc: 0.7236842105263158; ap: 0.8546642722274911
2024_03_27_23_25_41 Train loss: 0.5919488072395325 at step: 600 lr 0.000162
(Val @ epoch 74) acc: 0.5986842105263158; ap: 0.8040659738871021
(Val @ epoch 75) acc: 0.743421052631579; ap: 0.8217627037310469
(Val @ epoch 76) acc: 0.7368421052631579; ap: 0.8555657517856169
(Val @ epoch 77) acc: 0.7302631578947368; ap: 0.8334616716418951
(Val @ epoch 78) acc: 0.75; ap: 0.8549861610308361
2024_03_27_23_25_50 Train loss: 0.5214514136314392 at step: 640 lr 0.000162
(Val @ epoch 79) acc: 0.7631578947368421; ap: 0.8675241045532279
(Val @ epoch 80) acc: 0.7631578947368421; ap: 0.8540511299439928
(Val @ epoch 81) acc: 0.7697368421052632; ap: 0.8603663016552421
(Val @ epoch 82) acc: 0.7763157894736842; ap: 0.8375535942780208
(Val @ epoch 83) acc: 0.7894736842105263; ap: 0.8648947880675446
2024_03_27_23_25_59 Train loss: 0.6534488201141357 at step: 680 lr 0.000162
(Val @ epoch 84) acc: 0.7960526315789473; ap: 0.8776970058587028
(Val @ epoch 85) acc: 0.756578947368421; ap: 0.8658293243779938
(Val @ epoch 86) acc: 0.7697368421052632; ap: 0.8571074363803067
(Val @ epoch 87) acc: 0.8026315789473685; ap: 0.8770507425863847
(Val @ epoch 88) acc: 0.7763157894736842; ap: 0.8664738331732178
2024_03_27_23_26_08 Train loss: 0.5379617214202881 at step: 720 lr 0.000162
(Val @ epoch 89) acc: 0.8092105263157895; ap: 0.8752828326094982
2024_03_27_23_26_10 changing lr at the end of epoch 90, iters 728
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 90) acc: 0.7894736842105263; ap: 0.8918210731568816
(Val @ epoch 91) acc: 0.756578947368421; ap: 0.8722324166646948
(Val @ epoch 92) acc: 0.8223684210526315; ap: 0.8904796683406663
(Val @ epoch 93) acc: 0.7894736842105263; ap: 0.9036749352981878
2024_03_27_23_26_17 Train loss: 0.5990341305732727 at step: 760 lr 0.00014580000000000002
(Val @ epoch 94) acc: 0.8026315789473685; ap: 0.9043437041307607
(Val @ epoch 95) acc: 0.7960526315789473; ap: 0.9068991033988046
(Val @ epoch 96) acc: 0.7828947368421053; ap: 0.9080471695083627
(Val @ epoch 97) acc: 0.7631578947368421; ap: 0.900919963381695
(Val @ epoch 98) acc: 0.7763157894736842; ap: 0.900436946187374
2024_03_27_23_26_26 Train loss: 0.7520111799240112 at step: 800 lr 0.00014580000000000002
(Val @ epoch 99) acc: 0.8157894736842105; ap: 0.8878900969184255
(Val @ epoch 100) acc: 0.8223684210526315; ap: 0.9038806307497168
(Val @ epoch 101) acc: 0.8289473684210527; ap: 0.9025323392971987
(Val @ epoch 102) acc: 0.8157894736842105; ap: 0.9064780118470669
(Val @ epoch 103) acc: 0.8026315789473685; ap: 0.8927075596051058
2024_03_27_23_26_35 Train loss: 0.5037164688110352 at step: 840 lr 0.00014580000000000002
(Val @ epoch 104) acc: 0.7763157894736842; ap: 0.886153672221492
(Val @ epoch 105) acc: 0.8026315789473685; ap: 0.885415885133687
(Val @ epoch 106) acc: 0.8157894736842105; ap: 0.8909203548052772
(Val @ epoch 107) acc: 0.8092105263157895; ap: 0.9025200241604056
(Val @ epoch 108) acc: 0.8421052631578947; ap: 0.9074537535571481
2024_03_27_23_26_44 Train loss: 0.38318485021591187 at step: 880 lr 0.00014580000000000002
(Val @ epoch 109) acc: 0.7828947368421053; ap: 0.8789152996072754
(Val @ epoch 110) acc: 0.8157894736842105; ap: 0.8994178411686903
(Val @ epoch 111) acc: 0.8157894736842105; ap: 0.9073139647912054
(Val @ epoch 112) acc: 0.8092105263157895; ap: 0.8959251346605082
(Val @ epoch 113) acc: 0.8355263157894737; ap: 0.8909169133960274
2024_03_27_23_26_52 Train loss: 0.7573870420455933 at step: 920 lr 0.00014580000000000002
(Val @ epoch 114) acc: 0.8092105263157895; ap: 0.9163960619466807
(Val @ epoch 115) acc: 0.8355263157894737; ap: 0.9157203389536214
(Val @ epoch 116) acc: 0.8289473684210527; ap: 0.9041470157473095
(Val @ epoch 117) acc: 0.8157894736842105; ap: 0.9205579112497099
(Val @ epoch 118) acc: 0.8618421052631579; ap: 0.9165149303475526
2024_03_27_23_27_01 Train loss: 0.35733526945114136 at step: 960 lr 0.00014580000000000002
(Val @ epoch 119) acc: 0.868421052631579; ap: 0.916048208193751
2024_03_27_23_27_03 changing lr at the end of epoch 120, iters 968
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 120) acc: 0.8618421052631579; ap: 0.9160567419942467
(Val @ epoch 121) acc: 0.8157894736842105; ap: 0.9226793089072207
(Val @ epoch 122) acc: 0.868421052631579; ap: 0.9212613239308095
(Val @ epoch 123) acc: 0.8552631578947368; ap: 0.9310274380601017
2024_03_27_23_27_10 Train loss: 0.35966163873672485 at step: 1000 lr 0.00013122000000000003
(Val @ epoch 124) acc: 0.868421052631579; ap: 0.9268691862521743
(Val @ epoch 125) acc: 0.8486842105263158; ap: 0.9243476332467399
(Val @ epoch 126) acc: 0.8092105263157895; ap: 0.9211463002672611
(Val @ epoch 127) acc: 0.8486842105263158; ap: 0.9212033353204387
(Val @ epoch 128) acc: 0.8618421052631579; ap: 0.9252258935773929
2024_03_27_23_27_19 Train loss: 0.47042393684387207 at step: 1040 lr 0.00013122000000000003
(Val @ epoch 129) acc: 0.8618421052631579; ap: 0.9147875802357052
(Val @ epoch 130) acc: 0.8223684210526315; ap: 0.9185590508408362
(Val @ epoch 131) acc: 0.875; ap: 0.9322931435479336
(Val @ epoch 132) acc: 0.8355263157894737; ap: 0.9179384097023549
(Val @ epoch 133) acc: 0.7763157894736842; ap: 0.9261569023051792
2024_03_27_23_27_28 Train loss: 0.5850369334220886 at step: 1080 lr 0.00013122000000000003
(Val @ epoch 134) acc: 0.7828947368421053; ap: 0.9359871316118857
(Val @ epoch 135) acc: 0.8618421052631579; ap: 0.9252712488118809
(Val @ epoch 136) acc: 0.875; ap: 0.9195888580203594
(Val @ epoch 137) acc: 0.8421052631578947; ap: 0.9140130747159605
(Val @ epoch 138) acc: 0.8421052631578947; ap: 0.9026386154799194
2024_03_27_23_27_36 Train loss: 0.5867446660995483 at step: 1120 lr 0.00013122000000000003
(Val @ epoch 139) acc: 0.8157894736842105; ap: 0.8974631950815445
(Val @ epoch 140) acc: 0.8092105263157895; ap: 0.9159517233820644
(Val @ epoch 141) acc: 0.8421052631578947; ap: 0.9241878154112885
(Val @ epoch 142) acc: 0.8289473684210527; ap: 0.9160882018200992
(Val @ epoch 143) acc: 0.8223684210526315; ap: 0.9338050914010224
2024_03_27_23_27_45 Train loss: 0.5062583088874817 at step: 1160 lr 0.00013122000000000003
(Val @ epoch 144) acc: 0.868421052631579; ap: 0.9364932058909826
(Val @ epoch 145) acc: 0.868421052631579; ap: 0.9274160806381683
(Val @ epoch 146) acc: 0.8552631578947368; ap: 0.930278906734603
(Val @ epoch 147) acc: 0.8552631578947368; ap: 0.9236317156140358
(Val @ epoch 148) acc: 0.8421052631578947; ap: 0.926821254453734
2024_03_27_23_27_54 Train loss: 0.40968459844589233 at step: 1200 lr 0.00013122000000000003
(Val @ epoch 149) acc: 0.8618421052631579; ap: 0.9286970442541626
2024_03_27_23_27_56 changing lr at the end of epoch 150, iters 1208
*************************
Changing lr from 0.00013122000000000003 to 0.00011809800000000003
*************************
(Val @ epoch 150) acc: 0.8092105263157895; ap: 0.9232518878882607
(Val @ epoch 151) acc: 0.8355263157894737; ap: 0.9121163317049844
(Val @ epoch 152) acc: 0.8289473684210527; ap: 0.9322960561475955
(Val @ epoch 153) acc: 0.868421052631579; ap: 0.9297235526223342
2024_03_27_23_28_03 Train loss: 0.2991642355918884 at step: 1240 lr 0.00011809800000000003
(Val @ epoch 154) acc: 0.8552631578947368; ap: 0.9263618402885908
(Val @ epoch 155) acc: 0.8486842105263158; ap: 0.9294399290458739
(Val @ epoch 156) acc: 0.8618421052631579; ap: 0.9445407458326163
(Val @ epoch 157) acc: 0.8881578947368421; ap: 0.9393225515055472
(Val @ epoch 158) acc: 0.875; ap: 0.9326675734853629
2024_03_27_23_28_12 Train loss: 0.3755197525024414 at step: 1280 lr 0.00011809800000000003
(Val @ epoch 159) acc: 0.875; ap: 0.9437330299234059
(Val @ epoch 160) acc: 0.8486842105263158; ap: 0.9417315365196182
(Val @ epoch 161) acc: 0.8355263157894737; ap: 0.9319998612494778
(Val @ epoch 162) acc: 0.8486842105263158; ap: 0.9426801432501255
(Val @ epoch 163) acc: 0.8289473684210527; ap: 0.9371777119789657
2024_03_27_23_28_21 Train loss: 0.29671216011047363 at step: 1320 lr 0.00011809800000000003
(Val @ epoch 164) acc: 0.875; ap: 0.9410572884675263
(Val @ epoch 165) acc: 0.881578947368421; ap: 0.9447452322572849
(Val @ epoch 166) acc: 0.875; ap: 0.9411620519778869
(Val @ epoch 167) acc: 0.875; ap: 0.9442520257259048
(Val @ epoch 168) acc: 0.875; ap: 0.9436134254906667
2024_03_27_23_28_30 Train loss: 0.47170814871788025 at step: 1360 lr 0.00011809800000000003
(Val @ epoch 169) acc: 0.8421052631578947; ap: 0.9332213285945978
(Val @ epoch 170) acc: 0.8552631578947368; ap: 0.9412297039614272
(Val @ epoch 171) acc: 0.8881578947368421; ap: 0.9451005426373157
(Val @ epoch 172) acc: 0.8618421052631579; ap: 0.9403650102615015
(Val @ epoch 173) acc: 0.8355263157894737; ap: 0.9379667356272711
2024_03_27_23_28_38 Train loss: 0.354356050491333 at step: 1400 lr 0.00011809800000000003
(Val @ epoch 174) acc: 0.8618421052631579; ap: 0.9457517097265478
(Val @ epoch 175) acc: 0.8486842105263158; ap: 0.9386000398423266
(Val @ epoch 176) acc: 0.8486842105263158; ap: 0.9355583344473407
(Val @ epoch 177) acc: 0.8355263157894737; ap: 0.9441513872367868
(Val @ epoch 178) acc: 0.881578947368421; ap: 0.9432321144875152
2024_03_27_23_28_47 Train loss: 0.34535467624664307 at step: 1440 lr 0.00011809800000000003
(Val @ epoch 179) acc: 0.875; ap: 0.9438120441113261
2024_03_27_23_28_49 changing lr at the end of epoch 180, iters 1448
*************************
Changing lr from 0.00011809800000000003 to 0.00010628820000000004
*************************
(Val @ epoch 180) acc: 0.875; ap: 0.947664388945577
(Val @ epoch 181) acc: 0.9078947368421053; ap: 0.951541273307518
(Val @ epoch 182) acc: 0.8289473684210527; ap: 0.9456256895033847
(Val @ epoch 183) acc: 0.881578947368421; ap: 0.9390727629224134
2024_03_27_23_28_56 Train loss: 0.3919623792171478 at step: 1480 lr 0.00010628820000000004
(Val @ epoch 184) acc: 0.875; ap: 0.9492811376985301
(Val @ epoch 185) acc: 0.8421052631578947; ap: 0.9483058583198083
(Val @ epoch 186) acc: 0.8552631578947368; ap: 0.9392876269562855
(Val @ epoch 187) acc: 0.875; ap: 0.9417301007947586
(Val @ epoch 188) acc: 0.8289473684210527; ap: 0.9486762044071473
2024_03_27_23_29_05 Train loss: 0.31000304222106934 at step: 1520 lr 0.00010628820000000004
(Val @ epoch 189) acc: 0.8486842105263158; ap: 0.9553997455124758
(Val @ epoch 190) acc: 0.881578947368421; ap: 0.9461383032229886
(Val @ epoch 191) acc: 0.8618421052631579; ap: 0.9448041125631601
(Val @ epoch 192) acc: 0.8618421052631579; ap: 0.9519596799298524
(Val @ epoch 193) acc: 0.8881578947368421; ap: 0.9527696274544125
2024_03_27_23_29_13 Train loss: 0.4543791115283966 at step: 1560 lr 0.00010628820000000004
(Val @ epoch 194) acc: 0.875; ap: 0.9501859714676659
(Val @ epoch 195) acc: 0.8881578947368421; ap: 0.9559847848200792
(Val @ epoch 196) acc: 0.868421052631579; ap: 0.9494778694053518
(Val @ epoch 197) acc: 0.881578947368421; ap: 0.9515697141514364
(Val @ epoch 198) acc: 0.868421052631579; ap: 0.9583057487010569
2024_03_27_23_29_23 Train loss: 0.7946529984474182 at step: 1600 lr 0.00010628820000000004
(Val @ epoch 199) acc: 0.9078947368421053; ap: 0.9598451128975476
(Val @ epoch 200) acc: 0.8881578947368421; ap: 0.9577721238250537
(Val @ epoch 201) acc: 0.9078947368421053; ap: 0.9583678400971618
(Val @ epoch 202) acc: 0.875; ap: 0.9547367886707998
(Val @ epoch 203) acc: 0.881578947368421; ap: 0.9484727747076538
2024_03_27_23_29_31 Train loss: 0.35954052209854126 at step: 1640 lr 0.00010628820000000004
(Val @ epoch 204) acc: 0.8552631578947368; ap: 0.953966517835217
(Val @ epoch 205) acc: 0.875; ap: 0.95683442007861
(Val @ epoch 206) acc: 0.9078947368421053; ap: 0.9570404680645964
(Val @ epoch 207) acc: 0.8881578947368421; ap: 0.9575361585391783
(Val @ epoch 208) acc: 0.8947368421052632; ap: 0.95799003639925
2024_03_27_23_29_40 Train loss: 0.8914955854415894 at step: 1680 lr 0.00010628820000000004
(Val @ epoch 209) acc: 0.868421052631579; ap: 0.958006075135954
2024_03_27_23_29_42 changing lr at the end of epoch 210, iters 1688
*************************
Changing lr from 0.00010628820000000004 to 9.565938000000004e-05
*************************
(Val @ epoch 210) acc: 0.8947368421052632; ap: 0.9559526138808558
(Val @ epoch 211) acc: 0.9210526315789473; ap: 0.9605660184431818
(Val @ epoch 212) acc: 0.8618421052631579; ap: 0.9573507465697266
(Val @ epoch 213) acc: 0.8947368421052632; ap: 0.9564056212672448
2024_03_27_23_29_49 Train loss: 0.552972137928009 at step: 1720 lr 9.565938000000004e-05
(Val @ epoch 214) acc: 0.9078947368421053; ap: 0.9588508095509187
(Val @ epoch 215) acc: 0.8947368421052632; ap: 0.9594041193839352
(Val @ epoch 216) acc: 0.9078947368421053; ap: 0.9602124288284525
(Val @ epoch 217) acc: 0.9013157894736842; ap: 0.9593866469628018
(Val @ epoch 218) acc: 0.8947368421052632; ap: 0.9588590185548517
2024_03_27_23_29_58 Train loss: 0.6246341466903687 at step: 1760 lr 9.565938000000004e-05
(Val @ epoch 219) acc: 0.8881578947368421; ap: 0.9600251569636862
(Val @ epoch 220) acc: 0.8947368421052632; ap: 0.9607696845777678
(Val @ epoch 221) acc: 0.875; ap: 0.95752409066328
(Val @ epoch 222) acc: 0.8947368421052632; ap: 0.9523593045148628
(Val @ epoch 223) acc: 0.9144736842105263; ap: 0.9583966233849901
2024_03_27_23_30_07 Train loss: 0.6196892857551575 at step: 1800 lr 9.565938000000004e-05
(Val @ epoch 224) acc: 0.8947368421052632; ap: 0.9571166471648074
(Val @ epoch 225) acc: 0.8947368421052632; ap: 0.9546736046214289
(Val @ epoch 226) acc: 0.881578947368421; ap: 0.9516241931881455
(Val @ epoch 227) acc: 0.8947368421052632; ap: 0.9598249510280424
(Val @ epoch 228) acc: 0.8947368421052632; ap: 0.9552069610670308
2024_03_27_23_30_15 Train loss: 0.7899572849273682 at step: 1840 lr 9.565938000000004e-05
(Val @ epoch 229) acc: 0.8618421052631579; ap: 0.9580637661068838
(Val @ epoch 230) acc: 0.868421052631579; ap: 0.9598398537078665
(Val @ epoch 231) acc: 0.9144736842105263; ap: 0.9609626586385119
(Val @ epoch 232) acc: 0.8881578947368421; ap: 0.9499176969155615
(Val @ epoch 233) acc: 0.875; ap: 0.9585489681579183
2024_03_27_23_30_24 Train loss: 0.41032934188842773 at step: 1880 lr 9.565938000000004e-05
(Val @ epoch 234) acc: 0.9013157894736842; ap: 0.960812566754462
(Val @ epoch 235) acc: 0.9210526315789473; ap: 0.9608641561787016
(Val @ epoch 236) acc: 0.881578947368421; ap: 0.9602344303060769
(Val @ epoch 237) acc: 0.881578947368421; ap: 0.9522892482505906
(Val @ epoch 238) acc: 0.8881578947368421; ap: 0.9523827373742263
2024_03_27_23_30_33 Train loss: 0.44390058517456055 at step: 1920 lr 9.565938000000004e-05
(Val @ epoch 239) acc: 0.8618421052631579; ap: 0.96120727181499
2024_03_27_23_30_35 changing lr at the end of epoch 240, iters 1928
*************************
Changing lr from 9.565938000000004e-05 to 8.609344200000004e-05
*************************
(Val @ epoch 240) acc: 0.9013157894736842; ap: 0.9639274926372897
(Val @ epoch 241) acc: 0.8881578947368421; ap: 0.9637701251718342
(Val @ epoch 242) acc: 0.8947368421052632; ap: 0.9539744613683073
(Val @ epoch 243) acc: 0.8881578947368421; ap: 0.9624518612914428
2024_03_27_23_30_42 Train loss: 0.34355801343917847 at step: 1960 lr 8.609344200000004e-05
(Val @ epoch 244) acc: 0.9276315789473685; ap: 0.9660617928416468
(Val @ epoch 245) acc: 0.881578947368421; ap: 0.9665254215136759
(Val @ epoch 246) acc: 0.8947368421052632; ap: 0.9639736338694913
(Val @ epoch 247) acc: 0.9276315789473685; ap: 0.9683061110702499
(Val @ epoch 248) acc: 0.881578947368421; ap: 0.9675689613203386
2024_03_27_23_30_51 Train loss: 0.23314186930656433 at step: 2000 lr 8.609344200000004e-05
(Val @ epoch 249) acc: 0.881578947368421; ap: 0.9624135761587931
(Val @ epoch 250) acc: 0.9210526315789473; ap: 0.9655844359647427
(Val @ epoch 251) acc: 0.9276315789473685; ap: 0.9641769884077525
(Val @ epoch 252) acc: 0.9342105263157895; ap: 0.96657087553409
(Val @ epoch 253) acc: 0.8881578947368421; ap: 0.9612752240356779
2024_03_27_23_31_00 Train loss: 0.665550947189331 at step: 2040 lr 8.609344200000004e-05
(Val @ epoch 254) acc: 0.8947368421052632; ap: 0.9614737803479404
(Val @ epoch 255) acc: 0.9276315789473685; ap: 0.9634743145468215
(Val @ epoch 256) acc: 0.9210526315789473; ap: 0.9654907197405035
(Val @ epoch 257) acc: 0.9276315789473685; ap: 0.9680498574085765
(Val @ epoch 258) acc: 0.881578947368421; ap: 0.963960457252324
2024_03_27_23_31_09 Train loss: 0.6662330627441406 at step: 2080 lr 8.609344200000004e-05
(Val @ epoch 259) acc: 0.8881578947368421; ap: 0.9614573661774721
(Val @ epoch 260) acc: 0.9144736842105263; ap: 0.9659085128319362
(Val @ epoch 261) acc: 0.9078947368421053; ap: 0.9622985183381065
(Val @ epoch 262) acc: 0.8947368421052632; ap: 0.9620080498827012
(Val @ epoch 263) acc: 0.9013157894736842; ap: 0.9561764858368806
2024_03_27_23_31_17 Train loss: 0.6354789733886719 at step: 2120 lr 8.609344200000004e-05
(Val @ epoch 264) acc: 0.8618421052631579; ap: 0.9610090978355083
(Val @ epoch 265) acc: 0.881578947368421; ap: 0.9648010327609615
(Val @ epoch 266) acc: 0.9276315789473685; ap: 0.9671340726796445
(Val @ epoch 267) acc: 0.9013157894736842; ap: 0.9628749708294486
(Val @ epoch 268) acc: 0.9144736842105263; ap: 0.9666149251579986
2024_03_27_23_31_26 Train loss: 0.43611323833465576 at step: 2160 lr 8.609344200000004e-05
(Val @ epoch 269) acc: 0.881578947368421; ap: 0.9636137202754442
2024_03_27_23_31_28 changing lr at the end of epoch 270, iters 2168
*************************
Changing lr from 8.609344200000004e-05 to 7.748409780000004e-05
*************************
(Val @ epoch 270) acc: 0.881578947368421; ap: 0.9600477123393409
(Val @ epoch 271) acc: 0.9078947368421053; ap: 0.9592224689266334
(Val @ epoch 272) acc: 0.9210526315789473; ap: 0.9630499643448461
(Val @ epoch 273) acc: 0.9078947368421053; ap: 0.9631682189314703
2024_03_27_23_31_35 Train loss: 0.447662889957428 at step: 2200 lr 7.748409780000004e-05
(Val @ epoch 274) acc: 0.9078947368421053; ap: 0.9619144403116491
(Val @ epoch 275) acc: 0.9144736842105263; ap: 0.9645248004239015
(Val @ epoch 276) acc: 0.9210526315789473; ap: 0.9624692814922514
(Val @ epoch 277) acc: 0.9144736842105263; ap: 0.9618345318804057
(Val @ epoch 278) acc: 0.881578947368421; ap: 0.9564625452767803
2024_03_27_23_31_44 Train loss: 0.2626335322856903 at step: 2240 lr 7.748409780000004e-05
(Val @ epoch 279) acc: 0.9078947368421053; ap: 0.9641802307796867
(Val @ epoch 280) acc: 0.875; ap: 0.9644133672004247
(Val @ epoch 281) acc: 0.8947368421052632; ap: 0.9609490481663682
(Val @ epoch 282) acc: 0.9078947368421053; ap: 0.960497788791907
(Val @ epoch 283) acc: 0.9210526315789473; ap: 0.9655236771936568
2024_03_27_23_31_53 Train loss: 0.604584276676178 at step: 2280 lr 7.748409780000004e-05
(Val @ epoch 284) acc: 0.9210526315789473; ap: 0.9687040708624242
(Val @ epoch 285) acc: 0.9210526315789473; ap: 0.9644066757170809
(Val @ epoch 286) acc: 0.8947368421052632; ap: 0.96120264849637
(Val @ epoch 287) acc: 0.9144736842105263; ap: 0.9628336276969842
(Val @ epoch 288) acc: 0.9013157894736842; ap: 0.9631133446435669
2024_03_27_23_32_02 Train loss: 0.45993539690971375 at step: 2320 lr 7.748409780000004e-05
(Val @ epoch 289) acc: 0.881578947368421; ap: 0.963877885672007
(Val @ epoch 290) acc: 0.881578947368421; ap: 0.9609102792210586
(Val @ epoch 291) acc: 0.9144736842105263; ap: 0.960046130197823
(Val @ epoch 292) acc: 0.9144736842105263; ap: 0.962605313384246
(Val @ epoch 293) acc: 0.9144736842105263; ap: 0.9642473642983056
2024_03_27_23_32_11 Train loss: 0.23494982719421387 at step: 2360 lr 7.748409780000004e-05
(Val @ epoch 294) acc: 0.9210526315789473; ap: 0.9662975865800102
(Val @ epoch 295) acc: 0.9210526315789473; ap: 0.9669283281942522
(Val @ epoch 296) acc: 0.9144736842105263; ap: 0.9676589388571919
(Val @ epoch 297) acc: 0.9210526315789473; ap: 0.9636950783625233
(Val @ epoch 298) acc: 0.8881578947368421; ap: 0.9672186106693927
2024_03_27_23_32_20 Train loss: 0.31422653794288635 at step: 2400 lr 7.748409780000004e-05
(Val @ epoch 299) acc: 0.8947368421052632; ap: 0.9684707480487361
2024_03_27_23_32_21 changing lr at the end of epoch 300, iters 2408
*************************
Changing lr from 7.748409780000004e-05 to 6.973568802000003e-05
*************************
(Val @ epoch 300) acc: 0.9276315789473685; ap: 0.9691935513217802
(Val @ epoch 301) acc: 0.9342105263157895; ap: 0.9694224481424972
(Val @ epoch 302) acc: 0.8947368421052632; ap: 0.9676940748257146
(Val @ epoch 303) acc: 0.9210526315789473; ap: 0.9682723366881517
2024_03_27_23_32_28 Train loss: 0.7492033839225769 at step: 2440 lr 6.973568802000003e-05
(Val @ epoch 304) acc: 0.9210526315789473; ap: 0.9707354723378666
(Val @ epoch 305) acc: 0.8947368421052632; ap: 0.9687663663688469
(Val @ epoch 306) acc: 0.9078947368421053; ap: 0.9622626461002473
(Val @ epoch 307) acc: 0.881578947368421; ap: 0.9583968840148775
(Val @ epoch 308) acc: 0.8618421052631579; ap: 0.970203857316272
2024_03_27_23_32_37 Train loss: 0.8364193439483643 at step: 2480 lr 6.973568802000003e-05
(Val @ epoch 309) acc: 0.8881578947368421; ap: 0.969447203553633
(Val @ epoch 310) acc: 0.8552631578947368; ap: 0.9650327849826249
(Val @ epoch 311) acc: 0.9210526315789473; ap: 0.9662078454688312
(Val @ epoch 312) acc: 0.9276315789473685; ap: 0.9692011398166851
(Val @ epoch 313) acc: 0.9276315789473685; ap: 0.9705484088521656
2024_03_27_23_32_46 Train loss: 0.2601383626461029 at step: 2520 lr 6.973568802000003e-05
(Val @ epoch 314) acc: 0.9144736842105263; ap: 0.9665764768796229
(Val @ epoch 315) acc: 0.9144736842105263; ap: 0.9656412265239868
(Val @ epoch 316) acc: 0.9144736842105263; ap: 0.9717780295678934
(Val @ epoch 317) acc: 0.9144736842105263; ap: 0.9715330463868521
(Val @ epoch 318) acc: 0.9210526315789473; ap: 0.9689048367572642
2024_03_27_23_32_55 Train loss: 0.4413602352142334 at step: 2560 lr 6.973568802000003e-05
(Val @ epoch 319) acc: 0.9210526315789473; ap: 0.9723046370392628
(Val @ epoch 320) acc: 0.9276315789473685; ap: 0.9708889810174498
(Val @ epoch 321) acc: 0.9210526315789473; ap: 0.9641959991129638
(Val @ epoch 322) acc: 0.9210526315789473; ap: 0.9689484452898827
(Val @ epoch 323) acc: 0.9144736842105263; ap: 0.9667426425854938
2024_03_27_23_33_04 Train loss: 0.8545517921447754 at step: 2600 lr 6.973568802000003e-05
(Val @ epoch 324) acc: 0.8947368421052632; ap: 0.9656496648431273
(Val @ epoch 325) acc: 0.9276315789473685; ap: 0.9671288450439866
(Val @ epoch 326) acc: 0.9210526315789473; ap: 0.9673073770357556
(Val @ epoch 327) acc: 0.9210526315789473; ap: 0.9715994579992447
(Val @ epoch 328) acc: 0.9210526315789473; ap: 0.9716068024729008
2024_03_27_23_33_12 Train loss: 0.2608891427516937 at step: 2640 lr 6.973568802000003e-05
(Val @ epoch 329) acc: 0.9144736842105263; ap: 0.9687887511992724
2024_03_27_23_33_14 changing lr at the end of epoch 330, iters 2648
*************************
Changing lr from 6.973568802000003e-05 to 6.276211921800003e-05
*************************
(Val @ epoch 330) acc: 0.9342105263157895; ap: 0.9704557435707539
(Val @ epoch 331) acc: 0.8881578947368421; ap: 0.9688276470095539
(Val @ epoch 332) acc: 0.9210526315789473; ap: 0.9692737310781779
(Val @ epoch 333) acc: 0.9144736842105263; ap: 0.964885038381818
2024_03_27_23_33_21 Train loss: 0.25290796160697937 at step: 2680 lr 6.276211921800003e-05
(Val @ epoch 334) acc: 0.9276315789473685; ap: 0.9690579671844078
(Val @ epoch 335) acc: 0.9210526315789473; ap: 0.9671920416347912
(Val @ epoch 336) acc: 0.875; ap: 0.9690486771232408
(Val @ epoch 337) acc: 0.8618421052631579; ap: 0.9685360722626573
(Val @ epoch 338) acc: 0.868421052631579; ap: 0.9682562920017517
2024_03_27_23_33_30 Train loss: 0.35219234228134155 at step: 2720 lr 6.276211921800003e-05
(Val @ epoch 339) acc: 0.875; ap: 0.9666312445004379
(Val @ epoch 340) acc: 0.9078947368421053; ap: 0.9655399314354736
(Val @ epoch 341) acc: 0.9276315789473685; ap: 0.968977888999352
(Val @ epoch 342) acc: 0.9144736842105263; ap: 0.9646281348338887
(Val @ epoch 343) acc: 0.9276315789473685; ap: 0.9693933774723795
2024_03_27_23_33_39 Train loss: 0.3954262137413025 at step: 2760 lr 6.276211921800003e-05
(Val @ epoch 344) acc: 0.9276315789473685; ap: 0.9719431435952383
(Val @ epoch 345) acc: 0.9276315789473685; ap: 0.9719536972531355
(Val @ epoch 346) acc: 0.9144736842105263; ap: 0.9683277586405779
(Val @ epoch 347) acc: 0.9342105263157895; ap: 0.9706538948565641
(Val @ epoch 348) acc: 0.9276315789473685; ap: 0.9720388011334712
2024_03_27_23_33_48 Train loss: 0.554885983467102 at step: 2800 lr 6.276211921800003e-05
(Val @ epoch 349) acc: 0.9276315789473685; ap: 0.9738509090904373
(Val @ epoch 350) acc: 0.9342105263157895; ap: 0.9730223559306653
(Val @ epoch 351) acc: 0.9342105263157895; ap: 0.9720502982115147
(Val @ epoch 352) acc: 0.9276315789473685; ap: 0.9718083738987917
(Val @ epoch 353) acc: 0.9276315789473685; ap: 0.9733385408325729
2024_03_27_23_33_57 Train loss: 0.2815086245536804 at step: 2840 lr 6.276211921800003e-05
(Val @ epoch 354) acc: 0.9210526315789473; ap: 0.9751099891198081
(Val @ epoch 355) acc: 0.9342105263157895; ap: 0.9743571115321872
(Val @ epoch 356) acc: 0.9013157894736842; ap: 0.9680352970464575
(Val @ epoch 357) acc: 0.9078947368421053; ap: 0.9601004608941281
(Val @ epoch 358) acc: 0.8881578947368421; ap: 0.9735157418006746
2024_03_27_23_34_05 Train loss: 0.3788626790046692 at step: 2880 lr 6.276211921800003e-05
(Val @ epoch 359) acc: 0.9342105263157895; ap: 0.9745404326058924
2024_03_27_23_34_07 changing lr at the end of epoch 360, iters 2888
*************************
Changing lr from 6.276211921800003e-05 to 5.6485907296200035e-05
*************************
(Val @ epoch 360) acc: 0.9342105263157895; ap: 0.9684241146321375
(Val @ epoch 361) acc: 0.9276315789473685; ap: 0.9721563188630821
(Val @ epoch 362) acc: 0.9144736842105263; ap: 0.9677407437503397
(Val @ epoch 363) acc: 0.9144736842105263; ap: 0.9696354321587579
2024_03_27_23_34_14 Train loss: 0.7488099932670593 at step: 2920 lr 5.6485907296200035e-05
(Val @ epoch 364) acc: 0.9078947368421053; ap: 0.9678284162475997
(Val @ epoch 365) acc: 0.9144736842105263; ap: 0.969604666309981
(Val @ epoch 366) acc: 0.9276315789473685; ap: 0.9724535528028032
(Val @ epoch 367) acc: 0.9342105263157895; ap: 0.973131815077834
(Val @ epoch 368) acc: 0.9078947368421053; ap: 0.9723271159268414
2024_03_27_23_34_23 Train loss: 0.2852086126804352 at step: 2960 lr 5.6485907296200035e-05
(Val @ epoch 369) acc: 0.9342105263157895; ap: 0.9733041595103192
(Val @ epoch 370) acc: 0.9013157894736842; ap: 0.9726345983866793
(Val @ epoch 371) acc: 0.9210526315789473; ap: 0.9666141804056406
(Val @ epoch 372) acc: 0.9210526315789473; ap: 0.9704773861460836
(Val @ epoch 373) acc: 0.9144736842105263; ap: 0.9756067650069494
2024_03_27_23_34_32 Train loss: 0.4319317936897278 at step: 3000 lr 5.6485907296200035e-05
(Val @ epoch 374) acc: 0.9210526315789473; ap: 0.9742754751439557
(Val @ epoch 375) acc: 0.9013157894736842; ap: 0.9734914802979922
(Val @ epoch 376) acc: 0.9276315789473685; ap: 0.973909934673849
(Val @ epoch 377) acc: 0.9276315789473685; ap: 0.9742341549364065
(Val @ epoch 378) acc: 0.9342105263157895; ap: 0.9752245183586862
2024_03_27_23_34_41 Train loss: 0.2808105945587158 at step: 3040 lr 5.6485907296200035e-05
(Val @ epoch 379) acc: 0.9078947368421053; ap: 0.9729855098092204
(Val @ epoch 380) acc: 0.9210526315789473; ap: 0.9747865124010626
(Val @ epoch 381) acc: 0.9210526315789473; ap: 0.9733061777573092
(Val @ epoch 382) acc: 0.9276315789473685; ap: 0.9753438018332992
(Val @ epoch 383) acc: 0.9013157894736842; ap: 0.9735352055440447
2024_03_27_23_34_49 Train loss: 0.19346925616264343 at step: 3080 lr 5.6485907296200035e-05
(Val @ epoch 384) acc: 0.9013157894736842; ap: 0.9669650471904775
(Val @ epoch 385) acc: 0.9144736842105263; ap: 0.9665448466690945
(Val @ epoch 386) acc: 0.9342105263157895; ap: 0.9742023226862715
(Val @ epoch 387) acc: 0.9078947368421053; ap: 0.9754306879960983
(Val @ epoch 388) acc: 0.9210526315789473; ap: 0.9753540498370781
2024_03_27_23_34_58 Train loss: 0.41353973746299744 at step: 3120 lr 5.6485907296200035e-05
(Val @ epoch 389) acc: 0.9342105263157895; ap: 0.9756678240746209
2024_03_27_23_35_00 changing lr at the end of epoch 390, iters 3128
*************************
Changing lr from 5.6485907296200035e-05 to 5.083731656658003e-05
*************************
(Val @ epoch 390) acc: 0.9078947368421053; ap: 0.9764315141363178
(Val @ epoch 391) acc: 0.9342105263157895; ap: 0.9769932181015659
(Val @ epoch 392) acc: 0.881578947368421; ap: 0.9763164054656442
(Val @ epoch 393) acc: 0.9078947368421053; ap: 0.9703446230202869
2024_03_27_23_35_07 Train loss: 0.20300951600074768 at step: 3160 lr 5.083731656658003e-05
(Val @ epoch 394) acc: 0.9210526315789473; ap: 0.9734354424059526
(Val @ epoch 395) acc: 0.9407894736842105; ap: 0.9763994105588065
(Val @ epoch 396) acc: 0.9342105263157895; ap: 0.9761649670154632
(Val @ epoch 397) acc: 0.9210526315789473; ap: 0.9759348458181965
(Val @ epoch 398) acc: 0.9210526315789473; ap: 0.9759382979393196
2024_03_27_23_35_16 Train loss: 0.3153884708881378 at step: 3200 lr 5.083731656658003e-05
(Val @ epoch 399) acc: 0.9276315789473685; ap: 0.9772901028914703
(Val @ epoch 400) acc: 0.9473684210526315; ap: 0.9773791491516607
(Val @ epoch 401) acc: 0.9407894736842105; ap: 0.9780184486048997
(Val @ epoch 402) acc: 0.9078947368421053; ap: 0.9770822793458845
(Val @ epoch 403) acc: 0.9078947368421053; ap: 0.975503603818225
2024_03_27_23_35_26 Train loss: 0.7247806787490845 at step: 3240 lr 5.083731656658003e-05
(Val @ epoch 404) acc: 0.9078947368421053; ap: 0.9773676948927388
(Val @ epoch 405) acc: 0.9407894736842105; ap: 0.9779233519010209
(Val @ epoch 406) acc: 0.9276315789473685; ap: 0.9771637030481954
(Val @ epoch 407) acc: 0.9078947368421053; ap: 0.9757048717458633
(Val @ epoch 408) acc: 0.9013157894736842; ap: 0.9762900162620355
2024_03_27_23_35_35 Train loss: 0.33126384019851685 at step: 3280 lr 5.083731656658003e-05
(Val @ epoch 409) acc: 0.9013157894736842; ap: 0.9764274900593072
(Val @ epoch 410) acc: 0.9078947368421053; ap: 0.9765955268267669
(Val @ epoch 411) acc: 0.9473684210526315; ap: 0.9771538822040174
(Val @ epoch 412) acc: 0.9407894736842105; ap: 0.9769232408279532
(Val @ epoch 413) acc: 0.9407894736842105; ap: 0.9775754549689198
2024_03_27_23_35_44 Train loss: 0.34763628244400024 at step: 3320 lr 5.083731656658003e-05
(Val @ epoch 414) acc: 0.9144736842105263; ap: 0.9771622080573708
(Val @ epoch 415) acc: 0.9078947368421053; ap: 0.9757718843599769
(Val @ epoch 416) acc: 0.9342105263157895; ap: 0.9751107745514738
(Val @ epoch 417) acc: 0.9407894736842105; ap: 0.9764152270785885
(Val @ epoch 418) acc: 0.9276315789473685; ap: 0.9759753762871932
2024_03_27_23_35_53 Train loss: 0.43214142322540283 at step: 3360 lr 5.083731656658003e-05
(Val @ epoch 419) acc: 0.9210526315789473; ap: 0.9756536734737123
2024_03_27_23_35_54 changing lr at the end of epoch 420, iters 3368
*************************
Changing lr from 5.083731656658003e-05 to 4.575358490992203e-05
*************************
(Val @ epoch 420) acc: 0.9078947368421053; ap: 0.9714978979297938
(Val @ epoch 421) acc: 0.9078947368421053; ap: 0.9708063204973826
(Val @ epoch 422) acc: 0.9144736842105263; ap: 0.9731157726904911
(Val @ epoch 423) acc: 0.9144736842105263; ap: 0.9747892372827784
2024_03_27_23_36_01 Train loss: 0.32894468307495117 at step: 3400 lr 4.575358490992203e-05
(Val @ epoch 424) acc: 0.9276315789473685; ap: 0.9772989489274809
(Val @ epoch 425) acc: 0.9407894736842105; ap: 0.9780505788374473
(Val @ epoch 426) acc: 0.9407894736842105; ap: 0.9780505788374473
(Val @ epoch 427) acc: 0.9078947368421053; ap: 0.976686902077435
(Val @ epoch 428) acc: 0.9013157894736842; ap: 0.9767279102797882
2024_03_27_23_36_10 Train loss: 0.3635806441307068 at step: 3440 lr 4.575358490992203e-05
(Val @ epoch 429) acc: 0.9342105263157895; ap: 0.9782209156084039
(Val @ epoch 430) acc: 0.9013157894736842; ap: 0.9788377584849106
(Val @ epoch 431) acc: 0.9407894736842105; ap: 0.9786616154002211
(Val @ epoch 432) acc: 0.9407894736842105; ap: 0.9783231023529376
(Val @ epoch 433) acc: 0.9276315789473685; ap: 0.9782785278729825
2024_03_27_23_36_19 Train loss: 0.5813298225402832 at step: 3480 lr 4.575358490992203e-05
(Val @ epoch 434) acc: 0.9144736842105263; ap: 0.9786123581110763
(Val @ epoch 435) acc: 0.9013157894736842; ap: 0.9738322436744573
(Val @ epoch 436) acc: 0.9078947368421053; ap: 0.971211194326199
(Val @ epoch 437) acc: 0.9276315789473685; ap: 0.9754867988762876
(Val @ epoch 438) acc: 0.9407894736842105; ap: 0.9786181338009472
2024_03_27_23_36_28 Train loss: 0.18856720626354218 at step: 3520 lr 4.575358490992203e-05
(Val @ epoch 439) acc: 0.9407894736842105; ap: 0.9797747329255451
(Val @ epoch 440) acc: 0.8947368421052632; ap: 0.9797366538001702
(Val @ epoch 441) acc: 0.9342105263157895; ap: 0.9791055831851426
(Val @ epoch 442) acc: 0.9342105263157895; ap: 0.9789149307031612
(Val @ epoch 443) acc: 0.9407894736842105; ap: 0.9789593315604365
2024_03_27_23_36_37 Train loss: 0.489091694355011 at step: 3560 lr 4.575358490992203e-05
(Val @ epoch 444) acc: 0.9342105263157895; ap: 0.9799288860054595
(Val @ epoch 445) acc: 0.9407894736842105; ap: 0.979366702919193
(Val @ epoch 446) acc: 0.9210526315789473; ap: 0.9796182034255927
(Val @ epoch 447) acc: 0.9342105263157895; ap: 0.9781352316960851
(Val @ epoch 448) acc: 0.9473684210526315; ap: 0.9780878193245415
2024_03_27_23_36_45 Train loss: 0.29235392808914185 at step: 3600 lr 4.575358490992203e-05
(Val @ epoch 449) acc: 0.881578947368421; ap: 0.9772293449115259
2024_03_27_23_36_47 changing lr at the end of epoch 450, iters 3608
*************************
Changing lr from 4.575358490992203e-05 to 4.117822641892983e-05
*************************
(Val @ epoch 450) acc: 0.881578947368421; ap: 0.978494740434703
(Val @ epoch 451) acc: 0.9342105263157895; ap: 0.9787988205339906
(Val @ epoch 452) acc: 0.9473684210526315; ap: 0.9801003836396813
(Val @ epoch 453) acc: 0.9473684210526315; ap: 0.980108263515989
2024_03_27_23_36_54 Train loss: 0.242801234126091 at step: 3640 lr 4.117822641892983e-05
(Val @ epoch 454) acc: 0.9407894736842105; ap: 0.9800673425473936
(Val @ epoch 455) acc: 0.9342105263157895; ap: 0.9802268056600101
(Val @ epoch 456) acc: 0.9342105263157895; ap: 0.9794766199743524
(Val @ epoch 457) acc: 0.9078947368421053; ap: 0.9790091212567325
(Val @ epoch 458) acc: 0.8947368421052632; ap: 0.9794231202901261
2024_03_27_23_37_03 Train loss: 0.34515872597694397 at step: 3680 lr 4.117822641892983e-05
(Val @ epoch 459) acc: 0.9013157894736842; ap: 0.9791069148098311
(Val @ epoch 460) acc: 0.9407894736842105; ap: 0.9791269614027052
(Val @ epoch 461) acc: 0.9407894736842105; ap: 0.9798575904983979
(Val @ epoch 462) acc: 0.9407894736842105; ap: 0.9795113717734901
(Val @ epoch 463) acc: 0.9342105263157895; ap: 0.9798790791118379
2024_03_27_23_37_12 Train loss: 0.324674516916275 at step: 3720 lr 4.117822641892983e-05
(Val @ epoch 464) acc: 0.9407894736842105; ap: 0.9809847874262922
(Val @ epoch 465) acc: 0.9342105263157895; ap: 0.9808610874105899
(Val @ epoch 466) acc: 0.9407894736842105; ap: 0.9811253170126092
(Val @ epoch 467) acc: 0.9342105263157895; ap: 0.9803082633244697
(Val @ epoch 468) acc: 0.9407894736842105; ap: 0.9799675758668087
2024_03_27_23_37_20 Train loss: 0.22603164613246918 at step: 3760 lr 4.117822641892983e-05
(Val @ epoch 469) acc: 0.9473684210526315; ap: 0.9792604950366135
(Val @ epoch 470) acc: 0.9276315789473685; ap: 0.9800666387230461
(Val @ epoch 471) acc: 0.9013157894736842; ap: 0.980113536460218
(Val @ epoch 472) acc: 0.9276315789473685; ap: 0.978335262856156
(Val @ epoch 473) acc: 0.9210526315789473; ap: 0.9798439285814661
2024_03_27_23_37_29 Train loss: 1.0004124641418457 at step: 3800 lr 4.117822641892983e-05
(Val @ epoch 474) acc: 0.9144736842105263; ap: 0.9815463426853382
(Val @ epoch 475) acc: 0.9407894736842105; ap: 0.9811658755495173
(Val @ epoch 476) acc: 0.9407894736842105; ap: 0.9813104895687569
(Val @ epoch 477) acc: 0.9144736842105263; ap: 0.981404449542239
(Val @ epoch 478) acc: 0.9078947368421053; ap: 0.9779488390773965
2024_03_27_23_37_38 Train loss: 0.29730379581451416 at step: 3840 lr 4.117822641892983e-05
(Val @ epoch 479) acc: 0.9276315789473685; ap: 0.9781144256554993
2024_03_27_23_37_40 changing lr at the end of epoch 480, iters 3848
*************************
Changing lr from 4.117822641892983e-05 to 3.7060403777036845e-05
*************************
(Val @ epoch 480) acc: 0.9473684210526315; ap: 0.9807413556996412
(Val @ epoch 481) acc: 0.9276315789473685; ap: 0.980212453610708
(Val @ epoch 482) acc: 0.9013157894736842; ap: 0.9796853112335459
(Val @ epoch 483) acc: 0.9078947368421053; ap: 0.9803579982165391
2024_03_27_23_37_47 Train loss: 0.3310319185256958 at step: 3880 lr 3.7060403777036845e-05
(Val @ epoch 484) acc: 0.9210526315789473; ap: 0.9796396050877206
(Val @ epoch 485) acc: 0.9473684210526315; ap: 0.9800148513468427
(Val @ epoch 486) acc: 0.9407894736842105; ap: 0.9806278186987026
(Val @ epoch 487) acc: 0.9407894736842105; ap: 0.9802279119901222
(Val @ epoch 488) acc: 0.9342105263157895; ap: 0.9806357275640032
2024_03_27_23_37_56 Train loss: 0.2659466862678528 at step: 3920 lr 3.7060403777036845e-05
(Val @ epoch 489) acc: 0.9407894736842105; ap: 0.9800191669652119
(Val @ epoch 490) acc: 0.9473684210526315; ap: 0.980760048837053
(Val @ epoch 491) acc: 0.9342105263157895; ap: 0.981615740009714
(Val @ epoch 492) acc: 0.9407894736842105; ap: 0.9817425952575904
(Val @ epoch 493) acc: 0.9539473684210527; ap: 0.9812895420447049
2024_03_27_23_38_05 Train loss: 0.1390962451696396 at step: 3960 lr 3.7060403777036845e-05
(Val @ epoch 494) acc: 0.9407894736842105; ap: 0.9810411372118671
(Val @ epoch 495) acc: 0.9473684210526315; ap: 0.9808131819784214
(Val @ epoch 496) acc: 0.9407894736842105; ap: 0.9806219331150557
(Val @ epoch 497) acc: 0.9473684210526315; ap: 0.9799418790377035
(Val @ epoch 498) acc: 0.9342105263157895; ap: 0.9803079644423117
2024_03_27_23_38_14 Train loss: 0.3746011257171631 at step: 4000 lr 3.7060403777036845e-05
(Val @ epoch 499) acc: 0.9210526315789473; ap: 0.9805968612191015
*************************
2024_03_27_23_38_15
(0 FastGan   ) acc: 94.6; ap: 98.9
(1 StyleGAN_ADA) acc: 91.5; ap: 95.6
(2 Mean      ) acc: 93.1; ap: 97.2
*************************
2024_03_27_23_38_16
Saving model ./checkpoints/resnet10_60per2024_03_27_23_23_28/model_epoch_last.pth
