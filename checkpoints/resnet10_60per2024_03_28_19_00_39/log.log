train.py  --name  resnet10_60per  --dataroot  /home/ashish/detect_dataset/splitted_data_60/  --classes  FastGan,StyleGAN_ADA  --batch_size  128  --delr_freq  60  --lr  0.0002  --niter  1000
Model size: 5113.00000
cwd: /home/ashish/NPR-DeepfakeDetectionResnet18
(Val @ epoch 0) acc: 0.5; ap: 0.4159395385199444
(Val @ epoch 1) acc: 0.5; ap: 0.41310241311044826
(Val @ epoch 2) acc: 0.5; ap: 0.4152834966146779
(Val @ epoch 3) acc: 0.5; ap: 0.42284661722298267
2024_03_28_19_00_48 Train loss: 0.7440948486328125 at step: 40 lr 0.0002
(Val @ epoch 4) acc: 0.5; ap: 0.5110487667474022
(Val @ epoch 5) acc: 0.506578947368421; ap: 0.5767724196022963
(Val @ epoch 6) acc: 0.5131578947368421; ap: 0.5944827887292446
(Val @ epoch 7) acc: 0.5197368421052632; ap: 0.5978592798290945
(Val @ epoch 8) acc: 0.5657894736842105; ap: 0.5808614173946239
2024_03_28_19_00_57 Train loss: 0.7481922507286072 at step: 80 lr 0.0002
(Val @ epoch 9) acc: 0.5855263157894737; ap: 0.5842221790162111
(Val @ epoch 10) acc: 0.5723684210526315; ap: 0.5892135052440408
(Val @ epoch 11) acc: 0.5723684210526315; ap: 0.5949271052043784
(Val @ epoch 12) acc: 0.5855263157894737; ap: 0.5988492625490689
(Val @ epoch 13) acc: 0.5921052631578947; ap: 0.6078506726236431
2024_03_28_19_01_06 Train loss: 0.6646275520324707 at step: 120 lr 0.0002
(Val @ epoch 14) acc: 0.5855263157894737; ap: 0.6107788683236823
(Val @ epoch 15) acc: 0.6118421052631579; ap: 0.6175975080875016
(Val @ epoch 16) acc: 0.5986842105263158; ap: 0.613400238287035
(Val @ epoch 17) acc: 0.5855263157894737; ap: 0.6218217186835345
(Val @ epoch 18) acc: 0.5789473684210527; ap: 0.6345244191242447
2024_03_28_19_01_16 Train loss: 0.5741356611251831 at step: 160 lr 0.0002
(Val @ epoch 19) acc: 0.5723684210526315; ap: 0.6348421573732571
(Val @ epoch 20) acc: 0.6052631578947368; ap: 0.6335420076125509
(Val @ epoch 21) acc: 0.6118421052631579; ap: 0.6362486920536722
(Val @ epoch 22) acc: 0.6052631578947368; ap: 0.6425901994202068
(Val @ epoch 23) acc: 0.5789473684210527; ap: 0.6498293987538418
2024_03_28_19_01_25 Train loss: 0.7576649188995361 at step: 200 lr 0.0002
(Val @ epoch 24) acc: 0.5789473684210527; ap: 0.6630570097072703
(Val @ epoch 25) acc: 0.5657894736842105; ap: 0.661415719724711
(Val @ epoch 26) acc: 0.5657894736842105; ap: 0.669229526330695
(Val @ epoch 27) acc: 0.5723684210526315; ap: 0.6606681313559867
(Val @ epoch 28) acc: 0.5723684210526315; ap: 0.6557180134756782
2024_03_28_19_01_34 Train loss: 0.6544457674026489 at step: 240 lr 0.0002
(Val @ epoch 29) acc: 0.5986842105263158; ap: 0.6700987068140076
(Val @ epoch 30) acc: 0.6381578947368421; ap: 0.6848552169902935
(Val @ epoch 31) acc: 0.5986842105263158; ap: 0.6723342840364155
(Val @ epoch 32) acc: 0.5921052631578947; ap: 0.6690859804052602
(Val @ epoch 33) acc: 0.618421052631579; ap: 0.6772214381864547
2024_03_28_19_01_43 Train loss: 0.6089143753051758 at step: 280 lr 0.0002
(Val @ epoch 34) acc: 0.6513157894736842; ap: 0.6901018247075354
(Val @ epoch 35) acc: 0.625; ap: 0.6911589903031327
(Val @ epoch 36) acc: 0.6447368421052632; ap: 0.7009599773211962
(Val @ epoch 37) acc: 0.631578947368421; ap: 0.7108454120942157
(Val @ epoch 38) acc: 0.6513157894736842; ap: 0.7211871938857833
2024_03_28_19_01_53 Train loss: 0.7335187792778015 at step: 320 lr 0.0002
(Val @ epoch 39) acc: 0.631578947368421; ap: 0.7125765896623556
(Val @ epoch 40) acc: 0.618421052631579; ap: 0.7052647111356012
(Val @ epoch 41) acc: 0.618421052631579; ap: 0.6973026153479116
(Val @ epoch 42) acc: 0.6381578947368421; ap: 0.7047046440037781
(Val @ epoch 43) acc: 0.6710526315789473; ap: 0.7671115942304849
2024_03_28_19_02_02 Train loss: 0.6603589057922363 at step: 360 lr 0.0002
(Val @ epoch 44) acc: 0.6842105263157895; ap: 0.7763306260438765
(Val @ epoch 45) acc: 0.6710526315789473; ap: 0.7506652862779325
(Val @ epoch 46) acc: 0.6776315789473685; ap: 0.7648559944542989
(Val @ epoch 47) acc: 0.6907894736842105; ap: 0.7526044095278753
(Val @ epoch 48) acc: 0.6710526315789473; ap: 0.7808738944107614
2024_03_28_19_02_11 Train loss: 0.6483399271965027 at step: 400 lr 0.0002
(Val @ epoch 49) acc: 0.6842105263157895; ap: 0.7782945014391496
(Val @ epoch 50) acc: 0.6842105263157895; ap: 0.7823808633480853
(Val @ epoch 51) acc: 0.6710526315789473; ap: 0.7650464487355735
(Val @ epoch 52) acc: 0.7105263157894737; ap: 0.8172062948182471
(Val @ epoch 53) acc: 0.7039473684210527; ap: 0.8209789699786916
2024_03_28_19_02_20 Train loss: 0.5780164003372192 at step: 440 lr 0.0002
(Val @ epoch 54) acc: 0.6907894736842105; ap: 0.7963730964665859
(Val @ epoch 55) acc: 0.7039473684210527; ap: 0.7985935156650605
(Val @ epoch 56) acc: 0.7171052631578947; ap: 0.8279389992432865
(Val @ epoch 57) acc: 0.6907894736842105; ap: 0.7979444039074586
(Val @ epoch 58) acc: 0.7171052631578947; ap: 0.8270550475163618
2024_03_28_19_02_29 Train loss: 0.5110534429550171 at step: 480 lr 0.0002
(Val @ epoch 59) acc: 0.7236842105263158; ap: 0.8215333796541225
2024_03_28_19_02_31 changing lr at the end of epoch 60, iters 488
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 60) acc: 0.7236842105263158; ap: 0.8296474738568255
(Val @ epoch 61) acc: 0.7236842105263158; ap: 0.8186453280686841
(Val @ epoch 62) acc: 0.7236842105263158; ap: 0.8370026500882463
(Val @ epoch 63) acc: 0.756578947368421; ap: 0.8550290621596425
2024_03_28_19_02_38 Train loss: 0.4970254600048065 at step: 520 lr 0.00018
(Val @ epoch 64) acc: 0.7236842105263158; ap: 0.8486780426547889
(Val @ epoch 65) acc: 0.6776315789473685; ap: 0.8266977877275669
(Val @ epoch 66) acc: 0.7039473684210527; ap: 0.8302452599391954
(Val @ epoch 67) acc: 0.7236842105263158; ap: 0.8465930342510977
(Val @ epoch 68) acc: 0.7302631578947368; ap: 0.8512047557793435
2024_03_28_19_02_47 Train loss: 0.690597414970398 at step: 560 lr 0.00018
(Val @ epoch 69) acc: 0.6842105263157895; ap: 0.8298092391632772
(Val @ epoch 70) acc: 0.7236842105263158; ap: 0.8284142478725631
(Val @ epoch 71) acc: 0.7894736842105263; ap: 0.8638667739290604
(Val @ epoch 72) acc: 0.743421052631579; ap: 0.8701837825742303
(Val @ epoch 73) acc: 0.7302631578947368; ap: 0.8601731384081344
2024_03_28_19_02_56 Train loss: 0.5798321962356567 at step: 600 lr 0.00018
(Val @ epoch 74) acc: 0.6447368421052632; ap: 0.8145066463505034
(Val @ epoch 75) acc: 0.743421052631579; ap: 0.8500969984284084
(Val @ epoch 76) acc: 0.7631578947368421; ap: 0.8662545437688354
(Val @ epoch 77) acc: 0.7171052631578947; ap: 0.8338859984679712
(Val @ epoch 78) acc: 0.7894736842105263; ap: 0.8750005791804711
2024_03_28_19_03_06 Train loss: 0.5157203674316406 at step: 640 lr 0.00018
(Val @ epoch 79) acc: 0.8026315789473685; ap: 0.8872258944039094
(Val @ epoch 80) acc: 0.7697368421052632; ap: 0.8674590012877753
(Val @ epoch 81) acc: 0.8026315789473685; ap: 0.8791747345606011
(Val @ epoch 82) acc: 0.7894736842105263; ap: 0.8547228769970301
(Val @ epoch 83) acc: 0.7894736842105263; ap: 0.8767747004003046
2024_03_28_19_03_15 Train loss: 0.6509374380111694 at step: 680 lr 0.00018
(Val @ epoch 84) acc: 0.8092105263157895; ap: 0.8840606371066472
(Val @ epoch 85) acc: 0.7828947368421053; ap: 0.8762237701805431
(Val @ epoch 86) acc: 0.8026315789473685; ap: 0.8779048601578077
(Val @ epoch 87) acc: 0.8092105263157895; ap: 0.8921650292038186
(Val @ epoch 88) acc: 0.7828947368421053; ap: 0.8778793589291689
2024_03_28_19_03_24 Train loss: 0.5181912183761597 at step: 720 lr 0.00018
(Val @ epoch 89) acc: 0.8157894736842105; ap: 0.8898915207237713
(Val @ epoch 90) acc: 0.7960526315789473; ap: 0.8993155216177104
(Val @ epoch 91) acc: 0.756578947368421; ap: 0.8778385940777071
(Val @ epoch 92) acc: 0.8092105263157895; ap: 0.902990118630349
(Val @ epoch 93) acc: 0.8157894736842105; ap: 0.9132805626551677
2024_03_28_19_03_33 Train loss: 0.6216471195220947 at step: 760 lr 0.00018
(Val @ epoch 94) acc: 0.8223684210526315; ap: 0.9111451811910297
(Val @ epoch 95) acc: 0.8026315789473685; ap: 0.9140774552714634
(Val @ epoch 96) acc: 0.7894736842105263; ap: 0.9176747925841231
(Val @ epoch 97) acc: 0.7894736842105263; ap: 0.9076606262272979
(Val @ epoch 98) acc: 0.7960526315789473; ap: 0.914952082833756
2024_03_28_19_03_42 Train loss: 0.7562525272369385 at step: 800 lr 0.00018
(Val @ epoch 99) acc: 0.8157894736842105; ap: 0.8963085312162543
(Val @ epoch 100) acc: 0.7960526315789473; ap: 0.9200615650230733
(Val @ epoch 101) acc: 0.8026315789473685; ap: 0.9131671406460582
(Val @ epoch 102) acc: 0.8223684210526315; ap: 0.9182524807881651
(Val @ epoch 103) acc: 0.8157894736842105; ap: 0.8984702247053828
2024_03_28_19_03_51 Train loss: 0.4745258688926697 at step: 840 lr 0.00018
(Val @ epoch 104) acc: 0.7894736842105263; ap: 0.888577767387024
(Val @ epoch 105) acc: 0.8355263157894737; ap: 0.8935785508363049
(Val @ epoch 106) acc: 0.8092105263157895; ap: 0.9022382332359664
(Val @ epoch 107) acc: 0.8157894736842105; ap: 0.9155497353983362
(Val @ epoch 108) acc: 0.8486842105263158; ap: 0.9161877998232517
2024_03_28_19_04_01 Train loss: 0.35073086619377136 at step: 880 lr 0.00018
(Val @ epoch 109) acc: 0.8157894736842105; ap: 0.8844724065708075
(Val @ epoch 110) acc: 0.7631578947368421; ap: 0.9193421987084681
(Val @ epoch 111) acc: 0.8486842105263158; ap: 0.9166731979553682
(Val @ epoch 112) acc: 0.7894736842105263; ap: 0.9002115028966395
(Val @ epoch 113) acc: 0.8421052631578947; ap: 0.9070020873308471
2024_03_28_19_04_10 Train loss: 0.7548965215682983 at step: 920 lr 0.00018
(Val @ epoch 114) acc: 0.8289473684210527; ap: 0.930199512117186
(Val @ epoch 115) acc: 0.8355263157894737; ap: 0.9224537624830559
(Val @ epoch 116) acc: 0.8618421052631579; ap: 0.9190182259456309
(Val @ epoch 117) acc: 0.8157894736842105; ap: 0.9357632640606173
(Val @ epoch 118) acc: 0.868421052631579; ap: 0.9241844223836735
2024_03_28_19_04_19 Train loss: 0.32423967123031616 at step: 960 lr 0.00018
(Val @ epoch 119) acc: 0.868421052631579; ap: 0.9286950878354017
2024_03_28_19_04_21 changing lr at the end of epoch 120, iters 968
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 120) acc: 0.868421052631579; ap: 0.9305695613031155
(Val @ epoch 121) acc: 0.8092105263157895; ap: 0.9376009663256244
(Val @ epoch 122) acc: 0.8618421052631579; ap: 0.9366133355575992
(Val @ epoch 123) acc: 0.881578947368421; ap: 0.9406980861681523
2024_03_28_19_04_28 Train loss: 0.33070099353790283 at step: 1000 lr 0.000162
(Val @ epoch 124) acc: 0.868421052631579; ap: 0.9362058177717311
(Val @ epoch 125) acc: 0.868421052631579; ap: 0.9358755467007444
(Val @ epoch 126) acc: 0.8223684210526315; ap: 0.9288670474827042
(Val @ epoch 127) acc: 0.868421052631579; ap: 0.9335542912862518
(Val @ epoch 128) acc: 0.875; ap: 0.9367971605736785
2024_03_28_19_04_37 Train loss: 0.4408811330795288 at step: 1040 lr 0.000162
(Val @ epoch 129) acc: 0.8618421052631579; ap: 0.9277320181320452
(Val @ epoch 130) acc: 0.8092105263157895; ap: 0.934633241195742
(Val @ epoch 131) acc: 0.868421052631579; ap: 0.9403555446001337
(Val @ epoch 132) acc: 0.868421052631579; ap: 0.9235733871699181
(Val @ epoch 133) acc: 0.7631578947368421; ap: 0.9443774452557055
2024_03_28_19_04_46 Train loss: 0.5751649737358093 at step: 1080 lr 0.000162
(Val @ epoch 134) acc: 0.8289473684210527; ap: 0.9452914343900716
(Val @ epoch 135) acc: 0.881578947368421; ap: 0.9321603873673476
(Val @ epoch 136) acc: 0.868421052631579; ap: 0.9324957308561909
(Val @ epoch 137) acc: 0.8486842105263158; ap: 0.9242201051123311
(Val @ epoch 138) acc: 0.8289473684210527; ap: 0.9131420416586411
2024_03_28_19_04_55 Train loss: 0.6019797921180725 at step: 1120 lr 0.000162
(Val @ epoch 139) acc: 0.8157894736842105; ap: 0.911964386218244
(Val @ epoch 140) acc: 0.8223684210526315; ap: 0.9298665483311878
(Val @ epoch 141) acc: 0.868421052631579; ap: 0.9349909386081635
(Val @ epoch 142) acc: 0.8355263157894737; ap: 0.9304609739768683
(Val @ epoch 143) acc: 0.8289473684210527; ap: 0.9473006426790292
2024_03_28_19_05_05 Train loss: 0.4896228313446045 at step: 1160 lr 0.000162
(Val @ epoch 144) acc: 0.8618421052631579; ap: 0.9448840828175455
(Val @ epoch 145) acc: 0.8881578947368421; ap: 0.9431452551323668
(Val @ epoch 146) acc: 0.8618421052631579; ap: 0.945039691133181
(Val @ epoch 147) acc: 0.8552631578947368; ap: 0.9383220280025673
(Val @ epoch 148) acc: 0.8618421052631579; ap: 0.9453564671385732
2024_03_28_19_05_14 Train loss: 0.3859120309352875 at step: 1200 lr 0.000162
(Val @ epoch 149) acc: 0.8486842105263158; ap: 0.9412270367051446
(Val @ epoch 150) acc: 0.8289473684210527; ap: 0.9349807204104773
(Val @ epoch 151) acc: 0.8552631578947368; ap: 0.9326799831429324
(Val @ epoch 152) acc: 0.881578947368421; ap: 0.9462228909138554
(Val @ epoch 153) acc: 0.8421052631578947; ap: 0.9372134233036997
2024_03_28_19_05_23 Train loss: 0.25940781831741333 at step: 1240 lr 0.000162
(Val @ epoch 154) acc: 0.8552631578947368; ap: 0.9413294882771964
(Val @ epoch 155) acc: 0.8486842105263158; ap: 0.9483411264298413
(Val @ epoch 156) acc: 0.9013157894736842; ap: 0.9531713242715764
(Val @ epoch 157) acc: 0.8947368421052632; ap: 0.9500474844826199
(Val @ epoch 158) acc: 0.875; ap: 0.9505692802663772
2024_03_28_19_05_32 Train loss: 0.3373149633407593 at step: 1280 lr 0.000162
(Val @ epoch 159) acc: 0.9013157894736842; ap: 0.953823739634313
(Val @ epoch 160) acc: 0.8355263157894737; ap: 0.9493134923448735
(Val @ epoch 161) acc: 0.875; ap: 0.9469568022176387
(Val @ epoch 162) acc: 0.8289473684210527; ap: 0.951378027619554
(Val @ epoch 163) acc: 0.8421052631578947; ap: 0.948123311509831
2024_03_28_19_05_41 Train loss: 0.25558143854141235 at step: 1320 lr 0.000162
(Val @ epoch 164) acc: 0.868421052631579; ap: 0.9561567935790912
(Val @ epoch 165) acc: 0.8947368421052632; ap: 0.9579413737897089
(Val @ epoch 166) acc: 0.8486842105263158; ap: 0.9578030095645572
(Val @ epoch 167) acc: 0.875; ap: 0.9599277664971511
(Val @ epoch 168) acc: 0.8355263157894737; ap: 0.954123444401518
2024_03_28_19_05_50 Train loss: 0.4155158996582031 at step: 1360 lr 0.000162
(Val @ epoch 169) acc: 0.868421052631579; ap: 0.9438904237932231
(Val @ epoch 170) acc: 0.881578947368421; ap: 0.9529036657118171
(Val @ epoch 171) acc: 0.881578947368421; ap: 0.955318232179212
(Val @ epoch 172) acc: 0.8947368421052632; ap: 0.9533382727413477
(Val @ epoch 173) acc: 0.8092105263157895; ap: 0.9572869127803865
2024_03_28_19_06_00 Train loss: 0.30940157175064087 at step: 1400 lr 0.000162
(Val @ epoch 174) acc: 0.9078947368421053; ap: 0.9575165932719175
(Val @ epoch 175) acc: 0.8486842105263158; ap: 0.9461229030267161
(Val @ epoch 176) acc: 0.8223684210526315; ap: 0.9558669379294239
(Val @ epoch 177) acc: 0.8947368421052632; ap: 0.9574068906841295
(Val @ epoch 178) acc: 0.875; ap: 0.9509282053249488
2024_03_28_19_06_09 Train loss: 0.31218597292900085 at step: 1440 lr 0.000162
(Val @ epoch 179) acc: 0.9013157894736842; ap: 0.9607853687944078
2024_03_28_19_06_11 changing lr at the end of epoch 180, iters 1448
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 180) acc: 0.9078947368421053; ap: 0.9599765441153677
(Val @ epoch 181) acc: 0.9144736842105263; ap: 0.9635080381596605
(Val @ epoch 182) acc: 0.8421052631578947; ap: 0.9560985445095967
(Val @ epoch 183) acc: 0.875; ap: 0.9578460055590785
2024_03_28_19_06_18 Train loss: 0.3524644076824188 at step: 1480 lr 0.00014580000000000002
(Val @ epoch 184) acc: 0.9078947368421053; ap: 0.9616823567112176
(Val @ epoch 185) acc: 0.8355263157894737; ap: 0.9578116829037103
(Val @ epoch 186) acc: 0.8947368421052632; ap: 0.9510857852943281
(Val @ epoch 187) acc: 0.8947368421052632; ap: 0.959339277636713
(Val @ epoch 188) acc: 0.8618421052631579; ap: 0.9604754389356546
2024_03_28_19_06_27 Train loss: 0.29499179124832153 at step: 1520 lr 0.00014580000000000002
(Val @ epoch 189) acc: 0.868421052631579; ap: 0.9671804847003548
(Val @ epoch 190) acc: 0.8618421052631579; ap: 0.9587544761367159
(Val @ epoch 191) acc: 0.9013157894736842; ap: 0.9590248785041828
(Val @ epoch 192) acc: 0.9144736842105263; ap: 0.9641559272934649
(Val @ epoch 193) acc: 0.8881578947368421; ap: 0.963875702607999
2024_03_28_19_06_36 Train loss: 0.44992929697036743 at step: 1560 lr 0.00014580000000000002
(Val @ epoch 194) acc: 0.9210526315789473; ap: 0.9645062019095617
(Val @ epoch 195) acc: 0.9210526315789473; ap: 0.9665679972066384
(Val @ epoch 196) acc: 0.881578947368421; ap: 0.960821957571764
(Val @ epoch 197) acc: 0.8552631578947368; ap: 0.9663697098622952
(Val @ epoch 198) acc: 0.9210526315789473; ap: 0.9682149880927914
2024_03_28_19_06_46 Train loss: 0.8185985088348389 at step: 1600 lr 0.00014580000000000002
(Val @ epoch 199) acc: 0.9013157894736842; ap: 0.9692041699285396
(Val @ epoch 200) acc: 0.9276315789473685; ap: 0.9683264888108433
(Val @ epoch 201) acc: 0.9342105263157895; ap: 0.9667590506512204
(Val @ epoch 202) acc: 0.868421052631579; ap: 0.9642409761481596
(Val @ epoch 203) acc: 0.9144736842105263; ap: 0.9618740207313163
2024_03_28_19_06_55 Train loss: 0.31496790051460266 at step: 1640 lr 0.00014580000000000002
(Val @ epoch 204) acc: 0.8881578947368421; ap: 0.9675648299484462
(Val @ epoch 205) acc: 0.9276315789473685; ap: 0.9660378439550416
(Val @ epoch 206) acc: 0.9210526315789473; ap: 0.9671363478796822
(Val @ epoch 207) acc: 0.9078947368421053; ap: 0.9671581632572632
(Val @ epoch 208) acc: 0.9276315789473685; ap: 0.9664742169990741
2024_03_28_19_07_04 Train loss: 0.8411688804626465 at step: 1680 lr 0.00014580000000000002
(Val @ epoch 209) acc: 0.8881578947368421; ap: 0.969226821218962
(Val @ epoch 210) acc: 0.9276315789473685; ap: 0.9686211695975511
(Val @ epoch 211) acc: 0.9407894736842105; ap: 0.969598318038834
(Val @ epoch 212) acc: 0.875; ap: 0.9662542992901867
(Val @ epoch 213) acc: 0.9210526315789473; ap: 0.9709058194997767
2024_03_28_19_07_13 Train loss: 0.5437933802604675 at step: 1720 lr 0.00014580000000000002
(Val @ epoch 214) acc: 0.8947368421052632; ap: 0.9696229075541878
(Val @ epoch 215) acc: 0.8947368421052632; ap: 0.9682635969894713
(Val @ epoch 216) acc: 0.9144736842105263; ap: 0.9708340415220164
(Val @ epoch 217) acc: 0.9144736842105263; ap: 0.9699290981247616
(Val @ epoch 218) acc: 0.9144736842105263; ap: 0.9700259528742257
2024_03_28_19_07_22 Train loss: 0.6184136867523193 at step: 1760 lr 0.00014580000000000002
(Val @ epoch 219) acc: 0.9276315789473685; ap: 0.9714937665063265
(Val @ epoch 220) acc: 0.9078947368421053; ap: 0.9724382267908752
(Val @ epoch 221) acc: 0.881578947368421; ap: 0.9680082789324144
(Val @ epoch 222) acc: 0.9013157894736842; ap: 0.9661091187908311
(Val @ epoch 223) acc: 0.9342105263157895; ap: 0.9730055486370115
2024_03_28_19_07_32 Train loss: 0.5809109210968018 at step: 1800 lr 0.00014580000000000002
(Val @ epoch 224) acc: 0.8881578947368421; ap: 0.9668598140746557
(Val @ epoch 225) acc: 0.9013157894736842; ap: 0.9664435465526027
(Val @ epoch 226) acc: 0.9144736842105263; ap: 0.9651193352068045
(Val @ epoch 227) acc: 0.8881578947368421; ap: 0.9706074584608922
(Val @ epoch 228) acc: 0.9144736842105263; ap: 0.9660560290646708
2024_03_28_19_07_41 Train loss: 0.840823769569397 at step: 1840 lr 0.00014580000000000002
(Val @ epoch 229) acc: 0.881578947368421; ap: 0.9739232474707055
(Val @ epoch 230) acc: 0.9342105263157895; ap: 0.9741492093141497
(Val @ epoch 231) acc: 0.9276315789473685; ap: 0.9751242086732189
(Val @ epoch 232) acc: 0.9210526315789473; ap: 0.9661912007407413
(Val @ epoch 233) acc: 0.9342105263157895; ap: 0.9756382373604315
2024_03_28_19_07_50 Train loss: 0.3811356723308563 at step: 1880 lr 0.00014580000000000002
(Val @ epoch 234) acc: 0.9342105263157895; ap: 0.9750995229196969
(Val @ epoch 235) acc: 0.9276315789473685; ap: 0.9749608016445998
(Val @ epoch 236) acc: 0.868421052631579; ap: 0.9729099447631868
(Val @ epoch 237) acc: 0.868421052631579; ap: 0.9654800477479651
(Val @ epoch 238) acc: 0.9276315789473685; ap: 0.9700916326102849
2024_03_28_19_07_59 Train loss: 0.47076791524887085 at step: 1920 lr 0.00014580000000000002
(Val @ epoch 239) acc: 0.9013157894736842; ap: 0.9734852451675459
2024_03_28_19_08_01 changing lr at the end of epoch 240, iters 1928
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 240) acc: 0.9144736842105263; ap: 0.9739102698499965
(Val @ epoch 241) acc: 0.9078947368421053; ap: 0.9754585930810588
(Val @ epoch 242) acc: 0.9078947368421053; ap: 0.9700996850952609
(Val @ epoch 243) acc: 0.9144736842105263; ap: 0.9750118473351735
2024_03_28_19_08_08 Train loss: 0.35200315713882446 at step: 1960 lr 0.00013122000000000003
(Val @ epoch 244) acc: 0.8881578947368421; ap: 0.9761631344843861
(Val @ epoch 245) acc: 0.9342105263157895; ap: 0.9779097673442632
(Val @ epoch 246) acc: 0.9473684210526315; ap: 0.9766014192454835
(Val @ epoch 247) acc: 0.9276315789473685; ap: 0.9751087175440231
(Val @ epoch 248) acc: 0.881578947368421; ap: 0.976526941376041
2024_03_28_19_08_18 Train loss: 0.20074689388275146 at step: 2000 lr 0.00013122000000000003
(Val @ epoch 249) acc: 0.9342105263157895; ap: 0.9770151789447227
(Val @ epoch 250) acc: 0.9407894736842105; ap: 0.9789136266882358
(Val @ epoch 251) acc: 0.9276315789473685; ap: 0.9771259221042989
(Val @ epoch 252) acc: 0.9342105263157895; ap: 0.9781378323223161
(Val @ epoch 253) acc: 0.875; ap: 0.9716274164661227
2024_03_28_19_08_27 Train loss: 0.6151736974716187 at step: 2040 lr 0.00013122000000000003
(Val @ epoch 254) acc: 0.9407894736842105; ap: 0.9765018149344622
(Val @ epoch 255) acc: 0.9210526315789473; ap: 0.9750989145581954
(Val @ epoch 256) acc: 0.9407894736842105; ap: 0.9766171904530981
(Val @ epoch 257) acc: 0.9276315789473685; ap: 0.9769809325814297
(Val @ epoch 258) acc: 0.8947368421052632; ap: 0.9760997324600377
2024_03_28_19_08_36 Train loss: 0.6880223751068115 at step: 2080 lr 0.00013122000000000003
(Val @ epoch 259) acc: 0.9210526315789473; ap: 0.9749422707950595
(Val @ epoch 260) acc: 0.9144736842105263; ap: 0.9756949879622362
(Val @ epoch 261) acc: 0.9210526315789473; ap: 0.9761846439725765
(Val @ epoch 262) acc: 0.9276315789473685; ap: 0.9760422182879996
(Val @ epoch 263) acc: 0.9144736842105263; ap: 0.9734803709922286
2024_03_28_19_08_45 Train loss: 0.6341734528541565 at step: 2120 lr 0.00013122000000000003
(Val @ epoch 264) acc: 0.9342105263157895; ap: 0.9752742569796743
(Val @ epoch 265) acc: 0.9210526315789473; ap: 0.9752478874203754
(Val @ epoch 266) acc: 0.9342105263157895; ap: 0.9754353046543369
(Val @ epoch 267) acc: 0.9210526315789473; ap: 0.9742670051149935
(Val @ epoch 268) acc: 0.9144736842105263; ap: 0.978891391240131
2024_03_28_19_08_55 Train loss: 0.40020751953125 at step: 2160 lr 0.00013122000000000003
(Val @ epoch 269) acc: 0.8881578947368421; ap: 0.9745391203985917
(Val @ epoch 270) acc: 0.875; ap: 0.9687100619089731
(Val @ epoch 271) acc: 0.9276315789473685; ap: 0.9730148020101517
(Val @ epoch 272) acc: 0.9013157894736842; ap: 0.9741967283694367
(Val @ epoch 273) acc: 0.9342105263157895; ap: 0.9763549965457351
2024_03_28_19_09_04 Train loss: 0.3344145715236664 at step: 2200 lr 0.00013122000000000003
(Val @ epoch 274) acc: 0.9407894736842105; ap: 0.97660855370937
(Val @ epoch 275) acc: 0.9276315789473685; ap: 0.9783188289137877
(Val @ epoch 276) acc: 0.9342105263157895; ap: 0.976871202099093
(Val @ epoch 277) acc: 0.9342105263157895; ap: 0.9758212032371221
(Val @ epoch 278) acc: 0.8947368421052632; ap: 0.9701168848429673
2024_03_28_19_09_13 Train loss: 0.2306492030620575 at step: 2240 lr 0.00013122000000000003
(Val @ epoch 279) acc: 0.9407894736842105; ap: 0.9747065602690101
(Val @ epoch 280) acc: 0.875; ap: 0.9756939033162926
(Val @ epoch 281) acc: 0.9407894736842105; ap: 0.9762762182197083
(Val @ epoch 282) acc: 0.9144736842105263; ap: 0.977793451340303
(Val @ epoch 283) acc: 0.9407894736842105; ap: 0.9781073451364193
2024_03_28_19_09_22 Train loss: 0.5556620955467224 at step: 2280 lr 0.00013122000000000003
(Val @ epoch 284) acc: 0.9407894736842105; ap: 0.9792241801772446
(Val @ epoch 285) acc: 0.9210526315789473; ap: 0.9776618263838011
(Val @ epoch 286) acc: 0.9210526315789473; ap: 0.9761560931681277
(Val @ epoch 287) acc: 0.8947368421052632; ap: 0.9765775530812076
(Val @ epoch 288) acc: 0.9013157894736842; ap: 0.9765292036306785
2024_03_28_19_09_31 Train loss: 0.4580938220024109 at step: 2320 lr 0.00013122000000000003
(Val @ epoch 289) acc: 0.8947368421052632; ap: 0.9769947938134061
(Val @ epoch 290) acc: 0.9078947368421053; ap: 0.9733978942625557
(Val @ epoch 291) acc: 0.9407894736842105; ap: 0.9756871890155786
(Val @ epoch 292) acc: 0.9342105263157895; ap: 0.975621679253407
(Val @ epoch 293) acc: 0.9342105263157895; ap: 0.9776316627671485
2024_03_28_19_09_41 Train loss: 0.18236252665519714 at step: 2360 lr 0.00013122000000000003
(Val @ epoch 294) acc: 0.9407894736842105; ap: 0.978872177385894
(Val @ epoch 295) acc: 0.9342105263157895; ap: 0.9785690974346194
(Val @ epoch 296) acc: 0.9342105263157895; ap: 0.9787697000603582
(Val @ epoch 297) acc: 0.9276315789473685; ap: 0.977097410411023
(Val @ epoch 298) acc: 0.9144736842105263; ap: 0.9787087621555516
2024_03_28_19_09_50 Train loss: 0.27557024359703064 at step: 2400 lr 0.00013122000000000003
(Val @ epoch 299) acc: 0.9407894736842105; ap: 0.9792434697111468
2024_03_28_19_09_52 changing lr at the end of epoch 300, iters 2408
*************************
Changing lr from 0.00013122000000000003 to 0.00011809800000000003
*************************
(Val @ epoch 300) acc: 0.9473684210526315; ap: 0.9788795161732533
(Val @ epoch 301) acc: 0.9473684210526315; ap: 0.9799066554250664
(Val @ epoch 302) acc: 0.9144736842105263; ap: 0.9779328836740531
(Val @ epoch 303) acc: 0.9473684210526315; ap: 0.979730718430771
2024_03_28_19_09_59 Train loss: 0.7747440338134766 at step: 2440 lr 0.00011809800000000003
(Val @ epoch 304) acc: 0.9473684210526315; ap: 0.9805068812065032
(Val @ epoch 305) acc: 0.8947368421052632; ap: 0.979157358440763
(Val @ epoch 306) acc: 0.9276315789473685; ap: 0.9748569771340242
(Val @ epoch 307) acc: 0.881578947368421; ap: 0.9778017664967498
(Val @ epoch 308) acc: 0.9342105263157895; ap: 0.9805023087662523
2024_03_28_19_10_08 Train loss: 0.8678621053695679 at step: 2480 lr 0.00011809800000000003
(Val @ epoch 309) acc: 0.881578947368421; ap: 0.9774545542746307
(Val @ epoch 310) acc: 0.8947368421052632; ap: 0.9780661462912462
(Val @ epoch 311) acc: 0.9473684210526315; ap: 0.9800613599503141
(Val @ epoch 312) acc: 0.9473684210526315; ap: 0.9808657970158737
(Val @ epoch 313) acc: 0.9407894736842105; ap: 0.9812154102622214
2024_03_28_19_10_17 Train loss: 0.23611055314540863 at step: 2520 lr 0.00011809800000000003
(Val @ epoch 314) acc: 0.9342105263157895; ap: 0.9816374026789898
(Val @ epoch 315) acc: 0.9407894736842105; ap: 0.9804839008194445
(Val @ epoch 316) acc: 0.9407894736842105; ap: 0.9813433012469985
(Val @ epoch 317) acc: 0.8947368421052632; ap: 0.9798503960427404
(Val @ epoch 318) acc: 0.9473684210526315; ap: 0.980596299755568
2024_03_28_19_10_26 Train loss: 0.4392779469490051 at step: 2560 lr 0.00011809800000000003
(Val @ epoch 319) acc: 0.9276315789473685; ap: 0.98141744503377
(Val @ epoch 320) acc: 0.9210526315789473; ap: 0.9802371443363948
(Val @ epoch 321) acc: 0.9144736842105263; ap: 0.9794571210267278
(Val @ epoch 322) acc: 0.9473684210526315; ap: 0.9815156589239082
(Val @ epoch 323) acc: 0.8947368421052632; ap: 0.9791152741359817
2024_03_28_19_10_35 Train loss: 0.900898277759552 at step: 2600 lr 0.00011809800000000003
(Val @ epoch 324) acc: 0.9144736842105263; ap: 0.9786001986737232
(Val @ epoch 325) acc: 0.9473684210526315; ap: 0.9812920754606473
(Val @ epoch 326) acc: 0.9407894736842105; ap: 0.9805695896530403
(Val @ epoch 327) acc: 0.9539473684210527; ap: 0.9811862798405319
(Val @ epoch 328) acc: 0.9407894736842105; ap: 0.9806796534204313
2024_03_28_19_10_44 Train loss: 0.1928516924381256 at step: 2640 lr 0.00011809800000000003
(Val @ epoch 329) acc: 0.9210526315789473; ap: 0.9805495165304918
(Val @ epoch 330) acc: 0.8881578947368421; ap: 0.9797091665743543
(Val @ epoch 331) acc: 0.9013157894736842; ap: 0.9773626422085144
(Val @ epoch 332) acc: 0.9473684210526315; ap: 0.9809395924779363
(Val @ epoch 333) acc: 0.9210526315789473; ap: 0.9777463054162154
2024_03_28_19_10_53 Train loss: 0.17000912129878998 at step: 2680 lr 0.00011809800000000003
(Val @ epoch 334) acc: 0.9407894736842105; ap: 0.9797238340422378
(Val @ epoch 335) acc: 0.9210526315789473; ap: 0.9799967213497871
(Val @ epoch 336) acc: 0.8881578947368421; ap: 0.9785313823391207
(Val @ epoch 337) acc: 0.8618421052631579; ap: 0.9782248516581187
(Val @ epoch 338) acc: 0.9013157894736842; ap: 0.9797749685937175
2024_03_28_19_11_03 Train loss: 0.3029385805130005 at step: 2720 lr 0.00011809800000000003
(Val @ epoch 339) acc: 0.9276315789473685; ap: 0.981003265219044
(Val @ epoch 340) acc: 0.9473684210526315; ap: 0.9809833733127067
(Val @ epoch 341) acc: 0.9407894736842105; ap: 0.9823090421805846
(Val @ epoch 342) acc: 0.9276315789473685; ap: 0.9776024749316224
(Val @ epoch 343) acc: 0.9210526315789473; ap: 0.9837080641478769
2024_03_28_19_11_12 Train loss: 0.3816249966621399 at step: 2760 lr 0.00011809800000000003
(Val @ epoch 344) acc: 0.9078947368421053; ap: 0.9818377867363342
(Val @ epoch 345) acc: 0.9276315789473685; ap: 0.9833746297244551
(Val @ epoch 346) acc: 0.9342105263157895; ap: 0.9826896087351761
(Val @ epoch 347) acc: 0.9473684210526315; ap: 0.9829216372752777
(Val @ epoch 348) acc: 0.9276315789473685; ap: 0.981864075248683
2024_03_28_19_11_21 Train loss: 0.5213299989700317 at step: 2800 lr 0.00011809800000000003
(Val @ epoch 349) acc: 0.9407894736842105; ap: 0.9825299739557487
(Val @ epoch 350) acc: 0.9342105263157895; ap: 0.982682635537655
(Val @ epoch 351) acc: 0.9473684210526315; ap: 0.982418898171466
(Val @ epoch 352) acc: 0.9539473684210527; ap: 0.9819201214301475
(Val @ epoch 353) acc: 0.9276315789473685; ap: 0.9840821804179392
2024_03_28_19_11_30 Train loss: 0.25713467597961426 at step: 2840 lr 0.00011809800000000003
(Val @ epoch 354) acc: 0.9276315789473685; ap: 0.9840587628470737
(Val @ epoch 355) acc: 0.9407894736842105; ap: 0.9843767309688842
(Val @ epoch 356) acc: 0.9342105263157895; ap: 0.9809155794243293
(Val @ epoch 357) acc: 0.9078947368421053; ap: 0.9787281499387319
(Val @ epoch 358) acc: 0.9407894736842105; ap: 0.9844479566490199
2024_03_28_19_11_39 Train loss: 0.2672828435897827 at step: 2880 lr 0.00011809800000000003
(Val @ epoch 359) acc: 0.9144736842105263; ap: 0.98325885223334
2024_03_28_19_11_41 changing lr at the end of epoch 360, iters 2888
*************************
Changing lr from 0.00011809800000000003 to 0.00010628820000000004
*************************
(Val @ epoch 360) acc: 0.9342105263157895; ap: 0.9797317355667612
(Val @ epoch 361) acc: 0.9407894736842105; ap: 0.9840034707456842
(Val @ epoch 362) acc: 0.9144736842105263; ap: 0.9777046998083971
(Val @ epoch 363) acc: 0.9342105263157895; ap: 0.980610940925223
2024_03_28_19_11_48 Train loss: 0.7999083399772644 at step: 2920 lr 0.00010628820000000004
(Val @ epoch 364) acc: 0.9407894736842105; ap: 0.9809295647186338
(Val @ epoch 365) acc: 0.9342105263157895; ap: 0.9812916508485615
(Val @ epoch 366) acc: 0.9473684210526315; ap: 0.982515612977544
(Val @ epoch 367) acc: 0.9539473684210527; ap: 0.9840949985895819
(Val @ epoch 368) acc: 0.9276315789473685; ap: 0.9831247954959734
2024_03_28_19_11_57 Train loss: 0.2893046736717224 at step: 2960 lr 0.00010628820000000004
(Val @ epoch 369) acc: 0.9473684210526315; ap: 0.9831634547347834
(Val @ epoch 370) acc: 0.9078947368421053; ap: 0.9829419276847056
(Val @ epoch 371) acc: 0.9342105263157895; ap: 0.9792374120646039
(Val @ epoch 372) acc: 0.9144736842105263; ap: 0.9832875378782449
(Val @ epoch 373) acc: 0.9539473684210527; ap: 0.9844416299107053
2024_03_28_19_12_07 Train loss: 0.4171328842639923 at step: 3000 lr 0.00010628820000000004
(Val @ epoch 374) acc: 0.9210526315789473; ap: 0.9824386791831872
(Val @ epoch 375) acc: 0.9276315789473685; ap: 0.9827318277306248
(Val @ epoch 376) acc: 0.9407894736842105; ap: 0.9841573472837246
(Val @ epoch 377) acc: 0.9276315789473685; ap: 0.9827163643653518
(Val @ epoch 378) acc: 0.9407894736842105; ap: 0.9845202638804688
2024_03_28_19_12_16 Train loss: 0.2709995210170746 at step: 3040 lr 0.00010628820000000004
(Val @ epoch 379) acc: 0.9539473684210527; ap: 0.9837292102808437
(Val @ epoch 380) acc: 0.9407894736842105; ap: 0.9839724487758114
(Val @ epoch 381) acc: 0.9210526315789473; ap: 0.9825925692407569
(Val @ epoch 382) acc: 0.9473684210526315; ap: 0.983497277594269
(Val @ epoch 383) acc: 0.9078947368421053; ap: 0.9828873990809767
2024_03_28_19_12_25 Train loss: 0.14623311161994934 at step: 3080 lr 0.00010628820000000004
(Val @ epoch 384) acc: 0.9144736842105263; ap: 0.9768571187284203
(Val @ epoch 385) acc: 0.9407894736842105; ap: 0.9810606040560622
(Val @ epoch 386) acc: 0.9407894736842105; ap: 0.9845609982760187
(Val @ epoch 387) acc: 0.9210526315789473; ap: 0.9833533695090906
(Val @ epoch 388) acc: 0.9473684210526315; ap: 0.983990265846995
2024_03_28_19_12_34 Train loss: 0.3591475188732147 at step: 3120 lr 0.00010628820000000004
(Val @ epoch 389) acc: 0.9342105263157895; ap: 0.9854622064936411
(Val @ epoch 390) acc: 0.9473684210526315; ap: 0.9873201580833061
(Val @ epoch 391) acc: 0.9276315789473685; ap: 0.9858262698889292
(Val @ epoch 392) acc: 0.9276315789473685; ap: 0.9846074482305689
(Val @ epoch 393) acc: 0.9342105263157895; ap: 0.9831976837384098
2024_03_28_19_12_43 Train loss: 0.188369020819664 at step: 3160 lr 0.00010628820000000004
(Val @ epoch 394) acc: 0.9539473684210527; ap: 0.9855116800127048
(Val @ epoch 395) acc: 0.9276315789473685; ap: 0.9837164567838371
(Val @ epoch 396) acc: 0.9407894736842105; ap: 0.9838646689790341
(Val @ epoch 397) acc: 0.9473684210526315; ap: 0.9842946205761212
(Val @ epoch 398) acc: 0.9539473684210527; ap: 0.9852069891288896
2024_03_28_19_12_52 Train loss: 0.23465344309806824 at step: 3200 lr 0.00010628820000000004
(Val @ epoch 399) acc: 0.9078947368421053; ap: 0.9870444648589632
(Val @ epoch 400) acc: 0.9473684210526315; ap: 0.9856966180703698
(Val @ epoch 401) acc: 0.9671052631578947; ap: 0.9866921107465604
(Val @ epoch 402) acc: 0.9144736842105263; ap: 0.985588350305863
(Val @ epoch 403) acc: 0.9144736842105263; ap: 0.9843057666993063
2024_03_28_19_13_02 Train loss: 0.647045910358429 at step: 3240 lr 0.00010628820000000004
(Val @ epoch 404) acc: 0.9276315789473685; ap: 0.9850314695364875
(Val @ epoch 405) acc: 0.9473684210526315; ap: 0.9864768894134844
(Val @ epoch 406) acc: 0.9342105263157895; ap: 0.9849209255547481
(Val @ epoch 407) acc: 0.9342105263157895; ap: 0.9855113765449429
(Val @ epoch 408) acc: 0.9078947368421053; ap: 0.9852065931219336
2024_03_28_19_13_11 Train loss: 0.2888765037059784 at step: 3280 lr 0.00010628820000000004
(Val @ epoch 409) acc: 0.9144736842105263; ap: 0.9838527846172201
(Val @ epoch 410) acc: 0.9210526315789473; ap: 0.9847761247987099
(Val @ epoch 411) acc: 0.9539473684210527; ap: 0.9859199975651662
(Val @ epoch 412) acc: 0.9473684210526315; ap: 0.9857654136102727
(Val @ epoch 413) acc: 0.9473684210526315; ap: 0.9864730039427562
2024_03_28_19_13_20 Train loss: 0.26432883739471436 at step: 3320 lr 0.00010628820000000004
(Val @ epoch 414) acc: 0.9078947368421053; ap: 0.9849466295641642
(Val @ epoch 415) acc: 0.9276315789473685; ap: 0.9842356778498648
(Val @ epoch 416) acc: 0.9407894736842105; ap: 0.9850628733658963
(Val @ epoch 417) acc: 0.9210526315789473; ap: 0.9850435552452901
(Val @ epoch 418) acc: 0.9013157894736842; ap: 0.983889120331129
2024_03_28_19_13_29 Train loss: 0.43773388862609863 at step: 3360 lr 0.00010628820000000004
(Val @ epoch 419) acc: 0.9276315789473685; ap: 0.9843657719364755
2024_03_28_19_13_31 changing lr at the end of epoch 420, iters 3368
*************************
Changing lr from 0.00010628820000000004 to 9.565938000000004e-05
*************************
(Val @ epoch 420) acc: 0.9276315789473685; ap: 0.9841162056801427
(Val @ epoch 421) acc: 0.9473684210526315; ap: 0.9856129512588754
(Val @ epoch 422) acc: 0.9342105263157895; ap: 0.985672530404957
(Val @ epoch 423) acc: 0.9276315789473685; ap: 0.985693150914845
2024_03_28_19_13_38 Train loss: 0.33072423934936523 at step: 3400 lr 9.565938000000004e-05
(Val @ epoch 424) acc: 0.9342105263157895; ap: 0.9866302523003534
(Val @ epoch 425) acc: 0.9473684210526315; ap: 0.9866852478429392
(Val @ epoch 426) acc: 0.9342105263157895; ap: 0.9869418439450456
(Val @ epoch 427) acc: 0.9276315789473685; ap: 0.9850573832313043
(Val @ epoch 428) acc: 0.9342105263157895; ap: 0.9851244364987681
2024_03_28_19_13_47 Train loss: 0.326159805059433 at step: 3440 lr 9.565938000000004e-05
(Val @ epoch 429) acc: 0.9473684210526315; ap: 0.9878134791430555
(Val @ epoch 430) acc: 0.9276315789473685; ap: 0.9887250085829004
(Val @ epoch 431) acc: 0.9539473684210527; ap: 0.988334379996783
(Val @ epoch 432) acc: 0.9539473684210527; ap: 0.988334379996783
(Val @ epoch 433) acc: 0.9276315789473685; ap: 0.9882149342251633
2024_03_28_19_13_57 Train loss: 0.6869920492172241 at step: 3480 lr 9.565938000000004e-05
(Val @ epoch 434) acc: 0.9078947368421053; ap: 0.9886462471407674
(Val @ epoch 435) acc: 0.9144736842105263; ap: 0.9805765418145156
(Val @ epoch 436) acc: 0.9473684210526315; ap: 0.9851658538493956
(Val @ epoch 437) acc: 0.9539473684210527; ap: 0.9875196658487209
(Val @ epoch 438) acc: 0.9473684210526315; ap: 0.9877825603216654
2024_03_28_19_14_06 Train loss: 0.14803458750247955 at step: 3520 lr 9.565938000000004e-05
(Val @ epoch 439) acc: 0.9210526315789473; ap: 0.9879207735016743
(Val @ epoch 440) acc: 0.9342105263157895; ap: 0.9871465303244882
(Val @ epoch 441) acc: 0.9539473684210527; ap: 0.9870780200311732
(Val @ epoch 442) acc: 0.9539473684210527; ap: 0.9879207735016743
(Val @ epoch 443) acc: 0.9407894736842105; ap: 0.9887771834559143
2024_03_28_19_14_15 Train loss: 0.498393177986145 at step: 3560 lr 9.565938000000004e-05
(Val @ epoch 444) acc: 0.9342105263157895; ap: 0.9874413547274106
(Val @ epoch 445) acc: 0.9539473684210527; ap: 0.9876992501928137
(Val @ epoch 446) acc: 0.9342105263157895; ap: 0.9881411010364963
(Val @ epoch 447) acc: 0.9342105263157895; ap: 0.9879407930178536
(Val @ epoch 448) acc: 0.9342105263157895; ap: 0.9855952756306556
2024_03_28_19_14_24 Train loss: 0.24267883598804474 at step: 3600 lr 9.565938000000004e-05
(Val @ epoch 449) acc: 0.8618421052631579; ap: 0.9878256316541344
(Val @ epoch 450) acc: 0.9473684210526315; ap: 0.9872689201819465
(Val @ epoch 451) acc: 0.9210526315789473; ap: 0.9914772649430935
(Val @ epoch 452) acc: 0.9539473684210527; ap: 0.991722011369687
(Val @ epoch 453) acc: 0.9671052631578947; ap: 0.9903055055432998
2024_03_28_19_14_33 Train loss: 0.16280335187911987 at step: 3640 lr 9.565938000000004e-05
(Val @ epoch 454) acc: 0.9539473684210527; ap: 0.9894328985880791
(Val @ epoch 455) acc: 0.9539473684210527; ap: 0.9902711045121774
(Val @ epoch 456) acc: 0.9407894736842105; ap: 0.9884991812052946
(Val @ epoch 457) acc: 0.9078947368421053; ap: 0.9875888291864799
(Val @ epoch 458) acc: 0.8947368421052632; ap: 0.9871419492688432
2024_03_28_19_14_42 Train loss: 0.2646973729133606 at step: 3680 lr 9.565938000000004e-05
(Val @ epoch 459) acc: 0.9342105263157895; ap: 0.9865919847810627
(Val @ epoch 460) acc: 0.9539473684210527; ap: 0.9879010683539657
(Val @ epoch 461) acc: 0.9539473684210527; ap: 0.9892624340041295
(Val @ epoch 462) acc: 0.9539473684210527; ap: 0.9881290018998945
(Val @ epoch 463) acc: 0.9539473684210527; ap: 0.9878604734358774
2024_03_28_19_14_51 Train loss: 0.23587572574615479 at step: 3720 lr 9.565938000000004e-05
(Val @ epoch 464) acc: 0.9539473684210527; ap: 0.9907957772216742
(Val @ epoch 465) acc: 0.9473684210526315; ap: 0.988886275845041
(Val @ epoch 466) acc: 0.9605263157894737; ap: 0.9886482638976921
(Val @ epoch 467) acc: 0.9473684210526315; ap: 0.9889023845917801
(Val @ epoch 468) acc: 0.9407894736842105; ap: 0.9892129079883685
2024_03_28_19_15_00 Train loss: 0.16327759623527527 at step: 3760 lr 9.565938000000004e-05
(Val @ epoch 469) acc: 0.9407894736842105; ap: 0.9892739369980605
(Val @ epoch 470) acc: 0.9276315789473685; ap: 0.9893365267269818
(Val @ epoch 471) acc: 0.9473684210526315; ap: 0.9878761758373706
(Val @ epoch 472) acc: 0.9276315789473685; ap: 0.9884333915261653
(Val @ epoch 473) acc: 0.9144736842105263; ap: 0.9902855682017285
2024_03_28_19_15_10 Train loss: 1.145691156387329 at step: 3800 lr 9.565938000000004e-05
(Val @ epoch 474) acc: 0.9342105263157895; ap: 0.9913586103653875
(Val @ epoch 475) acc: 0.9605263157894737; ap: 0.9885943047878326
(Val @ epoch 476) acc: 0.9671052631578947; ap: 0.9897074179062745
(Val @ epoch 477) acc: 0.9210526315789473; ap: 0.9893379031636487
(Val @ epoch 478) acc: 0.8947368421052632; ap: 0.9862636777378753
2024_03_28_19_15_19 Train loss: 0.2346876859664917 at step: 3840 lr 9.565938000000004e-05
(Val @ epoch 479) acc: 0.9407894736842105; ap: 0.988655060191953
2024_03_28_19_15_21 changing lr at the end of epoch 480, iters 3848
*************************
Changing lr from 9.565938000000004e-05 to 8.609344200000004e-05
*************************
(Val @ epoch 480) acc: 0.9605263157894737; ap: 0.9904372027257317
(Val @ epoch 481) acc: 0.9342105263157895; ap: 0.9899525564855162
(Val @ epoch 482) acc: 0.9342105263157895; ap: 0.9879314864496639
(Val @ epoch 483) acc: 0.9605263157894737; ap: 0.9884991812052946
2024_03_28_19_15_28 Train loss: 0.2279302328824997 at step: 3880 lr 8.609344200000004e-05
(Val @ epoch 484) acc: 0.9539473684210527; ap: 0.9887899444988639
(Val @ epoch 485) acc: 0.9605263157894737; ap: 0.9904869319264579
(Val @ epoch 486) acc: 0.9473684210526315; ap: 0.989781797578188
(Val @ epoch 487) acc: 0.9605263157894737; ap: 0.9881918456324454
(Val @ epoch 488) acc: 0.9605263157894737; ap: 0.9892961684395596
2024_03_28_19_15_37 Train loss: 0.2415488213300705 at step: 3920 lr 8.609344200000004e-05
(Val @ epoch 489) acc: 0.9605263157894737; ap: 0.9893466743422138
(Val @ epoch 490) acc: 0.9473684210526315; ap: 0.99138748815918
(Val @ epoch 491) acc: 0.9473684210526315; ap: 0.9918660963834381
(Val @ epoch 492) acc: 0.9605263157894737; ap: 0.9899915884573034
(Val @ epoch 493) acc: 0.9605263157894737; ap: 0.9892151403025028
2024_03_28_19_15_46 Train loss: 0.0857049748301506 at step: 3960 lr 8.609344200000004e-05
(Val @ epoch 494) acc: 0.9539473684210527; ap: 0.9899158058887804
(Val @ epoch 495) acc: 0.9539473684210527; ap: 0.990252126406453
(Val @ epoch 496) acc: 0.9473684210526315; ap: 0.9907505653755359
(Val @ epoch 497) acc: 0.9605263157894737; ap: 0.9899498121577346
(Val @ epoch 498) acc: 0.9539473684210527; ap: 0.9906959859818905
2024_03_28_19_15_55 Train loss: 0.2697548568248749 at step: 4000 lr 8.609344200000004e-05
(Val @ epoch 499) acc: 0.8947368421052632; ap: 0.989247652865076
(Val @ epoch 500) acc: 0.9473684210526315; ap: 0.9880465461702298
(Val @ epoch 501) acc: 0.9473684210526315; ap: 0.9915459921007043
(Val @ epoch 502) acc: 0.9539473684210527; ap: 0.99237681180935
(Val @ epoch 503) acc: 0.9539473684210527; ap: 0.991451084742902
2024_03_28_19_16_05 Train loss: 0.30897149443626404 at step: 4040 lr 8.609344200000004e-05
(Val @ epoch 504) acc: 0.9539473684210527; ap: 0.9891755070541363
(Val @ epoch 505) acc: 0.9473684210526315; ap: 0.9904374492393466
(Val @ epoch 506) acc: 0.9539473684210527; ap: 0.9902340364162853
(Val @ epoch 507) acc: 0.9539473684210527; ap: 0.9900953900880574
(Val @ epoch 508) acc: 0.9473684210526315; ap: 0.990768434013751
2024_03_28_19_16_14 Train loss: 0.1064595878124237 at step: 4080 lr 8.609344200000004e-05
(Val @ epoch 509) acc: 0.9605263157894737; ap: 0.9893470451214146
(Val @ epoch 510) acc: 0.9473684210526315; ap: 0.9893470451214146
(Val @ epoch 511) acc: 0.9539473684210527; ap: 0.9912596868818039
(Val @ epoch 512) acc: 0.9605263157894737; ap: 0.9902789474042909
(Val @ epoch 513) acc: 0.9473684210526315; ap: 0.9914433197884348
2024_03_28_19_16_23 Train loss: 0.15954351425170898 at step: 4120 lr 8.609344200000004e-05
(Val @ epoch 514) acc: 0.9276315789473685; ap: 0.9913431245104021
(Val @ epoch 515) acc: 0.8947368421052632; ap: 0.9884789790043914
(Val @ epoch 516) acc: 0.9539473684210527; ap: 0.9922303851951518
(Val @ epoch 517) acc: 0.9407894736842105; ap: 0.9917137980815076
(Val @ epoch 518) acc: 0.9473684210526315; ap: 0.9904349697323612
2024_03_28_19_16_32 Train loss: 0.24541237950325012 at step: 4160 lr 8.609344200000004e-05
(Val @ epoch 519) acc: 0.9407894736842105; ap: 0.9899171255971623
(Val @ epoch 520) acc: 0.9605263157894737; ap: 0.9889101802784377
(Val @ epoch 521) acc: 0.9342105263157895; ap: 0.9899403721585955
(Val @ epoch 522) acc: 0.9013157894736842; ap: 0.9886939062855515
(Val @ epoch 523) acc: 0.9407894736842105; ap: 0.988754528975772
2024_03_28_19_16_41 Train loss: 0.18102985620498657 at step: 4200 lr 8.609344200000004e-05
(Val @ epoch 524) acc: 0.9539473684210527; ap: 0.9881035714981665
(Val @ epoch 525) acc: 0.9539473684210527; ap: 0.9898843786599685
(Val @ epoch 526) acc: 0.9539473684210527; ap: 0.9896630009643119
(Val @ epoch 527) acc: 0.9539473684210527; ap: 0.991818441971895
(Val @ epoch 528) acc: 0.9407894736842105; ap: 0.9898020444016224
2024_03_28_19_16_50 Train loss: 0.3778143525123596 at step: 4240 lr 8.609344200000004e-05
(Val @ epoch 529) acc: 0.9539473684210527; ap: 0.9911184349365679
(Val @ epoch 530) acc: 0.9605263157894737; ap: 0.9915969042131411
(Val @ epoch 531) acc: 0.9605263157894737; ap: 0.9907421847443757
(Val @ epoch 532) acc: 0.9473684210526315; ap: 0.9903891316612736
(Val @ epoch 533) acc: 0.9473684210526315; ap: 0.992156452201927
2024_03_28_19_16_59 Train loss: 0.1275406777858734 at step: 4280 lr 8.609344200000004e-05
(Val @ epoch 534) acc: 0.9671052631578947; ap: 0.9917432661208528
(Val @ epoch 535) acc: 0.9539473684210527; ap: 0.9893929253167281
(Val @ epoch 536) acc: 0.9473684210526315; ap: 0.99156602554676
(Val @ epoch 537) acc: 0.9144736842105263; ap: 0.9910280079982265
(Val @ epoch 538) acc: 0.9407894736842105; ap: 0.9914739765330266
2024_03_28_19_17_09 Train loss: 0.21759049594402313 at step: 4320 lr 8.609344200000004e-05
(Val @ epoch 539) acc: 0.9539473684210527; ap: 0.9912146409415764
2024_03_28_19_17_11 changing lr at the end of epoch 540, iters 4328
*************************
Changing lr from 8.609344200000004e-05 to 7.748409780000004e-05
*************************
(Val @ epoch 540) acc: 0.9539473684210527; ap: 0.9927400768817886
(Val @ epoch 541) acc: 0.9539473684210527; ap: 0.9938591003247914
(Val @ epoch 542) acc: 0.9605263157894737; ap: 0.9934501762455125
(Val @ epoch 543) acc: 0.9539473684210527; ap: 0.9923543235469007
2024_03_28_19_17_18 Train loss: 0.18681436777114868 at step: 4360 lr 7.748409780000004e-05
(Val @ epoch 544) acc: 0.9473684210526315; ap: 0.9932816628567774
(Val @ epoch 545) acc: 0.9605263157894737; ap: 0.9926167897424709
(Val @ epoch 546) acc: 0.9473684210526315; ap: 0.9927816229211283
(Val @ epoch 547) acc: 0.9342105263157895; ap: 0.9918967460639924
(Val @ epoch 548) acc: 0.9539473684210527; ap: 0.989833955331847
2024_03_28_19_17_27 Train loss: 0.09651362895965576 at step: 4400 lr 7.748409780000004e-05
(Val @ epoch 549) acc: 0.9605263157894737; ap: 0.9918509718339281
(Val @ epoch 550) acc: 0.9539473684210527; ap: 0.9919755764190548
(Val @ epoch 551) acc: 0.9276315789473685; ap: 0.9918534762969546
(Val @ epoch 552) acc: 0.9473684210526315; ap: 0.9915401019684995
(Val @ epoch 553) acc: 0.9473684210526315; ap: 0.9918743405408024
2024_03_28_19_17_36 Train loss: 0.05651674419641495 at step: 4440 lr 7.748409780000004e-05
(Val @ epoch 554) acc: 0.9473684210526315; ap: 0.993202094340667
(Val @ epoch 555) acc: 0.9539473684210527; ap: 0.9933582402433985
(Val @ epoch 556) acc: 0.9605263157894737; ap: 0.9921145425345242
(Val @ epoch 557) acc: 0.9605263157894737; ap: 0.989738692788829
(Val @ epoch 558) acc: 0.9473684210526315; ap: 0.9901280952651174
2024_03_28_19_17_45 Train loss: 0.11330226808786392 at step: 4480 lr 7.748409780000004e-05
(Val @ epoch 559) acc: 0.9539473684210527; ap: 0.9911880471413319
(Val @ epoch 560) acc: 0.9473684210526315; ap: 0.9921788232571646
(Val @ epoch 561) acc: 0.9407894736842105; ap: 0.992481781070277
(Val @ epoch 562) acc: 0.9013157894736842; ap: 0.9878288065709759
(Val @ epoch 563) acc: 0.9539473684210527; ap: 0.9896267118609913
2024_03_28_19_17_54 Train loss: 0.08167024701833725 at step: 4520 lr 7.748409780000004e-05
(Val @ epoch 564) acc: 0.9539473684210527; ap: 0.9910264045943301
(Val @ epoch 565) acc: 0.9539473684210527; ap: 0.9913870693114097
(Val @ epoch 566) acc: 0.9605263157894737; ap: 0.9917308929776162
(Val @ epoch 567) acc: 0.9473684210526315; ap: 0.9893957963893826
(Val @ epoch 568) acc: 0.9605263157894737; ap: 0.9902125787662773
2024_03_28_19_18_04 Train loss: 0.27812516689300537 at step: 4560 lr 7.748409780000004e-05
(Val @ epoch 569) acc: 0.9539473684210527; ap: 0.9901743014115929
(Val @ epoch 570) acc: 0.9605263157894737; ap: 0.9899595423290303
(Val @ epoch 571) acc: 0.9605263157894737; ap: 0.9903274290262768
(Val @ epoch 572) acc: 0.9473684210526315; ap: 0.9893346267717684
(Val @ epoch 573) acc: 0.9473684210526315; ap: 0.9885536216818959
2024_03_28_19_18_13 Train loss: 0.35866063833236694 at step: 4600 lr 7.748409780000004e-05
(Val @ epoch 574) acc: 0.9473684210526315; ap: 0.989943996835098
(Val @ epoch 575) acc: 0.9407894736842105; ap: 0.9920671367682337
(Val @ epoch 576) acc: 0.9473684210526315; ap: 0.9919309494123938
(Val @ epoch 577) acc: 0.9473684210526315; ap: 0.9906435159284759
(Val @ epoch 578) acc: 0.9605263157894737; ap: 0.9925424006926115
2024_03_28_19_18_22 Train loss: 0.2430059015750885 at step: 4640 lr 7.748409780000004e-05
(Val @ epoch 579) acc: 0.9539473684210527; ap: 0.9920925252734968
(Val @ epoch 580) acc: 0.9671052631578947; ap: 0.9928137453074324
(Val @ epoch 581) acc: 0.9736842105263158; ap: 0.9915127233190508
(Val @ epoch 582) acc: 0.9539473684210527; ap: 0.9915808108689558
(Val @ epoch 583) acc: 0.9736842105263158; ap: 0.9925344981618417
2024_03_28_19_18_31 Train loss: 0.3851061165332794 at step: 4680 lr 7.748409780000004e-05
(Val @ epoch 584) acc: 0.9671052631578947; ap: 0.9915808108689558
(Val @ epoch 585) acc: 0.9605263157894737; ap: 0.9923933917690039
(Val @ epoch 586) acc: 0.9605263157894737; ap: 0.9937342563797228
(Val @ epoch 587) acc: 0.9407894736842105; ap: 0.9931632130893725
(Val @ epoch 588) acc: 0.9539473684210527; ap: 0.989588100834485
2024_03_28_19_18_40 Train loss: 0.1086285337805748 at step: 4720 lr 7.748409780000004e-05
(Val @ epoch 589) acc: 0.9407894736842105; ap: 0.9900032513200487
(Val @ epoch 590) acc: 0.9539473684210527; ap: 0.9942751435209435
(Val @ epoch 591) acc: 0.9539473684210527; ap: 0.9934640795848797
(Val @ epoch 592) acc: 0.9342105263157895; ap: 0.9942314740772773
(Val @ epoch 593) acc: 0.9473684210526315; ap: 0.9940974017921925
2024_03_28_19_18_49 Train loss: 0.0742674171924591 at step: 4760 lr 7.748409780000004e-05
(Val @ epoch 594) acc: 0.9539473684210527; ap: 0.9930787883741246
(Val @ epoch 595) acc: 0.9605263157894737; ap: 0.9929030867197611
(Val @ epoch 596) acc: 0.9605263157894737; ap: 0.9936029279176065
(Val @ epoch 597) acc: 0.9605263157894737; ap: 0.9923605587647492
(Val @ epoch 598) acc: 0.9539473684210527; ap: 0.9939928937550163
2024_03_28_19_18_59 Train loss: 0.4848134517669678 at step: 4800 lr 7.748409780000004e-05
(Val @ epoch 599) acc: 0.9539473684210527; ap: 0.994329626107995
2024_03_28_19_19_00 changing lr at the end of epoch 600, iters 4808
*************************
Changing lr from 7.748409780000004e-05 to 6.973568802000003e-05
*************************
(Val @ epoch 600) acc: 0.9144736842105263; ap: 0.9932271610804814
(Val @ epoch 601) acc: 0.9276315789473685; ap: 0.9925171969303982
(Val @ epoch 602) acc: 0.9539473684210527; ap: 0.9932270726599368
(Val @ epoch 603) acc: 0.9539473684210527; ap: 0.9937751335651941
2024_03_28_19_19_08 Train loss: 0.0630369707942009 at step: 4840 lr 6.973568802000003e-05
(Val @ epoch 604) acc: 0.9539473684210527; ap: 0.9931731252655063
(Val @ epoch 605) acc: 0.9605263157894737; ap: 0.9930509042075265
(Val @ epoch 606) acc: 0.9605263157894737; ap: 0.9937751335651941
(Val @ epoch 607) acc: 0.9605263157894737; ap: 0.9943591553847339
(Val @ epoch 608) acc: 0.9605263157894737; ap: 0.992987751864644
2024_03_28_19_19_17 Train loss: 0.35690638422966003 at step: 4880 lr 6.973568802000003e-05
(Val @ epoch 609) acc: 0.9539473684210527; ap: 0.9936590061235336
(Val @ epoch 610) acc: 0.9736842105263158; ap: 0.9940937717979451
(Val @ epoch 611) acc: 0.9013157894736842; ap: 0.9910867281058541
(Val @ epoch 612) acc: 0.9605263157894737; ap: 0.9913318998479573
(Val @ epoch 613) acc: 0.9473684210526315; ap: 0.9937808704820239
2024_03_28_19_19_26 Train loss: 0.28119492530822754 at step: 4920 lr 6.973568802000003e-05
(Val @ epoch 614) acc: 0.9605263157894737; ap: 0.9931151253983129
(Val @ epoch 615) acc: 0.9671052631578947; ap: 0.9929365475696462
(Val @ epoch 616) acc: 0.9605263157894737; ap: 0.9926026181164417
(Val @ epoch 617) acc: 0.9539473684210527; ap: 0.9941180613222922
(Val @ epoch 618) acc: 0.9407894736842105; ap: 0.9928584759169377
2024_03_28_19_19_35 Train loss: 0.24888695776462555 at step: 4960 lr 6.973568802000003e-05
(Val @ epoch 619) acc: 0.9144736842105263; ap: 0.9940620960934339
(Val @ epoch 620) acc: 0.9539473684210527; ap: 0.9926759203007599
(Val @ epoch 621) acc: 0.9473684210526315; ap: 0.9931644224770783
(Val @ epoch 622) acc: 0.9605263157894737; ap: 0.993071600108496
(Val @ epoch 623) acc: 0.9473684210526315; ap: 0.9905679916479951
2024_03_28_19_19_44 Train loss: 0.09136369824409485 at step: 5000 lr 6.973568802000003e-05
(Val @ epoch 624) acc: 0.9539473684210527; ap: 0.989100328664841
(Val @ epoch 625) acc: 0.9407894736842105; ap: 0.9932133713272159
(Val @ epoch 626) acc: 0.9605263157894737; ap: 0.9944683971725704
(Val @ epoch 627) acc: 0.9736842105263158; ap: 0.9935612304373157
(Val @ epoch 628) acc: 0.9605263157894737; ap: 0.9945529399737726
2024_03_28_19_19_53 Train loss: 0.16795532405376434 at step: 5040 lr 6.973568802000003e-05
(Val @ epoch 629) acc: 0.9605263157894737; ap: 0.9946806212812291
(Val @ epoch 630) acc: 0.9605263157894737; ap: 0.9936556847378611
(Val @ epoch 631) acc: 0.9605263157894737; ap: 0.9931098519867442
(Val @ epoch 632) acc: 0.9605263157894737; ap: 0.9942314740772773
(Val @ epoch 633) acc: 0.9342105263157895; ap: 0.9942314740772773
2024_03_28_19_20_03 Train loss: 0.07043300569057465 at step: 5080 lr 6.973568802000003e-05
(Val @ epoch 634) acc: 0.9407894736842105; ap: 0.9928201262114567
(Val @ epoch 635) acc: 0.9539473684210527; ap: 0.9925378764455294
(Val @ epoch 636) acc: 0.9539473684210527; ap: 0.988420777428603
(Val @ epoch 637) acc: 0.9078947368421053; ap: 0.9896419681942771
(Val @ epoch 638) acc: 0.9539473684210527; ap: 0.9944781280447477
2024_03_28_19_20_12 Train loss: 0.28726959228515625 at step: 5120 lr 6.973568802000003e-05
(Val @ epoch 639) acc: 0.9605263157894737; ap: 0.9945428927729315
(Val @ epoch 640) acc: 0.9605263157894737; ap: 0.9940428598381751
(Val @ epoch 641) acc: 0.9144736842105263; ap: 0.9945276687734687
(Val @ epoch 642) acc: 0.9736842105263158; ap: 0.9945276687734687
(Val @ epoch 643) acc: 0.9539473684210527; ap: 0.99407132443345
2024_03_28_19_20_21 Train loss: 0.15109474956989288 at step: 5160 lr 6.973568802000003e-05
(Val @ epoch 644) acc: 0.9539473684210527; ap: 0.995108501212421
(Val @ epoch 645) acc: 0.9802631578947368; ap: 0.9951537473663503
(Val @ epoch 646) acc: 0.9605263157894737; ap: 0.9932616148392571
(Val @ epoch 647) acc: 0.9605263157894737; ap: 0.9914205684930917
(Val @ epoch 648) acc: 0.9605263157894737; ap: 0.9942222457372611
2024_03_28_19_20_30 Train loss: 0.27047067880630493 at step: 5200 lr 6.973568802000003e-05
(Val @ epoch 649) acc: 0.9671052631578947; ap: 0.9946366933145376
(Val @ epoch 650) acc: 0.9605263157894737; ap: 0.9940907769505062
(Val @ epoch 651) acc: 0.9671052631578947; ap: 0.9939625152143727
(Val @ epoch 652) acc: 0.9407894736842105; ap: 0.9940195312724719
(Val @ epoch 653) acc: 0.9671052631578947; ap: 0.9948030781762134
2024_03_28_19_20_39 Train loss: 0.15205831825733185 at step: 5240 lr 6.973568802000003e-05
(Val @ epoch 654) acc: 0.9539473684210527; ap: 0.9944589322531426
(Val @ epoch 655) acc: 0.9539473684210527; ap: 0.9938901585166026
(Val @ epoch 656) acc: 0.9671052631578947; ap: 0.9949843958567156
(Val @ epoch 657) acc: 0.9736842105263158; ap: 0.9952728105872567
(Val @ epoch 658) acc: 0.9539473684210527; ap: 0.9934033457004933
2024_03_28_19_20_48 Train loss: 0.9249973297119141 at step: 5280 lr 6.973568802000003e-05
(Val @ epoch 659) acc: 0.9671052631578947; ap: 0.9946088429380255
2024_03_28_19_20_50 changing lr at the end of epoch 660, iters 5288
*************************
Changing lr from 6.973568802000003e-05 to 6.276211921800003e-05
*************************
(Val @ epoch 660) acc: 0.9736842105263158; ap: 0.9944415283826193
(Val @ epoch 661) acc: 0.9736842105263158; ap: 0.9940974017921925
(Val @ epoch 662) acc: 0.9342105263157895; ap: 0.9934408093344352
(Val @ epoch 663) acc: 0.9473684210526315; ap: 0.9926694873787414
2024_03_28_19_20_57 Train loss: 0.10991311073303223 at step: 5320 lr 6.276211921800003e-05
(Val @ epoch 664) acc: 0.9605263157894737; ap: 0.991962635592084
(Val @ epoch 665) acc: 0.9539473684210527; ap: 0.9938065575260417
(Val @ epoch 666) acc: 0.9605263157894737; ap: 0.9934958628957066
(Val @ epoch 667) acc: 0.9407894736842105; ap: 0.9936029279176065
(Val @ epoch 668) acc: 0.9671052631578947; ap: 0.9934227533026653
2024_03_28_19_21_06 Train loss: 0.05866310000419617 at step: 5360 lr 6.276211921800003e-05
(Val @ epoch 669) acc: 0.9671052631578947; ap: 0.9939372521483653
(Val @ epoch 670) acc: 0.9671052631578947; ap: 0.9934958628957066
(Val @ epoch 671) acc: 0.9671052631578947; ap: 0.9928528714312127
(Val @ epoch 672) acc: 0.9473684210526315; ap: 0.9929537060840127
(Val @ epoch 673) acc: 0.9407894736842105; ap: 0.994000168727638
2024_03_28_19_21_16 Train loss: 0.3032051920890808 at step: 5400 lr 6.276211921800003e-05
(Val @ epoch 674) acc: 0.9605263157894737; ap: 0.9943407158651139
(Val @ epoch 675) acc: 0.9671052631578947; ap: 0.9945816208308449
(Val @ epoch 676) acc: 0.9407894736842105; ap: 0.9937050030685927
(Val @ epoch 677) acc: 0.9605263157894737; ap: 0.995108501212421
(Val @ epoch 678) acc: 0.9539473684210527; ap: 0.9951451292798001
2024_03_28_19_21_25 Train loss: 0.47059115767478943 at step: 5440 lr 6.276211921800003e-05
(Val @ epoch 679) acc: 0.9671052631578947; ap: 0.994787212936108
(Val @ epoch 680) acc: 0.9539473684210527; ap: 0.994209856144801
(Val @ epoch 681) acc: 0.9539473684210527; ap: 0.994362146593144
(Val @ epoch 682) acc: 0.9539473684210527; ap: 0.9943727653588315
(Val @ epoch 683) acc: 0.9736842105263158; ap: 0.9933382152722845
2024_03_28_19_21_34 Train loss: 0.14058855175971985 at step: 5480 lr 6.276211921800003e-05
(Val @ epoch 684) acc: 0.9473684210526315; ap: 0.9935025246471202
(Val @ epoch 685) acc: 0.9407894736842105; ap: 0.9951978349919205
(Val @ epoch 686) acc: 0.9605263157894737; ap: 0.9954034270971835
(Val @ epoch 687) acc: 0.9671052631578947; ap: 0.9952425921252835
(Val @ epoch 688) acc: 0.9671052631578947; ap: 0.9954481842305468
2024_03_28_19_21_43 Train loss: 0.15862029790878296 at step: 5520 lr 6.276211921800003e-05
(Val @ epoch 689) acc: 0.9802631578947368; ap: 0.9952728105872567
(Val @ epoch 690) acc: 0.9539473684210527; ap: 0.9948894767532425
(Val @ epoch 691) acc: 0.9605263157894737; ap: 0.9949395371745371
(Val @ epoch 692) acc: 0.9736842105263158; ap: 0.9941605412501727
(Val @ epoch 693) acc: 0.9605263157894737; ap: 0.9938298470136613
2024_03_28_19_21_52 Train loss: 0.32364949584007263 at step: 5560 lr 6.276211921800003e-05
(Val @ epoch 694) acc: 0.9473684210526315; ap: 0.9945866135605992
(Val @ epoch 695) acc: 0.9407894736842105; ap: 0.9928555348640501
(Val @ epoch 696) acc: 0.9539473684210527; ap: 0.9900962322041592
(Val @ epoch 697) acc: 0.9473684210526315; ap: 0.9934666972141194
(Val @ epoch 698) acc: 0.9539473684210527; ap: 0.9945339039913174
2024_03_28_19_22_01 Train loss: 0.15230277180671692 at step: 5600 lr 6.276211921800003e-05
(Val @ epoch 699) acc: 0.9605263157894737; ap: 0.9948583630099802
(Val @ epoch 700) acc: 0.9605263157894737; ap: 0.9937141368253612
(Val @ epoch 701) acc: 0.9605263157894737; ap: 0.9937946923987523
(Val @ epoch 702) acc: 0.9605263157894737; ap: 0.9950672184819936
(Val @ epoch 703) acc: 0.9802631578947368; ap: 0.9948583630099802
2024_03_28_19_22_11 Train loss: 0.11741473525762558 at step: 5640 lr 6.276211921800003e-05
(Val @ epoch 704) acc: 0.9671052631578947; ap: 0.9931526023303088
(Val @ epoch 705) acc: 0.9539473684210527; ap: 0.9922397318278028
(Val @ epoch 706) acc: 0.9605263157894737; ap: 0.9936826992620613
(Val @ epoch 707) acc: 0.9605263157894737; ap: 0.9945866135605992
(Val @ epoch 708) acc: 0.9605263157894737; ap: 0.9952728105872567
2024_03_28_19_22_20 Train loss: 0.6501945853233337 at step: 5680 lr 6.276211921800003e-05
(Val @ epoch 709) acc: 0.9671052631578947; ap: 0.9943407158651139
(Val @ epoch 710) acc: 0.9539473684210527; ap: 0.9930889145057318
(Val @ epoch 711) acc: 0.9473684210526315; ap: 0.9949060553458137
(Val @ epoch 712) acc: 0.9605263157894737; ap: 0.9943407158651139
(Val @ epoch 713) acc: 0.9605263157894737; ap: 0.9954481842305468
2024_03_28_19_22_29 Train loss: 0.20468243956565857 at step: 5720 lr 6.276211921800003e-05
(Val @ epoch 714) acc: 0.9671052631578947; ap: 0.9943407158651139
(Val @ epoch 715) acc: 0.9802631578947368; ap: 0.9951451292798001
(Val @ epoch 716) acc: 0.9736842105263158; ap: 0.9954034270971835
(Val @ epoch 717) acc: 0.9407894736842105; ap: 0.9952728105872567
(Val @ epoch 718) acc: 0.9671052631578947; ap: 0.9951458610193747
2024_03_28_19_22_38 Train loss: 0.18924933671951294 at step: 5760 lr 6.276211921800003e-05
(Val @ epoch 719) acc: 0.9210526315789473; ap: 0.9946144639371112
2024_03_28_19_22_40 changing lr at the end of epoch 720, iters 5768
*************************
Changing lr from 6.276211921800003e-05 to 5.6485907296200035e-05
*************************
(Val @ epoch 720) acc: 0.9736842105263158; ap: 0.9939474911073095
(Val @ epoch 721) acc: 0.9671052631578947; ap: 0.9944867826296546
(Val @ epoch 722) acc: 0.9605263157894737; ap: 0.9954034270971835
(Val @ epoch 723) acc: 0.9605263157894737; ap: 0.9954034270971835
2024_03_28_19_22_47 Train loss: 0.1940937638282776 at step: 5800 lr 5.6485907296200035e-05
(Val @ epoch 724) acc: 0.9736842105263158; ap: 0.9952728105872567
(Val @ epoch 725) acc: 0.9671052631578947; ap: 0.9943886305989369
(Val @ epoch 726) acc: 0.9605263157894737; ap: 0.993431102007755
(Val @ epoch 727) acc: 0.9605263157894737; ap: 0.9952728105872567
(Val @ epoch 728) acc: 0.9605263157894737; ap: 0.9954034270971835
2024_03_28_19_22_56 Train loss: 0.2974609136581421 at step: 5840 lr 5.6485907296200035e-05
(Val @ epoch 729) acc: 0.9605263157894737; ap: 0.9940883846053274
(Val @ epoch 730) acc: 0.9605263157894737; ap: 0.9950455107534913
(Val @ epoch 731) acc: 0.9210526315789473; ap: 0.9936707818858653
(Val @ epoch 732) acc: 0.9210526315789473; ap: 0.9931957362722343
(Val @ epoch 733) acc: 0.9605263157894737; ap: 0.9909874490892515
2024_03_28_19_23_05 Train loss: 0.09470295906066895 at step: 5880 lr 5.6485907296200035e-05
(Val @ epoch 734) acc: 0.9407894736842105; ap: 0.9925421788468263
(Val @ epoch 735) acc: 0.9539473684210527; ap: 0.993568882136975
(Val @ epoch 736) acc: 0.9605263157894737; ap: 0.9947427594633307
(Val @ epoch 737) acc: 0.9802631578947368; ap: 0.9954752397370544
(Val @ epoch 738) acc: 0.9539473684210527; ap: 0.9954752397370544
2024_03_28_19_23_14 Train loss: 0.14635300636291504 at step: 5920 lr 5.6485907296200035e-05
(Val @ epoch 739) acc: 0.9605263157894737; ap: 0.9947210517348285
(Val @ epoch 740) acc: 0.9605263157894737; ap: 0.9952391177223479
(Val @ epoch 741) acc: 0.9605263157894737; ap: 0.9951507807183914
(Val @ epoch 742) acc: 0.9671052631578947; ap: 0.9942547062495714
(Val @ epoch 743) acc: 0.9539473684210527; ap: 0.9943344921813115
2024_03_28_19_23_24 Train loss: 0.13254177570343018 at step: 5960 lr 5.6485907296200035e-05
(Val @ epoch 744) acc: 0.9605263157894737; ap: 0.9941402231530378
(Val @ epoch 745) acc: 0.9605263157894737; ap: 0.9944342893221702
(Val @ epoch 746) acc: 0.9539473684210527; ap: 0.9934791141913879
(Val @ epoch 747) acc: 0.9605263157894737; ap: 0.9942738733990573
(Val @ epoch 748) acc: 0.9671052631578947; ap: 0.995283874855711
2024_03_28_19_23_33 Train loss: 0.1433759182691574 at step: 6000 lr 5.6485907296200035e-05
(Val @ epoch 749) acc: 0.9605263157894737; ap: 0.9949808199049645
(Val @ epoch 750) acc: 0.9671052631578947; ap: 0.9949808199049645
(Val @ epoch 751) acc: 0.9671052631578947; ap: 0.9949808199049645
(Val @ epoch 752) acc: 0.9736842105263158; ap: 0.9949808199049645
(Val @ epoch 753) acc: 0.9605263157894737; ap: 0.995108501212421
2024_03_28_19_23_42 Train loss: 0.18063053488731384 at step: 6040 lr 5.6485907296200035e-05
(Val @ epoch 754) acc: 0.9736842105263158; ap: 0.9946429285323861
(Val @ epoch 755) acc: 0.9539473684210527; ap: 0.9937202935829615
(Val @ epoch 756) acc: 0.9539473684210527; ap: 0.9943541482190292
(Val @ epoch 757) acc: 0.9605263157894737; ap: 0.9951451292798001
(Val @ epoch 758) acc: 0.9671052631578947; ap: 0.9955788007404737
2024_03_28_19_23_51 Train loss: 0.1513078510761261 at step: 6080 lr 5.6485907296200035e-05
(Val @ epoch 759) acc: 0.9539473684210527; ap: 0.9931078600009938
(Val @ epoch 760) acc: 0.9605263157894737; ap: 0.9931482001686726
(Val @ epoch 761) acc: 0.9671052631578947; ap: 0.993568882136975
(Val @ epoch 762) acc: 0.9605263157894737; ap: 0.9947922056658624
(Val @ epoch 763) acc: 0.9671052631578947; ap: 0.9954481842305468
2024_03_28_19_24_00 Train loss: 0.38998281955718994 at step: 6120 lr 5.6485907296200035e-05
(Val @ epoch 764) acc: 0.9407894736842105; ap: 0.9942857470650921
(Val @ epoch 765) acc: 0.9605263157894737; ap: 0.9954481842305468
(Val @ epoch 766) acc: 0.9605263157894737; ap: 0.9955788007404737
(Val @ epoch 767) acc: 0.9671052631578947; ap: 0.9948215125446115
(Val @ epoch 768) acc: 0.9736842105263158; ap: 0.9950857831577193
2024_03_28_19_24_10 Train loss: 0.2195398360490799 at step: 6160 lr 5.6485907296200035e-05
(Val @ epoch 769) acc: 0.9671052631578947; ap: 0.9948398941445341
(Val @ epoch 770) acc: 0.9736842105263158; ap: 0.9946124460364126
(Val @ epoch 771) acc: 0.9802631578947368; ap: 0.9953732086352104
(Val @ epoch 772) acc: 0.9736842105263158; ap: 0.9955788007404737
(Val @ epoch 773) acc: 0.9539473684210527; ap: 0.9949898086865787
2024_03_28_19_24_19 Train loss: 0.017050083726644516 at step: 6200 lr 5.6485907296200035e-05
(Val @ epoch 774) acc: 0.9539473684210527; ap: 0.9939216986748655
(Val @ epoch 775) acc: 0.9605263157894737; ap: 0.9949104095144293
(Val @ epoch 776) acc: 0.9473684210526315; ap: 0.9954481842305468
(Val @ epoch 777) acc: 0.9605263157894737; ap: 0.9954481842305468
(Val @ epoch 778) acc: 0.9671052631578947; ap: 0.9951174899940353
2024_03_28_19_24_28 Train loss: 0.12081269919872284 at step: 6240 lr 5.6485907296200035e-05
(Val @ epoch 779) acc: 0.9605263157894737; ap: 0.9949675793091523
2024_03_28_19_24_30 changing lr at the end of epoch 780, iters 6248
*************************
Changing lr from 5.6485907296200035e-05 to 5.083731656658003e-05
*************************
(Val @ epoch 780) acc: 0.9736842105263158; ap: 0.9946399152175194
(Val @ epoch 781) acc: 0.9539473684210527; ap: 0.9942113732269101
(Val @ epoch 782) acc: 0.9671052631578947; ap: 0.9942113732269101
(Val @ epoch 783) acc: 0.9605263157894737; ap: 0.9937828254239465
2024_03_28_19_24_37 Train loss: 0.8784397840499878 at step: 6280 lr 5.083731656658003e-05
(Val @ epoch 784) acc: 0.9671052631578947; ap: 0.9932865225991784
(Val @ epoch 785) acc: 0.9539473684210527; ap: 0.9927885530786158
(Val @ epoch 786) acc: 0.9013157894736842; ap: 0.9924339530560886
(Val @ epoch 787) acc: 0.9078947368421053; ap: 0.9926187640554286
(Val @ epoch 788) acc: 0.9276315789473685; ap: 0.9916737596802232
2024_03_28_19_24_46 Train loss: 0.7005482912063599 at step: 6320 lr 5.083731656658003e-05
(Val @ epoch 789) acc: 0.9539473684210527; ap: 0.9931745745770136
(Val @ epoch 790) acc: 0.9605263157894737; ap: 0.9928569006596525
(Val @ epoch 791) acc: 0.9407894736842105; ap: 0.992831275596291
(Val @ epoch 792) acc: 0.9539473684210527; ap: 0.9935528171380003
(Val @ epoch 793) acc: 0.9539473684210527; ap: 0.9937708204316277
2024_03_28_19_24_55 Train loss: 0.22211818397045135 at step: 6360 lr 5.083731656658003e-05
(Val @ epoch 794) acc: 0.9671052631578947; ap: 0.9941681073196358
(Val @ epoch 795) acc: 0.9605263157894737; ap: 0.9949675793091523
(Val @ epoch 796) acc: 0.9671052631578947; ap: 0.9947922056658624
(Val @ epoch 797) acc: 0.9671052631578947; ap: 0.995324613367054
(Val @ epoch 798) acc: 0.9605263157894737; ap: 0.9951939968571272
2024_03_28_19_25_05 Train loss: 0.9476995468139648 at step: 6400 lr 5.083731656658003e-05
(Val @ epoch 799) acc: 0.9736842105263158; ap: 0.9950417064087842
(Val @ epoch 800) acc: 0.9342105263157895; ap: 0.9949964757827117
(Val @ epoch 801) acc: 0.9605263157894737; ap: 0.994748134460171
(Val @ epoch 802) acc: 0.9539473684210527; ap: 0.9947922056658624
(Val @ epoch 803) acc: 0.9473684210526315; ap: 0.9942679044604944
2024_03_28_19_25_14 Train loss: 0.15507641434669495 at step: 6440 lr 5.083731656658003e-05
(Val @ epoch 804) acc: 0.9539473684210527; ap: 0.9944342893221702
(Val @ epoch 805) acc: 0.9539473684210527; ap: 0.9946399152175194
(Val @ epoch 806) acc: 0.9539473684210527; ap: 0.9936271121053535
(Val @ epoch 807) acc: 0.9605263157894737; ap: 0.9938052577073422
(Val @ epoch 808) acc: 0.9671052631578947; ap: 0.994745080447038
2024_03_28_19_25_23 Train loss: 0.3370441198348999 at step: 6480 lr 5.083731656658003e-05
(Val @ epoch 809) acc: 0.9539473684210527; ap: 0.9956144133729665
(Val @ epoch 810) acc: 0.9605263157894737; ap: 0.9955370812003645
(Val @ epoch 811) acc: 0.9736842105263158; ap: 0.9956169205154354
(Val @ epoch 812) acc: 0.9605263157894737; ap: 0.9953109303622186
(Val @ epoch 813) acc: 0.9539473684210527; ap: 0.9958898719509661
2024_03_28_19_25_32 Train loss: 0.06489939987659454 at step: 6520 lr 5.083731656658003e-05
(Val @ epoch 814) acc: 0.9671052631578947; ap: 0.995928275629244
(Val @ epoch 815) acc: 0.9407894736842105; ap: 0.9965041309755416
(Val @ epoch 816) acc: 0.9342105263157895; ap: 0.9951950300967778
(Val @ epoch 817) acc: 0.9671052631578947; ap: 0.9939151640066011
(Val @ epoch 818) acc: 0.9539473684210527; ap: 0.9956496267535466
2024_03_28_19_25_41 Train loss: 0.1471443623304367 at step: 6560 lr 5.083731656658003e-05
(Val @ epoch 819) acc: 0.9736842105263158; ap: 0.9962510449405283
(Val @ epoch 820) acc: 0.9802631578947368; ap: 0.9961142460349195
(Val @ epoch 821) acc: 0.9736842105263158; ap: 0.9959805919317385
(Val @ epoch 822) acc: 0.9407894736842105; ap: 0.9951507807183914
(Val @ epoch 823) acc: 0.9407894736842105; ap: 0.9936534414212386
2024_03_28_19_25_50 Train loss: 1.2432658672332764 at step: 6600 lr 5.083731656658003e-05
(Val @ epoch 824) acc: 0.9539473684210527; ap: 0.9924884477820014
(Val @ epoch 825) acc: 0.9539473684210527; ap: 0.9930536537183171
(Val @ epoch 826) acc: 0.9539473684210527; ap: 0.9943406613472922
(Val @ epoch 827) acc: 0.9605263157894737; ap: 0.9955481454688189
(Val @ epoch 828) acc: 0.9671052631578947; ap: 0.9955481454688189
2024_03_28_19_26_00 Train loss: 0.14349472522735596 at step: 6640 lr 5.083731656658003e-05
(Val @ epoch 829) acc: 0.9671052631578947; ap: 0.9954144913656379
(Val @ epoch 830) acc: 0.9671052631578947; ap: 0.9954144913656379
(Val @ epoch 831) acc: 0.9605263157894737; ap: 0.9955788007404737
(Val @ epoch 832) acc: 0.9671052631578947; ap: 0.9955788007404737
(Val @ epoch 833) acc: 0.9671052631578947; ap: 0.995283874855711
2024_03_28_19_26_09 Train loss: 0.8002244830131531 at step: 6680 lr 5.083731656658003e-05
(Val @ epoch 834) acc: 0.9671052631578947; ap: 0.9948206702611373
(Val @ epoch 835) acc: 0.9736842105263158; ap: 0.9940647327983227
(Val @ epoch 836) acc: 0.9605263157894737; ap: 0.9929458790345173
(Val @ epoch 837) acc: 0.9473684210526315; ap: 0.9950337366532702
(Val @ epoch 838) acc: 0.9407894736842105; ap: 0.9965699294287923
2024_03_28_19_26_18 Train loss: 0.16288858652114868 at step: 6720 lr 5.083731656658003e-05
(Val @ epoch 839) acc: 0.9539473684210527; ap: 0.9957520626327401
2024_03_28_19_26_20 changing lr at the end of epoch 840, iters 6728
*************************
Changing lr from 5.083731656658003e-05 to 4.575358490992203e-05
*************************
(Val @ epoch 840) acc: 0.9736842105263158; ap: 0.995283874855711
(Val @ epoch 841) acc: 0.9605263157894737; ap: 0.995283874855711
(Val @ epoch 842) acc: 0.9605263157894737; ap: 0.9949506014429913
(Val @ epoch 843) acc: 0.9671052631578947; ap: 0.9944486932941984
2024_03_28_19_26_27 Train loss: 0.2639023959636688 at step: 6760 lr 4.575358490992203e-05
(Val @ epoch 844) acc: 0.9671052631578947; ap: 0.9951561935482545
(Val @ epoch 845) acc: 0.9671052631578947; ap: 0.9949808199049645
(Val @ epoch 846) acc: 0.9802631578947368; ap: 0.9954481842305468
(Val @ epoch 847) acc: 0.9671052631578947; ap: 0.995108501212421
(Val @ epoch 848) acc: 0.9736842105263158; ap: 0.995108501212421
2024_03_28_19_26_36 Train loss: 0.4743940830230713 at step: 6800 lr 4.575358490992203e-05
(Val @ epoch 849) acc: 0.9605263157894737; ap: 0.995283874855711
(Val @ epoch 850) acc: 0.9539473684210527; ap: 0.9957812298902712
(Val @ epoch 851) acc: 0.9539473684210527; ap: 0.9934146882179588
(Val @ epoch 852) acc: 0.9671052631578947; ap: 0.9949012263497695
(Val @ epoch 853) acc: 0.9671052631578947; ap: 0.9957812298902712
2024_03_28_19_26_45 Train loss: 0.18016788363456726 at step: 6840 lr 4.575358490992203e-05
(Val @ epoch 854) acc: 0.9736842105263158; ap: 0.9960463619104213
(Val @ epoch 855) acc: 0.9605263157894737; ap: 0.9954863040055085
(Val @ epoch 856) acc: 0.9671052631578947; ap: 0.99449813949673
(Val @ epoch 857) acc: 0.9671052631578947; ap: 0.9958820525355855
(Val @ epoch 858) acc: 0.9671052631578947; ap: 0.9957219028917582
2024_03_28_19_26_55 Train loss: 1.4680709838867188 at step: 6880 lr 4.575358490992203e-05
(Val @ epoch 859) acc: 0.9736842105263158; ap: 0.996080566077587
(Val @ epoch 860) acc: 0.9539473684210527; ap: 0.9965041309755416
(Val @ epoch 861) acc: 0.9802631578947368; ap: 0.9961769784203481
(Val @ epoch 862) acc: 0.9671052631578947; ap: 0.9951832490547621
(Val @ epoch 863) acc: 0.9605263157894737; ap: 0.9960463619104213
2024_03_28_19_27_04 Train loss: 0.4744316637516022 at step: 6920 lr 4.575358490992203e-05
(Val @ epoch 864) acc: 0.9736842105263158; ap: 0.9955229320728878
(Val @ epoch 865) acc: 0.9605263157894737; ap: 0.9948483508043513
(Val @ epoch 866) acc: 0.9605263157894737; ap: 0.995313393044083
(Val @ epoch 867) acc: 0.9605263157894737; ap: 0.9963968616241458
(Val @ epoch 868) acc: 0.9802631578947368; ap: 0.9966948251021623
2024_03_28_19_27_13 Train loss: 0.1510893702507019 at step: 6960 lr 4.575358490992203e-05
(Val @ epoch 869) acc: 0.9736842105263158; ap: 0.9961769784203481
(Val @ epoch 870) acc: 0.9671052631578947; ap: 0.9958162825569028
(Val @ epoch 871) acc: 0.9736842105263158; ap: 0.9950564762789702
(Val @ epoch 872) acc: 0.9539473684210527; ap: 0.9953006249688768
(Val @ epoch 873) acc: 0.9605263157894737; ap: 0.9960460178536975
2024_03_28_19_27_22 Train loss: 0.1688460111618042 at step: 7000 lr 4.575358490992203e-05
(Val @ epoch 874) acc: 0.9671052631578947; ap: 0.9963106325235291
(Val @ epoch 875) acc: 0.9605263157894737; ap: 0.996051682899061
(Val @ epoch 876) acc: 0.9671052631578947; ap: 0.9964474314291379
(Val @ epoch 877) acc: 0.9671052631578947; ap: 0.9963106325235291
(Val @ epoch 878) acc: 0.9802631578947368; ap: 0.9962510449405283
2024_03_28_19_27_31 Train loss: 0.24629202485084534 at step: 7040 lr 4.575358490992203e-05
(Val @ epoch 879) acc: 0.9671052631578947; ap: 0.9959805919317385
(Val @ epoch 880) acc: 0.9605263157894737; ap: 0.9963106325235291
(Val @ epoch 881) acc: 0.9605263157894737; ap: 0.9952543417218106
(Val @ epoch 882) acc: 0.9539473684210527; ap: 0.9943203977679789
(Val @ epoch 883) acc: 0.9539473684210527; ap: 0.9925650957891893
2024_03_28_19_27_40 Train loss: 0.3637605905532837 at step: 7080 lr 4.575358490992203e-05
(Val @ epoch 884) acc: 0.9473684210526315; ap: 0.9920563880624144
(Val @ epoch 885) acc: 0.9671052631578947; ap: 0.9937009701764549
(Val @ epoch 886) acc: 0.9605263157894737; ap: 0.9937009701764549
(Val @ epoch 887) acc: 0.9539473684210527; ap: 0.9936098273776393
(Val @ epoch 888) acc: 0.9671052631578947; ap: 0.9939193175616384
2024_03_28_19_27_49 Train loss: 0.1296246349811554 at step: 7120 lr 4.575358490992203e-05
(Val @ epoch 889) acc: 0.9671052631578947; ap: 0.9949545118573953
(Val @ epoch 890) acc: 0.9671052631578947; ap: 0.9954315913494062
(Val @ epoch 891) acc: 0.9671052631578947; ap: 0.9951380194007929
(Val @ epoch 892) acc: 0.9671052631578947; ap: 0.9951380194007929
(Val @ epoch 893) acc: 0.9736842105263158; ap: 0.9957144983076761
2024_03_28_19_27_58 Train loss: 0.4629417657852173 at step: 7160 lr 4.575358490992203e-05
(Val @ epoch 894) acc: 0.9671052631578947; ap: 0.9955838817977493
(Val @ epoch 895) acc: 0.9736842105263158; ap: 0.9961769784203481
(Val @ epoch 896) acc: 0.9671052631578947; ap: 0.9958162825569028
(Val @ epoch 897) acc: 0.9605263157894737; ap: 0.9951545788593087
(Val @ epoch 898) acc: 0.9671052631578947; ap: 0.9956856710416685
2024_03_28_19_28_08 Train loss: 0.054062794893980026 at step: 7200 lr 4.575358490992203e-05
(Val @ epoch 899) acc: 0.9671052631578947; ap: 0.9964837003281759
2024_03_28_19_28_09 changing lr at the end of epoch 900, iters 7208
*************************
Changing lr from 4.575358490992203e-05 to 4.117822641892983e-05
*************************
(Val @ epoch 900) acc: 0.9671052631578947; ap: 0.996791321025105
(Val @ epoch 901) acc: 0.9671052631578947; ap: 0.9960917484076115
(Val @ epoch 902) acc: 0.9736842105263158; ap: 0.9963500462249949
(Val @ epoch 903) acc: 0.9802631578947368; ap: 0.9963106325235291
2024_03_28_19_28_17 Train loss: 0.9926525354385376 at step: 7240 lr 4.117822641892983e-05
(Val @ epoch 904) acc: 0.9671052631578947; ap: 0.9960463619104213
(Val @ epoch 905) acc: 0.9539473684210527; ap: 0.9960463619104213
(Val @ epoch 906) acc: 0.9671052631578947; ap: 0.9959707269239638
(Val @ epoch 907) acc: 0.9605263157894737; ap: 0.9957520626327401
(Val @ epoch 908) acc: 0.9671052631578947; ap: 0.9965041309755416
2024_03_28_19_28_26 Train loss: 0.31293192505836487 at step: 7280 lr 4.117822641892983e-05
(Val @ epoch 909) acc: 0.9605263157894737; ap: 0.9966166873091838
(Val @ epoch 910) acc: 0.9802631578947368; ap: 0.9965435446770072
(Val @ epoch 911) acc: 0.9736842105263158; ap: 0.9966545221194962
(Val @ epoch 912) acc: 0.9736842105263158; ap: 0.9958499754218116
(Val @ epoch 913) acc: 0.9671052631578947; ap: 0.9959805919317385
2024_03_28_19_28_35 Train loss: 0.08014632761478424 at step: 7320 lr 4.117822641892983e-05
(Val @ epoch 914) acc: 0.9736842105263158; ap: 0.9960230432264584
(Val @ epoch 915) acc: 0.9736842105263158; ap: 0.996219429715068
(Val @ epoch 916) acc: 0.9802631578947368; ap: 0.996219429715068
(Val @ epoch 917) acc: 0.9736842105263158; ap: 0.9964837003281759
(Val @ epoch 918) acc: 0.9802631578947368; ap: 0.9965208680163152
2024_03_28_19_28_44 Train loss: 0.7030397057533264 at step: 7360 lr 4.117822641892983e-05
(Val @ epoch 919) acc: 0.9473684210526315; ap: 0.9960230432264584
(Val @ epoch 920) acc: 0.9539473684210527; ap: 0.9928546776308316
(Val @ epoch 921) acc: 0.9605263157894737; ap: 0.9927061015836676
(Val @ epoch 922) acc: 0.9605263157894737; ap: 0.9956169205154354
(Val @ epoch 923) acc: 0.9473684210526315; ap: 0.9961938650177787
2024_03_28_19_28_53 Train loss: 0.18931898474693298 at step: 7400 lr 4.117822641892983e-05
(Val @ epoch 924) acc: 0.9671052631578947; ap: 0.9958953619190019
(Val @ epoch 925) acc: 0.9605263157894737; ap: 0.9963500462249949
(Val @ epoch 926) acc: 0.9802631578947368; ap: 0.9960230432264584
(Val @ epoch 927) acc: 0.9605263157894737; ap: 0.9952391177223479
(Val @ epoch 928) acc: 0.9539473684210527; ap: 0.9920330792230824
2024_03_28_19_29_02 Train loss: 0.24111078679561615 at step: 7440 lr 4.117822641892983e-05
(Val @ epoch 929) acc: 0.9605263157894737; ap: 0.993891240328496
(Val @ epoch 930) acc: 0.9671052631578947; ap: 0.9951252513255868
(Val @ epoch 931) acc: 0.9736842105263158; ap: 0.9962873138395663
(Val @ epoch 932) acc: 0.9671052631578947; ap: 0.9957897870162566
(Val @ epoch 933) acc: 0.9671052631578947; ap: 0.9959628548209033
2024_03_28_19_29_12 Train loss: 0.10519930720329285 at step: 7480 lr 4.117822641892983e-05
(Val @ epoch 934) acc: 0.9671052631578947; ap: 0.9960255872063318
(Val @ epoch 935) acc: 0.9736842105263158; ap: 0.9963193909533401
(Val @ epoch 936) acc: 0.9736842105263158; ap: 0.9964837003281759
(Val @ epoch 937) acc: 0.9473684210526315; ap: 0.9964837003281759
(Val @ epoch 938) acc: 0.9539473684210527; ap: 0.9967035835319733
2024_03_28_19_29_21 Train loss: 0.12613925337791443 at step: 7520 lr 4.117822641892983e-05
(Val @ epoch 939) acc: 0.9736842105263158; ap: 0.9966771987801882
(Val @ epoch 940) acc: 0.9671052631578947; ap: 0.9966204992337847
(Val @ epoch 941) acc: 0.9736842105263158; ap: 0.9964241127451751
(Val @ epoch 942) acc: 0.9736842105263158; ap: 0.9964241127451751
(Val @ epoch 943) acc: 0.9671052631578947; ap: 0.9963527397615253
2024_03_28_19_29_30 Train loss: 0.2911522388458252 at step: 7560 lr 4.117822641892983e-05
(Val @ epoch 944) acc: 0.9671052631578947; ap: 0.9962190856583443
(Val @ epoch 945) acc: 0.9671052631578947; ap: 0.9964241127451751
(Val @ epoch 946) acc: 0.9736842105263158; ap: 0.9954150513314992
(Val @ epoch 947) acc: 0.9539473684210527; ap: 0.995324613367054
(Val @ epoch 948) acc: 0.9736842105263158; ap: 0.9966204992337847
2024_03_28_19_29_39 Train loss: 0.45768114924430847 at step: 7600 lr 4.117822641892983e-05
(Val @ epoch 949) acc: 0.9736842105263158; ap: 0.9962510449405283
(Val @ epoch 950) acc: 0.9605263157894737; ap: 0.9959527673867267
(Val @ epoch 951) acc: 0.9539473684210527; ap: 0.9956963734989537
(Val @ epoch 952) acc: 0.9736842105263158; ap: 0.9958300276021347
(Val @ epoch 953) acc: 0.9736842105263158; ap: 0.9959709495054033
2024_03_28_19_29_48 Train loss: 0.2808142900466919 at step: 7640 lr 4.117822641892983e-05
(Val @ epoch 954) acc: 0.9671052631578947; ap: 0.9955968212880427
(Val @ epoch 955) acc: 0.9736842105263158; ap: 0.9956963734989537
(Val @ epoch 956) acc: 0.9736842105263158; ap: 0.9955657569890268
(Val @ epoch 957) acc: 0.9736842105263158; ap: 0.9957592554410393
(Val @ epoch 958) acc: 0.9671052631578947; ap: 0.9955657569890268
2024_03_28_19_29_57 Train loss: 0.36862608790397644 at step: 7680 lr 4.117822641892983e-05
(Val @ epoch 959) acc: 0.9539473684210527; ap: 0.9962536338822336
2024_03_28_19_29_59 changing lr at the end of epoch 960, iters 7688
*************************
Changing lr from 4.117822641892983e-05 to 3.7060403777036845e-05
*************************
(Val @ epoch 960) acc: 0.9671052631578947; ap: 0.9961230173723068
(Val @ epoch 961) acc: 0.9736842105263158; ap: 0.9960551203402322
(Val @ epoch 962) acc: 0.9605263157894737; ap: 0.9952813972283182
(Val @ epoch 963) acc: 0.9605263157894737; ap: 0.995551850237108
2024_03_28_19_30_07 Train loss: 0.6003171801567078 at step: 7720 lr 3.7060403777036845e-05
(Val @ epoch 964) acc: 0.9736842105263158; ap: 0.9955424383050641
(Val @ epoch 965) acc: 0.9671052631578947; ap: 0.9954380756815703
(Val @ epoch 966) acc: 0.9671052631578947; ap: 0.9965699294287923
(Val @ epoch 967) acc: 0.9671052631578947; ap: 0.9964097797849651
(Val @ epoch 968) acc: 0.9736842105263158; ap: 0.9956963734989537
2024_03_28_19_30_16 Train loss: 0.32314419746398926 at step: 7760 lr 3.7060403777036845e-05
(Val @ epoch 969) acc: 0.9671052631578947; ap: 0.9957388247936736
(Val @ epoch 970) acc: 0.9605263157894737; ap: 0.9954999870103441
(Val @ epoch 971) acc: 0.9736842105263158; ap: 0.9960629397556128
(Val @ epoch 972) acc: 0.9736842105263158; ap: 0.9954755337405753
(Val @ epoch 973) acc: 0.9736842105263158; ap: 0.9964837003281759
2024_03_28_19_30_25 Train loss: 0.14089006185531616 at step: 7800 lr 3.7060403777036845e-05
(Val @ epoch 974) acc: 0.9736842105263158; ap: 0.9954415468721455
(Val @ epoch 975) acc: 0.9736842105263158; ap: 0.9954999870103441
(Val @ epoch 976) acc: 0.9736842105263158; ap: 0.9962510449405283
(Val @ epoch 977) acc: 0.9671052631578947; ap: 0.9962510449405283
(Val @ epoch 978) acc: 0.9605263157894737; ap: 0.9955752009753265
2024_03_28_19_30_34 Train loss: 0.12319858372211456 at step: 7840 lr 3.7060403777036845e-05
(Val @ epoch 979) acc: 0.9605263157894737; ap: 0.995867279902807
(Val @ epoch 980) acc: 0.9539473684210527; ap: 0.9967605552561937
(Val @ epoch 981) acc: 0.9671052631578947; ap: 0.996791321025105
(Val @ epoch 982) acc: 0.9736842105263158; ap: 0.9969848194771175
(Val @ epoch 983) acc: 0.9736842105263158; ap: 0.9963351697877924
2024_03_28_19_30_44 Train loss: 0.32520341873168945 at step: 7880 lr 3.7060403777036845e-05
(Val @ epoch 984) acc: 0.9736842105263158; ap: 0.9957119998809353
(Val @ epoch 985) acc: 0.9671052631578947; ap: 0.9970046918124179
(Val @ epoch 986) acc: 0.9671052631578947; ap: 0.9971755136037382
(Val @ epoch 987) acc: 0.9802631578947368; ap: 0.9971755136037382
(Val @ epoch 988) acc: 0.9605263157894737; ap: 0.9975488059896326
2024_03_28_19_30_54 Train loss: 0.12118706852197647 at step: 7920 lr 3.7060403777036845e-05
(Val @ epoch 989) acc: 0.9802631578947368; ap: 0.9971755136037382
(Val @ epoch 990) acc: 0.9802631578947368; ap: 0.9969848194771175
(Val @ epoch 991) acc: 0.9736842105263158; ap: 0.9969039861225161
(Val @ epoch 992) acc: 0.9671052631578947; ap: 0.9971755136037382
(Val @ epoch 993) acc: 0.9736842105263158; ap: 0.9973634835285503
2024_03_28_19_31_04 Train loss: 0.7371028661727905 at step: 7960 lr 3.7060403777036845e-05
(Val @ epoch 994) acc: 0.9539473684210527; ap: 0.9973634835285503
(Val @ epoch 995) acc: 0.9605263157894737; ap: 0.9948353588408868
(Val @ epoch 996) acc: 0.9671052631578947; ap: 0.9965194514588724
(Val @ epoch 997) acc: 0.9671052631578947; ap: 0.9969264070074055
(Val @ epoch 998) acc: 0.9736842105263158; ap: 0.9970865566512328
2024_03_28_19_31_13 Train loss: 0.21078240871429443 at step: 8000 lr 3.7060403777036845e-05
(Val @ epoch 999) acc: 0.9539473684210527; ap: 0.996074526013947
*************************
2024_03_28_19_31_14
(0 FastGan   ) acc: 94.6; ap: 99.4
(1 StyleGAN_ADA) acc: 95.1; ap: 97.5
(2 Mean      ) acc: 94.9; ap: 98.4
*************************
2024_03_28_19_31_15
Saving model ./checkpoints/resnet10_60per2024_03_28_19_00_39/model_epoch_last.pth
